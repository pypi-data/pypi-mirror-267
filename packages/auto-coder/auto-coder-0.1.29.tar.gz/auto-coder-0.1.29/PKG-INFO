Metadata-Version: 2.1
Name: auto-coder
Version: 0.1.29
Summary: AutoCoder: AutoCoder
Author: allwefantasy
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Description-Content-Type: text/markdown
Requires-Dist: contextlib2
Requires-Dist: ninja
Requires-Dist: accelerate
Requires-Dist: bitsandbytes==0.39.0
Requires-Dist: transformers>=4.35.0
Requires-Dist: torch==2.1.2
Requires-Dist: ray[default]==2.9.1
Requires-Dist: sentencepiece
Requires-Dist: pyarrow==12.0.1
Requires-Dist: sentence-transformers
Requires-Dist: datasets
Requires-Dist: langchain
Requires-Dist: paramiko
Requires-Dist: einops
Requires-Dist: tqdm
Requires-Dist: loguru
Requires-Dist: pyjava==0.6.15
Requires-Dist: tiktoken
Requires-Dist: transformers_stream_generator
Requires-Dist: optimum
Requires-Dist: fastapi
Requires-Dist: uvicorn
Requires-Dist: retrying
Requires-Dist: zhipuai
Requires-Dist: dashscope
Requires-Dist: tiktoken
Requires-Dist: tabulate
Requires-Dist: jupyter_client
Requires-Dist: prompt-toolkit
Requires-Dist: llama_index
Requires-Dist: websocket-client
Requires-Dist: sqlmodel
Requires-Dist: wudao
Requires-Dist: SwissArmyTransformer
Requires-Dist: jieba
Requires-Dist: byzerllm>=0.1.60
Requires-Dist: GitPython
Requires-Dist: openai>=1.14.3
Requires-Dist: anthropic
Requires-Dist: google-generativeai
Requires-Dist: protobuf

<p align="center">
  <picture>    
    <img alt="auto-coder" src="./logo/auto-coder.jpeg" width=55%>
  </picture>
</p>

<h3 align="center">
Auto-Coder (powered by Byzer-LLM)
</h3>

<p align="center">
| <a href="./docs/en"><b>English</b></a> | <a href="./docs/zh"><b>‰∏≠Êñá</b></a> |

</p>

---

*Latest News* üî•

- [2024/04] Release Auto-Coder 0.1.29
- [2024/03] Release Auto-Coder 0.1.25
- [2024/03] Release Auto-Coder 0.1.24

---

## Table of Contents

- [Installation](#installation)
- [Brand new Installation](#raw-meta-matchine-installation)
- [Usage](#usage)
  - [Basic](#basic)
  - [Advanced](#advanced)
  - [Python Project Only Features](#python-project-only-features)
  - [TypeScript Project](#typescript-project)
  - [Real-Auto](#real-auto)
    - [Real-Auto with Search Engine](#real-auto-with-search-engine)



## Installation

```shell
conda create --name autocoder python=3.10.11
conda activate autocoder
pip install -U auto-coder
## if you want to use private/open-source models, uncomment this line.
# pip install -U vllm
ray start --head
```

## Setup machine for OpenSource/Private Model

You can use the script provided by Byzer-LLM to setup the nvidia-driver/cuda environment:

1. [CentOS 8 / Ubuntu 20.04 / Ubuntu 22.04](https://docs.byzer.org/#/byzer-lang/zh-cn/byzer-llm/deploy)

After the nvidia-driver/cuda environment is set up, you can install auto_coder like this:

```shell
pip install -U auto-coder
```


## Usage 

### LLM Model

> Recommend to use ÂçÉ‰πâÈÄöÈóÆMax/Qwen-Max SaaS model
> Make sure your model has at least 8k context.

Try to use the following command to deploy Qwen-Max:

```shell
byzerllm deploy  --pretrained_model_type saas/qianwen \
--infer_params saas.api_key=xxxxxxx saas.model=qwen-max \
--model qianwen_chat 
```

If your SaaS model support the OpenAI SDK, you can use the following command to deploy the model:

```shell
byzerllm deploy  --pretrained_model_type saas/official_openai \
--infer_params saas.api_key=xxxxxxx saas.model=yi-34b-chat-0205 saas.base_url=https://api.lingyiwanwu.com/v1 \
--model yi_chat
```

Then you can use the following command to test the model:

```shell
byzerllm query --model qianwen_chat --query "‰Ω†Â•Ω"
```

If you want to undeploy the model:

```shell
byzerllm undeploy --model qianwen_chat
```

If you want to deploy you private/open source model, please try to this [link](https://github.com/allwefantasy/byzer-llm)

### Basic 


The auto-coder provide two ways:

1. Generate context for the query and you can copy&&Paste to Web UI of ChatGPT/Claud3/Kimi.
2. Use the model from Byzer-LLM to generate the result directly.

>> Note: You should make sure the model has a long context length support, e.g. >32k. 

The auto-coder will collect the source code from the source directory, and then generate context into the target file based on the query.

Then you can copy the content of `output.txt` and paste it to Web UI of ChatGPT/Claud3/Kimi:

For example:

```shell
auto-coder --source_dir /home/winubuntu/projects/ByzerRawCopilot --target_file /home/winubuntu/projects/ByzerRawCopilot/output.txt --query "Â¶Ç‰ΩïËÆ©Ëøô‰∏™Á≥ªÁªüÂèØ‰ª•ÈÄöËøá auto-coder ÂëΩ‰ª§ÊâßË°åÔºü" 
```

You can also put all arguments into a yaml file:


```yaml
# /home/winubuntu/projects/ByzerRawCopilot/auto-coder.yaml
source_dir: /home/winubuntu/projects/ByzerRawCopilot
target_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt
query: |
  Â¶Ç‰ΩïËÆ©Ëøô‰∏™Á≥ªÁªüÂèØ‰ª•ÈÄöËøá auto-coder ÂëΩ‰ª§ÊâßË°åÔºü
```
  
Then use the following command:

```shell
auto-coder --file /home/winubuntu/projects/ByzerRawCopilot/auto-coder.yaml
``` 

If you want to use the model from Byzer-LLM, you can use the following command:

```shell
auto-coder --source_dir /home/winubuntu/projects/ByzerRawCopilot --target_file /home/winubuntu/projects/ByzerRawCopilot/output.txt --model qianwen_chat --execute --query "ÈáçÊñ∞ÁîüÊàê‰∏Ä‰∏™ is_likely_useful_file ÊñπÊ≥ïÔºåÊª°Ë∂≥reactjs+typescript ÁªÑÂêàÁöÑÈ°πÁõÆ„ÄÇ" 
```

In the above command, we provide a model and enable the execute mode, the auto-coder will collect the source code from the source directory, and then generate context for the query, and then use the model to generate the result, then put the result into the target file.

### How to reduce the context length?

As you may know, auto-coder will collect the source code from the source directory, and then generate context for the query, if the source directory is too large, the context will be too long, and the model may not be able to handle it.

There are two ways to reduce the context length:

1. Change the source_dir to sub directory of project.
2. Enable aut-coder's index feature.

In order to use the index feature, you should configure some extra parameters:

1. skip_build_index: false
2. model

For example:

```yaml
source_dir: /home/winubuntu/projects/ByzerRawCopilot 
target_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt 

model: qianwen_chat
model_max_length: 2000
model_max_input_length: 6000
anti_quota_limit: 13

skip_build_index: false

project_type: "copilot/.py"
query: |
  ‰ºòÂåñ copilot ÈáåÁöÑ get_suffix_from_project_type ÂáΩÊï∞Âπ∂Êõ¥Êñ∞ÂéüÊñá‰ª∂
```

Here we add a new parameter `skip_build_index`, by default, this value is true. 
If you set it to false and a model provide at the same time, then the auto-coder will generate index for the source code using the model(This may cost a lot of tokens), and the index file will be stored in a directory called `.auto-coder` in source directory. 

Once the index is created, the auto-coder will use the index to filter files and reduce the context length. notice that the filter action also use model, and it may cost tokens, so you should use it carefully.


### Advanced

> This feature only works with the model from Byzer-LLM.

Translate the markdown file in the project:

```shell

auto-coder --source_dir /home/winubuntu/projects/ByzerRawCopilot --target_file /home/winubuntu/projects/ByzerRawCopilot/output.txt --project_type "translate/‰∏≠Êñá/.md/cn" --model_max_length 2000 --model qianwen_chat 
```
When you want to translate some files, you must specify the model parameter. And the project_type is a litle bit complex, it's a combination of the following parameters:

- translate: the project type
- ‰∏≠Êñá: the target language you want to translate to
- .md: the file extension you want to translate
- cn: the new file suffix created with the translated content. for example, if the original file is README.md, the new file will be README-cn.md

So the final project_type is "translate/‰∏≠Êñá/.md/cn"

If your model is powerful enough, you can use the following command to do the same task:

```shell
auto-coder --source_dir /home/winubuntu/projects/ByzerRawCopilot --target_file /home/winubuntu/projects/ByzerRawCopilot/output.txt --model qianwen_chat --project_type translate --model_max_length 2000 --query "ÊääÈ°πÁõÆ‰∏≠ÁöÑmarkdownÊñáÊ°£ÁøªËØëÊàê‰∏≠Êñá"
```

The model will extract "translate/‰∏≠Êñá/.md/cn" from the query and then do the same thing as the previous command.

Note: The model_max_length is used to control the model's generation length, if the model_max_length is not set, the default value is 1024.
You should change the value based on your esitmating on the length of the translation.


### Python Project Only Features

In order to reduce the context length collected by the auto-coder, if you are dealing with a python project, you can use the following command:


```shell
auto-coder --target_file /home/winubuntu/projects/ByzerRawCopilot/output.txt --script_path /home/winubuntu/projects/ByzerRawCopilot/xxx --package_name byzer_copilot --project_type py-script --query "Â∏ÆÊàëÂÆûÁé∞scriptÊ®°Âùó‰∏≠ËøòÊ≤°ÊúâÂÆûÁé∞ÊñπÊ≥ï"

```

In the above command, we provide a script path and a package name, the script_path is the python file you are working on now, and the package_name 
is you cares about, then the auto-coder only collect the context from the package_name and imported by the script_path file, this will significantly reduce the context length.

When you refer `script module` in `--query`, you means you are talking about script_path file.

After the job is done, you can copy the prompt from the output.txt and paste it to Web of ChatGPT or other AI models.

If you specify the model, the auto-coder will use the model to generate the result, then put the result into the target file.

```shell
auto-coder --target_file /home/winubuntu/projects/ByzerRawCopilot/output.txt --script_path /home/winubuntu/projects/YOUR_PROJECT/xxx.py --package_name xxxx --project_type py-script --model qianwen_chat --execute --query "Â∏ÆÊàëÂÆûÁé∞scriptÊ®°Âùó‰∏≠ËøòÊ≤°ÊúâÂÆûÁé∞ÊñπÊ≥ï" 
```

## TypeScript Project

Just try to set the project_type to ts-script.

## Real-Auto


Here is a example:

```shell
auto-coder --source_dir /home/winubuntu/projects/ByzerRawCopilot --target_file /home/winubuntu/projects/ByzerRawCopilot/output.txt --project_type copilot --model_max_length 2000 --model qianwen_chat  --query "Â∏ÆÊàëÂàõÂª∫‰∏Ä‰∏™ÂêçÂ≠óÂè´t-copilot ÁöÑpythonÈ°πÁõÆÔºåÁîüÊàêÁöÑÁõÆÂΩïÈúÄË¶ÅÁ¨¶ÂêàÂåÖË£ÖÁöÑpythonÈ°πÁõÆÁªìÊûÑ"

```

This project type will automatically create a python project based on the query, and then generate the result based on the query. 

You can check all log in the `output.txt` file.

auto-coder also support python code interpreter,try this:
  
```shell 
auto-coder --source_dir /home/winubuntu/projects/ByzerRawCopilot --target_file /home/winubuntu/projects/ByzerRawCopilot/output.txt --project_type copilot --model_max_length 2000 --model qianwen_chat  --query "Áî®pythonÊâìÂç∞‰Ω†Â•ΩÔºå‰∏≠ÂõΩ" 
```

The content of the output.txt will be:

```text
=================CONVERSATION==================

user: 
Ê†πÊçÆÁî®Êà∑ÁöÑÈóÆÈ¢òÔºåÂØπÈóÆÈ¢òËøõË°åÊãÜËß£ÔºåÁÑ∂ÂêéÁîüÊàêÊâßË°åÊ≠•È™§„ÄÇ

ÁéØÂ¢É‰ø°ÊÅØÂ¶Ç‰∏ã:
Êìç‰ΩúÁ≥ªÁªü: linux 5.15.0-48-generic  
PythonÁâàÊú¨: 3.10.11
CondaÁéØÂ¢É: byzerllm-dev 
ÊîØÊåÅBash

Áî®Êà∑ÁöÑÈóÆÈ¢òÊòØÔºöÁî®pythonÊâìÂç∞‰Ω†Â•ΩÔºå‰∏≠ÂõΩ

ÊØèÊ¨°ÁîüÊàê‰∏Ä‰∏™ÊâßË°åÊ≠•È™§ÔºåÁÑ∂ÂêéËØ¢ÈóÆÊàëÊòØÂê¶ÁªßÁª≠ÔºåÂΩìÊàëÂõûÂ§çÁªßÁª≠ÔºåÁªßÁª≠ÁîüÊàê‰∏ã‰∏Ä‰∏™ÊâßË°åÊ≠•È™§„ÄÇ

assistant: 
{
  "code": "print('‰Ω†Â•ΩÔºå‰∏≠ÂõΩ')",
  "lang": "python",
  "total_steps": 1,
  "cwd": "",
  "env": {},
  "timeout": -1,
  "ignore_error": false
}

ÊòØÂê¶ÁªßÁª≠Ôºü
user: ÁªßÁª≠
=================RESULT==================

Python Code:
print('‰Ω†Â•ΩÔºå‰∏≠ÂõΩ')
Output:
‰Ω†Â•ΩÔºå‰∏≠ÂõΩ
--------------------
```

### Real-Auto with Search Engine

If you want to get a more stable result, you should introduce search engine:

```yaml
source_dir: /home/winubuntu/projects/ByzerRawCopilot 
target_file: /home/winubuntu/projects/ByzerRawCopilot/output.txt 

model: qianwen_short_chat
model_max_length: 2000
anti_quota_limit: 5

search_engine: bing
search_engine_token: xxxxxx

project_type: "copilot"
query: |
  Â∏ÆÊàëÂú®/tmp/ÁõÆÂΩï‰∏ãÂàõÂª∫‰∏Ä‰∏™ typescript + reactjs ÁªÑÊàêÁöÑÈ°πÁõÆÔºåÈ°πÁõÆÂêçÂ≠óÂè´ t-project
```

Here we add  new parameters  `search_engine` and `search_engine_token`, the search engine will provide more context for the model, and the model will use the context to generate the result.

For now, we support bing/google.  If you use bing, try to get the token from [here](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api).

The basic workflow is:

1. search the query 
2. reranking the search result by snippets
3. fetch the first search result and answer the question based on the full content.
4. generate the result based on the query and the full content.
5. get execute steps based on the result.
5. execute the steps by ShellClient/PythonClient in auto-coder.

Here is the output:

```text
Áî®Êà∑Â∞ùËØï: UserIntent.CREATE_NEW_PROJECT
search SearchEngine.BING for Â∏ÆÊàëÂú®/tmp/ÁõÆÂΩï‰∏ãÂàõÂª∫‰∏Ä‰∏™ typescript + reactjs ÁªÑÊàêÁöÑÈ°πÁõÆÔºåÈ°πÁõÆÂêçÂ≠óÂè´ t-project...
reraking the search result by snippets...
fetch https://blog.csdn.net/weixin_42429718/article/details/117402097 and answer the quesion (Â∏ÆÊàëÂú®/tmp/ÁõÆÂΩï‰∏ãÂàõÂª∫‰∏Ä‰∏™ typescript + reactjs ÁªÑÊàêÁöÑÈ°πÁõÆÔºåÈ°πÁõÆÂêçÂ≠óÂè´ t-project) based on the full content...
user: 
‰Ω†ÁÜüÊÇâÂêÑÁßçÁºñÁ®ãËØ≠Ë®Ä‰ª•ÂèäÁõ∏ÂÖ≥Ê°ÜÊû∂ÂØπÂ∫îÁöÑÈ°πÁõÆÁªìÊûÑ„ÄÇÁé∞Âú®Ôºå‰Ω†ÈúÄË¶Å
Ê†πÊçÆÁî®Êà∑ÁöÑÈóÆÈ¢òÔºåÊ†πÊçÆÊèê‰æõÁöÑ‰ø°ÊÅØÔºåÂØπÈóÆÈ¢òËøõË°åÊãÜËß£ÔºåÁÑ∂ÂêéÁîüÊàêÊâßË°åÊ≠•È™§ÔºåÂΩìÊâßË°åÂÆåÊâÄÊúâÊ≠•È™§ÔºåÊúÄÁªàÂ∏ÆÁîüÊàê‰∏Ä‰∏™Á¨¶ÂêàÂØπÂ∫îÁºñÁ®ãËØ≠Ë®ÄËßÑËåÉ‰ª•ÂèäÁõ∏ÂÖ≥Ê°ÜÊû∂ÁöÑÈ°πÁõÆÁªìÊûÑ„ÄÇ
Êï¥‰∏™ËøáÁ®ãÂè™ËÉΩ‰ΩøÁî® python/shell„ÄÇ

ÁéØÂ¢É‰ø°ÊÅØÂ¶Ç‰∏ã:
Êìç‰ΩúÁ≥ªÁªü: linux 5.15.0-48-generic  
PythonÁâàÊú¨: 3.10.11
CondaÁéØÂ¢É: byzerllm-dev 
ÊîØÊåÅBash

Áé∞Âú®ËØ∑ÂèÇËÄÉ‰∏ãÈù¢ÂÜÖÂÆπÔºö

Áî±‰∫éÊèê‰æõÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØ‰∏éÂú®LinuxÁéØÂ¢É‰∏ã‰ΩøÁî®ÂëΩ‰ª§Ë°åÂàõÂª∫‰∏Ä‰∏™TypeScriptÂíåReactJSÈ°πÁõÆÊó†ÂÖ≥ÔºåÊàëÂ∞ÜÂü∫‰∫é‰∏ÄËà¨Êìç‰ΩúÊ≠•È™§ÁªôÂá∫Ëß£Á≠î„ÄÇ

Ë¶ÅÂú®LinuxÁ≥ªÁªüÁöÑ `/tmp/` ÁõÆÂΩï‰∏ãÂàõÂª∫‰∏Ä‰∏™Áî±TypeScriptÂíåReactJSÁªÑÊàêÁöÑÈ°πÁõÆÔºåÂπ∂ÂëΩÂêç‰∏∫`t-project`ÔºåËØ∑ÊåâÁÖß‰ª•‰∏ãÊ≠•È™§Êìç‰ΩúÔºö

1. È¶ñÂÖàÔºåËØ∑Á°Æ‰øùÊÇ®Â∑≤ÂÖ®Â±ÄÂÆâË£Ö‰∫ÜNode.jsÂåÖÁÆ°ÁêÜÂô®ÔºànpmÔºâ‰ª•ÂèäÂàõÂª∫ReactÂ∫îÁî®ÁöÑËÑöÊâãÊû∂Â∑•ÂÖ∑ `create-react-app`„ÄÇÂ¶ÇÊûúÂ∞öÊú™ÂÆâË£ÖÔºåÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§ÂÆâË£ÖÔºö
   \```
   npm install -g create-react-app
   \```

2. ÁÑ∂ÂêéÔºåÁî±‰∫é`create-react-app`ÈªòËÆ§‰∏çÊîØÊåÅ TypeScriptÔºåÈúÄË¶ÅÂÆâË£Ö `create-react-app` ÁöÑ TypeScript ÁâàÊú¨ÔºåÂç≥ `react-scripts-ts`Ôºå‰ΩÜËØ∑Ê≥®ÊÑèÔºå`react-scripts-ts` Â∑≤‰∏çÂÜçÁª¥Êä§ÔºåÊúÄÊñ∞Êé®ËçêÂÅöÊ≥ïÊòØÁõ¥Êé•‰ΩøÁî® `create-react-app` Âπ∂ÈÄöËøá `--template typescript` ÂèÇÊï∞ÊåáÂÆö TypeScript Ê®°ÊùøÔºö

   \```sh
   npx create-react-app t-project --template typescript
   \```

   ËøôÊù°ÂëΩ‰ª§‰ºöÂú® `/tmp/` ÁõÆÂΩï‰∏ãÂàõÂª∫Âêç‰∏∫ `t-project` ÁöÑÊñ∞ReactÈ°πÁõÆÔºåÂπ∂ÈÖçÁΩÆ‰∏∫‰ΩøÁî®TypeScript„ÄÇ

3. ÂàõÂª∫ÂÆåÊàêÂêéÔºåËøõÂÖ•È°πÁõÆÁõÆÂΩïÂπ∂ÂêØÂä®ÂºÄÂèëÊúçÂä°Âô®Ôºö

   \```sh
   cd /tmp/t-project
   npm start
   \```

ËøôÊ†∑Â∞±ÊàêÂäüÂú® `/tmp/` ÁõÆÂΩï‰∏ãÂàõÂª∫‰∫Ü‰∏Ä‰∏™Âü∫‰∫éTypeScriptÂíåReactJSÁöÑÈ°πÁõÆ„ÄÇ[information is missing on]Â¶Ç‰ΩïÂÖ∑‰Ωì‰ΩøÁî®ÂëΩ‰ª§Ë°åÂú®LinuxÁ≥ªÁªü‰∏≠ÂàõÂª∫È°πÁõÆÔºåÂõ†‰∏∫‰∏ä‰∏ãÊñáÊ≤°ÊúâÊèê‰æõÁõ∏ÂÖ≥ÊåáÂØº„ÄÇ

Áî®Êà∑ÁöÑÈóÆÈ¢òÊòØÔºöÂ∏ÆÊàëÂú®/tmp/ÁõÆÂΩï‰∏ãÂàõÂª∫‰∏Ä‰∏™ typescript + reactjs ÁªÑÊàêÁöÑÈ°πÁõÆÔºåÈ°πÁõÆÂêçÂ≠óÂè´ t-project

ÊØèÊ¨°ÁîüÊàê‰∏Ä‰∏™ÊâßË°åÊ≠•È™§ÔºåÁÑ∂ÂêéËØ¢ÈóÆÊàëÊòØÂê¶ÁªßÁª≠ÔºåÂΩìÊàëÂõûÂ§çÁªßÁª≠ÔºåÁªßÁª≠ÁîüÊàê‰∏ã‰∏Ä‰∏™ÊâßË°åÊ≠•È™§„ÄÇ        

assistant: \```json
{
  "code": "npm install -g create-react-app",
  "lang": "shell",
  "total_steps": 3,
  "current_step": 1,
  "cwd": "/tmp",
  "env": {},
  "timeout": null,
  "ignore_error": false
}
\```
ËØ∑Á°ÆËÆ§ÊòØÂê¶Â∑≤ÂÆâË£ÖÂÖ®Â±ÄÁöÑ `create-react-app` Â∑•ÂÖ∑„ÄÇÂ¶ÇÊûúÁ°ÆËÆ§ÊàñÊâßË°åÂÆåÊàêÔºåËØ∑ÂõûÂ§ç‚ÄúÁªßÁª≠‚Äù„ÄÇ

user: ÁªßÁª≠

assistant: \```json
{
  "code": "npx create-react-app t-project --template typescript",
  "lang": "shell",
  "total_steps": 3,
  "current_step": 2,
  "cwd": "/tmp",
  "env": {},
  "timeout": null,
  "ignore_error": false
}
\```
ËØ∑Âú® `/tmp` ÁõÆÂΩï‰∏ãÊâßË°åÊ≠§ÂëΩ‰ª§‰ª•ÂàõÂª∫Âü∫‰∫é TypeScript ÁöÑ ReactJS È°πÁõÆ„ÄÇÂ¶ÇÊûúÈ°πÁõÆÂàõÂª∫ÂÆåÊàêÔºåËØ∑ÂõûÂ§ç‚ÄúÁªßÁª≠‚Äù„ÄÇ

user: ÁªßÁª≠

assistant: \```json
{
  "code": "cd t-project",
  "lang": "shell",
  "total_steps": 3,
  "current_step": 3,
  "cwd": "/tmp",
  "env": {},
  "timeout": null,
  "ignore_error": false
}
\```
ËØ∑Âú®ÁªàÁ´Ø‰∏≠ÂàáÊç¢Âà∞ÂàöÂàõÂª∫ÁöÑ `t-project` ÁõÆÂΩï„ÄÇËã•ÂàáÊç¢ÊàêÂäüÔºåËØ∑ÂõûÂ§ç‚ÄúÁªßÁª≠‚Äù‰ª•ËøõË°å‰∏ã‰∏ÄÊ≠•Êìç‰ΩúÔºåÂç≥ÂêØÂä®È°πÁõÆÂºÄÂèëÊúçÂä°Âô®„ÄÇ

user: ÁªßÁª≠
\```

There are three steps to create the new project, and the ShellClient in auto-coder will finally execute the three steps.
