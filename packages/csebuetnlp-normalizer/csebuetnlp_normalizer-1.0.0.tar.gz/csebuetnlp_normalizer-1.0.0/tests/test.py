#%%
from normalizer import normalize
from normalizer import const

def test_url_replacements():
    data = [
        [ 
            "‡¶Ø‡ßá‡¶≠‡¶æ‡¶¨‡ßá ‡¶Æ‡ßá‡¶Æ‡ßç‡¶¨‡¶æ‡¶∞ ‡¶π‡¶¨‡ßá‡¶® https://couchsurfing.com ‡¶è ‡¶è‡¶ï‡¶ü‡¶ø ‡¶è‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶ñ‡ßÅ‡¶≤‡ßÅ‡¶®‡•§",
            "‡¶Ø‡ßá‡¶≠‡¶æ‡¶¨‡ßá ‡¶Æ‡ßá‡¶Æ‡ßç‡¶¨‡¶æ‡¶∞ ‡¶π‡¶¨‡ßá‡¶® ‡¶è ‡¶è‡¶ï‡¶ü‡¶ø ‡¶è‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶ñ‡ßÅ‡¶≤‡ßÅ‡¶®‡•§"
        ]
    ]
    
    for d in data:
        normalized_text = normalize(d[0], url_replacement="")
        assert normalized_text == d[1]

def test_char_replacements():
    data = [
        [ 
            "La retirada de Occidente de Afganist√°n significa que hay mucho en juego para China, Rusia, Pakist√°n e Ir√°n.",
            "La retirada de Occidente de Afganistan significa que hay mucho en juego para China, Rusia, Pakistan e Iran."
        ]
    ]
    
    for d in data:
        normalized_text = normalize(d[0])
        assert normalized_text == d[1]


def test_emoji_replacements():
    data = [
        [ 
            "üóΩ–£–¥–∞—á–∏, –õ—é–±–≤–∏ –∏ –°—á–∞—Ç—å—è!üèù –°—Ä–æ—á–Ω–æüë®üèª‚Äçüé® –Ω—É–∂–Ω—ã –¢–∞–π–Ω—ã–µüì∫ –ê–≥–µ–Ω—Ç—ãüí°! –í —Å–≤—è–∑–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–ºüë©üèª‚Äçüç≥",
            "<<emoticon>>–£–¥–∞—á–∏, –õ—é–±–≤–∏ –∏ –°—á–∞—Ç—å—è!<<emoticon>> –°—Ä–æ—á–Ω–æ<<emoticon>> –Ω—É–∂–Ω—ã –¢–∞–π–Ω—ã–µ<<emoticon>> –ê–≥–µ–Ω—Ç—ã<<emoticon>>! –í —Å–≤—è–∑–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º<<emoticon>>"
        ]
    ]
    
    for d in data:
        normalized_text = normalize(d[0], emoji_replacement="<<emoticon>>")
        assert normalized_text == d[1]

def test_punct_replacements():
    data = [
        [ 
            "‡¶Ø‡ßá‡¶≠‡¶æ‡¶¨‡ßá ‡¶Æ‡ßá‡¶Æ‡ßç‡¶¨‡¶æ‡¶∞ ‡¶π‡¶¨‡ßá‡¶® https://couchsurfing.com ‡¶è ‡¶è‡¶ï‡¶ü‡¶ø ‡¶è‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶ñ‡ßÅ‡¶≤‡ßÅ‡¶®‡•§",
            "‡¶Ø‡ßá‡¶≠‡¶æ‡¶¨‡ßá ‡¶Æ‡ßá‡¶Æ‡ßç‡¶¨‡¶æ‡¶∞ ‡¶π‡¶¨‡ßá‡¶® https<<punct>><<punct>><<punct>>couchsurfing<<punct>>com ‡¶è ‡¶è‡¶ï‡¶ü‡¶ø ‡¶è‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶ñ‡ßÅ‡¶≤‡ßÅ‡¶®<<punct>>"
        ]
    ]
    
    for d in data:
        normalized_text = normalize(d[0], punct_replacement="<<punct>>")
        assert normalized_text == d[1]


def test_unicode_replacements():
    data = [
        [ 
            "\u09F7",
            "\u0964"
        ],
        [
            "\u09af\u09bc",
            "\u09af\u09bc"
        ],
        [
            "\u00a0",
            " "
        ],
        [
            '‡ßá‡¶∂‡ßó',
            '‡¶∂‡ßå'
        ]
    ]
    
    for d in data:
        normalized_text = normalize(d[0])
        assert normalized_text == d[1], f"{normalized_text} != {d[1]}" 



if __name__ == "__main__":
    test_url_replacements()
    test_char_replacements()
    test_emoji_replacements()
    test_punct_replacements()
    test_unicode_replacements()