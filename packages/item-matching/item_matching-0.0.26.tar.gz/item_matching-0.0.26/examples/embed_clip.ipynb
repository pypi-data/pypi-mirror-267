{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import duckdb\n",
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "import numpy as np\n",
    "from autofaiss import build_index\n",
    "from src.item_matching.build_index.func_img import PipelineImage\n",
    "from src.item_matching.build_index.model import Model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T06:32:10.329159Z",
     "start_time": "2024-04-09T06:32:07.903878Z"
    }
   },
   "id": "28f6df8c7caed3f9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Data \n",
    "\n",
    "Prepare 2 datasets: Database and Query"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fb85ea4eb5faddf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-09T06:34:31.000920Z",
     "start_time": "2024-04-09T06:32:10.329987Z"
    }
   },
   "source": [
    "path = Path('/home/kevin/Downloads/cb')\n",
    "path_db = path / 'cb_2024-03-07.parquet'\n",
    "\n",
    "# db\n",
    "query = f\"\"\"\n",
    "select *\n",
    ",concat('http://f.shopee.vn/file/', UNNEST(array_slice(string_split(images, ','), 1, 1))) image_url\n",
    "from parquet_scan('{str(path_db)}')\n",
    "order by item_id, images\n",
    "\"\"\"\n",
    "df_db = (\n",
    "    duckdb.sql(query).pl()\n",
    "    .select(pl.all().name.prefix(f'db_'))\n",
    "    .head(10_000)\n",
    ")\n",
    "pipe = PipelineImage(path, col_image='image_url')\n",
    "df_img_db = pipe.load_images('db')\n",
    "df_db = (\n",
    "    df_db.drop(['images'])\n",
    "    .join(df_img_db, on='db_image_url', how='left')\n",
    "    .filter(pl.col('db_exists'))\n",
    ")\n",
    "\n",
    "\n",
    "# q\n",
    "df_q = df_db.clone()\n",
    "df_q.columns = [f'q_{i.split('db_')[1]}' for i in df_db.columns]\n",
    "df_q.head()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading json in folder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 862006/862006 [02:02<00:00, 7013.91it/s]\n",
      "Loading jpg in folder: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 862006/862006 [00:00<00:00, 5214514.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m13:34:30\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mload_images\u001B[0m | \u001B[1m[Data] Load Images: (862006, 3)\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "shape: (5, 9)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ q_item_id ‚îÜ q_item_na ‚îÜ q_shop_id ‚îÜ q_images  ‚îÜ ‚Ä¶ ‚îÜ q_create_ ‚îÜ q_image_u ‚îÜ q_file_pa ‚îÜ q_exists ‚îÇ\n",
       "‚îÇ ---       ‚îÜ me        ‚îÜ ---       ‚îÜ ---       ‚îÜ   ‚îÜ datetime  ‚îÜ rl        ‚îÜ th        ‚îÜ ---      ‚îÇ\n",
       "‚îÇ i64       ‚îÜ ---       ‚îÜ i64       ‚îÜ str       ‚îÜ   ‚îÜ ---       ‚îÜ ---       ‚îÜ ---       ‚îÜ bool     ‚îÇ\n",
       "‚îÇ           ‚îÜ str       ‚îÜ           ‚îÜ           ‚îÜ   ‚îÜ date      ‚îÜ str       ‚îÜ str       ‚îÜ          ‚îÇ\n",
       "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
       "‚îÇ 299879156 ‚îÜ üçéFREE    ‚îÜ 619038499 ‚îÜ 020b2fb32 ‚îÜ ‚Ä¶ ‚îÜ 2022-03-1 ‚îÜ http://f. ‚îÜ /home/kev ‚îÜ true     ‚îÇ\n",
       "‚îÇ 4         ‚îÜ SHIPüçéLit ‚îÜ           ‚îÜ c9e0b6416 ‚îÜ   ‚îÜ 6         ‚îÜ shopee.vn ‚îÜ in/Downlo ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ hium067   ‚îÜ           ‚îÜ 3acaa30ca ‚îÜ   ‚îÜ           ‚îÜ /file/020 ‚îÜ ads/cb/do ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ 24Pcs/Bag ‚îÜ           ‚îÜ 439c8‚Ä¶    ‚îÜ   ‚îÜ           ‚îÜ b2fb3‚Ä¶    ‚îÜ wnloa‚Ä¶    ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ ‚Ä¶         ‚îÜ           ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ          ‚îÇ\n",
       "‚îÇ 299943096 ‚îÜ D·ª•ng c·ª•   ‚îÜ 619035621 ‚îÜ cn-111342 ‚îÜ ‚Ä¶ ‚îÜ 2022-05-2 ‚îÜ http://f. ‚îÜ /home/kev ‚îÜ true     ‚îÇ\n",
       "‚îÇ 9         ‚îÜ c·∫Øt rau   ‚îÜ           ‚îÜ 07-7qukw- ‚îÜ   ‚îÜ 6         ‚îÜ shopee.vn ‚îÜ in/Downlo ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ c·ªß ƒëa     ‚îÜ           ‚îÜ ljmfkrswh ‚îÜ   ‚îÜ           ‚îÜ /file/cn- ‚îÜ ads/cb/do ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ nƒÉng d·ª•ng ‚îÜ           ‚îÜ 81g72‚Ä¶    ‚îÜ   ‚îÜ           ‚îÜ 11134‚Ä¶    ‚îÜ wnloa‚Ä¶    ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ ‚Ä¶         ‚îÜ           ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ          ‚îÇ\n",
       "‚îÇ 299956883 ‚îÜ B·ªô 50 M√≥n ‚îÜ 779448044 ‚îÜ sg-111342 ‚îÜ ‚Ä¶ ‚îÜ 2022-07-2 ‚îÜ http://f. ‚îÜ /home/kev ‚îÜ true     ‚îÇ\n",
       "‚îÇ 3         ‚îÜ D·ª•ng C·ª•   ‚îÜ           ‚îÜ 01-22090- ‚îÜ   ‚îÜ 0         ‚îÜ shopee.vn ‚îÜ in/Downlo ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ L√†m S·∫°ch  ‚îÜ           ‚îÜ i9hdlm15f ‚îÜ   ‚îÜ           ‚îÜ /file/sg- ‚îÜ ads/cb/do ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ RƒÉng ‚Ä¶    ‚îÜ           ‚îÜ xhvd4‚Ä¶    ‚îÜ   ‚îÜ           ‚îÜ 11134‚Ä¶    ‚îÜ wnloa‚Ä¶    ‚îÜ          ‚îÇ\n",
       "‚îÇ 299971434 ‚îÜ [H√†ng     ‚îÜ 619019560 ‚îÜ sg-111342 ‚îÜ ‚Ä¶ ‚îÜ 2022-09-2 ‚îÜ http://f. ‚îÜ /home/kev ‚îÜ true     ‚îÇ\n",
       "‚îÇ 6         ‚îÜ HOT] Gi√°  ‚îÜ           ‚îÜ 01-22100- ‚îÜ   ‚îÜ 7         ‚îÜ shopee.vn ‚îÜ in/Downlo ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ ƒë·ª° ƒëi·ªán   ‚îÜ           ‚îÜ jwoiswgbw ‚îÜ   ‚îÜ           ‚îÜ /file/sg- ‚îÜ ads/cb/do ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ tho·∫°i     ‚îÜ           ‚îÜ bjv9e‚Ä¶    ‚îÜ   ‚îÜ           ‚îÜ 11134‚Ä¶    ‚îÜ wnloa‚Ä¶    ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ G·∫Øn‚Ä¶      ‚îÜ           ‚îÜ           ‚îÜ   ‚îÜ           ‚îÜ           ‚îÜ           ‚îÜ          ‚îÇ\n",
       "‚îÇ 299978716 ‚îÜ SUPERCUTE ‚îÜ 881303265 ‚îÜ e0c117e68 ‚îÜ ‚Ä¶ ‚îÜ 2022-11-1 ‚îÜ http://f. ‚îÜ /home/kev ‚îÜ true     ‚îÇ\n",
       "‚îÇ 5         ‚îÜ T√∫i V·∫£i   ‚îÜ           ‚îÜ b628d42f5 ‚îÜ   ‚îÜ 8         ‚îÜ shopee.vn ‚îÜ in/Downlo ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ canvas    ‚îÜ           ‚îÜ 28f77da2f ‚îÜ   ‚îÜ           ‚îÜ /file/e0c ‚îÜ ads/cb/do ‚îÜ          ‚îÇ\n",
       "‚îÇ           ‚îÜ ƒêeo Vai‚Ä¶  ‚îÜ           ‚îÜ 3d486‚Ä¶    ‚îÜ   ‚îÜ           ‚îÜ 117e6‚Ä¶    ‚îÜ wnloa‚Ä¶    ‚îÜ          ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
      ],
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>q_item_id</th><th>q_item_name</th><th>q_shop_id</th><th>q_images</th><th>q_level1_global_be_category</th><th>q_create_datetime</th><th>q_image_url</th><th>q_file_path</th><th>q_exists</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>date</td><td>str</td><td>str</td><td>bool</td></tr></thead><tbody><tr><td>2998791564</td><td>&quot;üçéFREE SHIPüçéLit‚Ä¶</td><td>619038499</td><td>&quot;020b2fb32c9e0b‚Ä¶</td><td>&quot;Fashion Access‚Ä¶</td><td>2022-03-16</td><td>&quot;http://f.shope‚Ä¶</td><td>&quot;/home/kevin/Do‚Ä¶</td><td>true</td></tr><tr><td>2999430969</td><td>&quot;D·ª•ng c·ª• c·∫Øt ra‚Ä¶</td><td>619035621</td><td>&quot;cn-11134207-7q‚Ä¶</td><td>&quot;Home &amp; Living&quot;</td><td>2022-05-26</td><td>&quot;http://f.shope‚Ä¶</td><td>&quot;/home/kevin/Do‚Ä¶</td><td>true</td></tr><tr><td>2999568833</td><td>&quot;B·ªô 50 M√≥n D·ª•ng‚Ä¶</td><td>779448044</td><td>&quot;sg-11134201-22‚Ä¶</td><td>&quot;Beauty&quot;</td><td>2022-07-20</td><td>&quot;http://f.shope‚Ä¶</td><td>&quot;/home/kevin/Do‚Ä¶</td><td>true</td></tr><tr><td>2999714346</td><td>&quot;[H√†ng HOT] Gi√°‚Ä¶</td><td>619019560</td><td>&quot;sg-11134201-22‚Ä¶</td><td>&quot;Automobiles&quot;</td><td>2022-09-27</td><td>&quot;http://f.shope‚Ä¶</td><td>&quot;/home/kevin/Do‚Ä¶</td><td>true</td></tr><tr><td>2999787165</td><td>&quot;SUPERCUTE T√∫i ‚Ä¶</td><td>881303265</td><td>&quot;e0c117e68b628d‚Ä¶</td><td>&quot;Women Bags&quot;</td><td>2022-11-18</td><td>&quot;http://f.shope‚Ä¶</td><td>&quot;/home/kevin/Do‚Ä¶</td><td>true</td></tr></tbody></table></div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Embeddings\n",
    "\n",
    "Use datasets and clip to transform images to vectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "539fe7f71bc59b80"
  },
  {
   "cell_type": "code",
   "source": "img_model, img_processor = Model().get_img_model(model_id='openai/clip-vit-base-patch32')",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T06:34:36.502297Z",
     "start_time": "2024-04-09T06:34:31.001574Z"
    }
   },
   "id": "70a41ebe42d746ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m13:34:31\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36m__init__\u001B[0m | \u001B[1m[Model] Run on: cuda\u001B[0m\n",
      "\u001B[32m13:34:36\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mget_img_model\u001B[0m | \u001B[1mImage model: openai/clip-vit-base-patch32\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = Dataset.from_pandas(df_db.to_pandas())\n",
    "fn_kwargs = {'col': f'db_file_path', 'processor': img_processor, 'model': img_model}\n",
    "dataset = dataset.map(Model().pp_img, batched=True, batch_size=128, fn_kwargs=fn_kwargs)\n",
    "dataset.set_format(type='numpy', columns=['img_embed'], output_all_columns=True)\n",
    "\n",
    "# save to disk\n",
    "path_tmp_array = Path('tmp/array')\n",
    "path_tmp_ds = Path('tmp/ds')\n",
    "np.save(path_tmp_array / 'array.npy', dataset['img_embed'])\n",
    "dataset.save_to_disk(path_tmp_ds / 'ds')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T06:34:41.148112Z",
     "start_time": "2024-04-09T06:34:36.503025Z"
    }
   },
   "id": "b95e12aa6944cd3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m13:34:36\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36m__init__\u001B[0m | \u001B[1m[Model] Run on: cuda\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10191 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dadfd3025dd849c4bf5e3af32546f26f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 450.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 265.81 MiB is free. Process 1610 has 132.68 MiB memory in use. Process 8291 has 17.46 MiB memory in use. Process 135909 has 294.00 MiB memory in use. Process 147787 has 3.47 GiB memory in use. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 1.97 GiB is allocated by PyTorch, and 44.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m dataset \u001B[38;5;241m=\u001B[39m Dataset\u001B[38;5;241m.\u001B[39mfrom_pandas(df_db\u001B[38;5;241m.\u001B[39mto_pandas())\n\u001B[1;32m      2\u001B[0m fn_kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdb_file_path\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocessor\u001B[39m\u001B[38;5;124m'\u001B[39m: img_processor, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m: img_model}\n\u001B[0;32m----> 3\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpp_img\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m768\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m dataset\u001B[38;5;241m.\u001B[39mset_format(\u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m'\u001B[39m, columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimg_embed\u001B[39m\u001B[38;5;124m'\u001B[39m], output_all_columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# save to disk\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/datasets/arrow_dataset.py:593\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    591\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    592\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 593\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    594\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    595\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[1;32m    596\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/datasets/arrow_dataset.py:558\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    551\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[1;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[1;32m    555\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[1;32m    556\u001B[0m }\n\u001B[1;32m    557\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[0;32m--> 558\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    559\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[1;32m    560\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/datasets/arrow_dataset.py:3105\u001B[0m, in \u001B[0;36mDataset.map\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3099\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3100\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[1;32m   3101\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3102\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[1;32m   3103\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3104\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[0;32m-> 3105\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_single\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdataset_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   3106\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   3107\u001B[0m \u001B[43m                \u001B[49m\u001B[43mshards_done\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/datasets/arrow_dataset.py:3482\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[1;32m   3478\u001B[0m indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m   3479\u001B[0m     \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m*\u001B[39m(\u001B[38;5;28mslice\u001B[39m(i, i \u001B[38;5;241m+\u001B[39m batch_size)\u001B[38;5;241m.\u001B[39mindices(shard\u001B[38;5;241m.\u001B[39mnum_rows)))\n\u001B[1;32m   3480\u001B[0m )  \u001B[38;5;66;03m# Something simpler?\u001B[39;00m\n\u001B[1;32m   3481\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3482\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[43mapply_function_on_filtered_inputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3485\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_same_num_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlist_indexes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3486\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3487\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3488\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NumExamplesMismatchError:\n\u001B[1;32m   3489\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetTransformationNotAllowedError(\n\u001B[1;32m   3490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3491\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/datasets/arrow_dataset.py:3361\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[1;32m   3359\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_rank:\n\u001B[1;32m   3360\u001B[0m     additional_args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (rank,)\n\u001B[0;32m-> 3361\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfn_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43madditional_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3362\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, LazyDict):\n\u001B[1;32m   3363\u001B[0m     processed_inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m   3364\u001B[0m         k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mkeys_to_format\n\u001B[1;32m   3365\u001B[0m     }\n",
      "File \u001B[0;32m~/PycharmProjects/item_matching/src/item_matching/build_index/model.py:41\u001B[0m, in \u001B[0;36mModel.pp_img\u001B[0;34m(batch, model, processor, col)\u001B[0m\n\u001B[1;32m     39\u001B[0m inputs \u001B[38;5;241m=\u001B[39m processor(images\u001B[38;5;241m=\u001B[39mimages, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n\u001B[0;32m---> 41\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mpooler_output\n\u001B[1;32m     43\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mnormalize(pooled_output, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:925\u001B[0m, in \u001B[0;36mCLIPVisionModel.forward\u001B[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    901\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;124;03mReturns:\u001B[39;00m\n\u001B[1;32m    903\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    921\u001B[0m \u001B[38;5;124;03m>>> pooled_output = outputs.pooler_output  # pooled CLS states\u001B[39;00m\n\u001B[1;32m    922\u001B[0m \u001B[38;5;124;03m```\"\"\"\u001B[39;00m\n\u001B[1;32m    923\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m--> 925\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvision_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    929\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    930\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:852\u001B[0m, in \u001B[0;36mCLIPVisionTransformer.forward\u001B[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    849\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(pixel_values)\n\u001B[1;32m    850\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpre_layrnorm(hidden_states)\n\u001B[0;32m--> 852\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    856\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    857\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    859\u001B[0m last_hidden_state \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    860\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m last_hidden_state[:, \u001B[38;5;241m0\u001B[39m, :]\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:638\u001B[0m, in \u001B[0;36mCLIPEncoder.forward\u001B[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    630\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    631\u001B[0m         encoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    632\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    635\u001B[0m         output_attentions,\n\u001B[1;32m    636\u001B[0m     )\n\u001B[1;32m    637\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 638\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mencoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    639\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    640\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    641\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcausal_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    642\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    643\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    645\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    647\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:390\u001B[0m, in \u001B[0;36mCLIPEncoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001B[0m\n\u001B[1;32m    388\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    389\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer_norm2(hidden_states)\n\u001B[0;32m--> 390\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    391\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    393\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/transformers/models/clip/modeling_clip.py:346\u001B[0m, in \u001B[0;36mCLIPMLP.forward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m    345\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(hidden_states)\n\u001B[0;32m--> 346\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactivation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(hidden_states)\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/transformers/activations.py:96\u001B[0m, in \u001B[0;36mQuickGELUActivation.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msigmoid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1.702\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 450.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 265.81 MiB is free. Process 1610 has 132.68 MiB memory in use. Process 8291 has 17.46 MiB memory in use. Process 135909 has 294.00 MiB memory in use. Process 147787 has 3.47 GiB memory in use. Including non-PyTorch memory, this process has 3.29 GiB memory in use. Of the allocated memory 1.97 GiB is allocated by PyTorch, and 44.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Indexing\n",
    "\n",
    "Build index to search items"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f5934d701125e88"
  },
  {
   "cell_type": "code",
   "source": [
    "path_index = Path('tmp/index')\n",
    "build_index(\n",
    "    str(path_tmp_array),\n",
    "    index_path=str(path_index / f'ip.index'),\n",
    "    index_infos_path=str(path_index / f'index.json'),\n",
    "    save_on_disk=True,\n",
    "    metric_type='ip',\n",
    "    verbose=30,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96cc5afcf31a1e6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load index into datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61b78720de597863"
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_db = concatenate_datasets([\n",
    "    load_from_disk(str(f)) for f in sorted(path_tmp_ds.glob('*'))\n",
    "])\n",
    "\n",
    "# add index\n",
    "dataset_db.load_faiss_index('img_embed', path_index / f'ip.index')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6efdce62f49c27b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4) Retrieve"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1790f349c3dd80ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Batch search top-k from datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b9699dadfcbd348"
  },
  {
   "cell_type": "code",
   "source": [
    "score, result = dataset_db.get_nearest_examples_batch(\n",
    "    'img_embed',\n",
    "    np.asarray(dataset_db['img_embed']),\n",
    "    k=5\n",
    ")\n",
    "\n",
    "dict_ = {'score_img': [list(i) for i in score]}\n",
    "df_score = pl.DataFrame(dict_)\n",
    "df_result = pl.DataFrame(result).drop(['img_embed'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bcba75a987dd8c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5) Post process"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b879e463bb688c29"
  },
  {
   "cell_type": "code",
   "source": [
    "df_match = pl.concat([df_q, df_result, df_score], how='horizontal')\n",
    "col_explode = [i for i in df_match.columns if 'db' in i] + ['score_img']\n",
    "df_match = df_match.explode(col_explode)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae3b4d1db34781b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_match"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c77a28d769f090a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# df_match.write_csv(path / 'match.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T09:39:35.884786Z",
     "start_time": "2024-03-07T09:39:35.850251Z"
    }
   },
   "id": "2dacba735258ef3d",
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
