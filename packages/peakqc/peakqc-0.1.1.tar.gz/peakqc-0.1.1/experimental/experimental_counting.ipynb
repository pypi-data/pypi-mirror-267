{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Counting Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:43:39.898178187Z",
     "start_time": "2023-12-15T14:43:39.854159700Z"
    }
   },
   "outputs": [],
   "source": [
    "bamfile = \"/mnt/workspace2/jdetlef/data/public_data/sorted_heart_left_ventricle_194.bam\"\n",
    "fragments_file = \"/mnt/workspace2/jdetlef/data/public_data/fragments_heart_left_ventricle_194_sorted.bed\"\n",
    "h5ad_file = \"/mnt/workspace2/jdetlef/data/public_data/heart_lv_SM-JF1NY.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:38:59.522229322Z",
     "start_time": "2023-12-15T14:38:54.585241821Z"
    }
   },
   "outputs": [],
   "source": [
    "import peakqc.general as general\n",
    "import peakqc.insertsizes as insertsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:45:37.197738984Z",
     "start_time": "2023-12-15T15:45:37.148652850Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import datetime\n",
    "from multiprocessing import Manager, Lock, Pool\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "from beartype import beartype\n",
    "import numpy.typing as npt\n",
    "from beartype.typing import Any, Optional, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:44:38.389758803Z",
     "start_time": "2023-12-15T14:44:34.966900277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 9110 × 1154611\n",
       "    obs: 'logUMI', 'tsse', 'tissue', 'cell type', 'Life stage', 'closest Cell Ontology term(s)', 'Cell Ontology ID'\n",
       "    var: 'Chromosome', 'hg38_Start', 'hg38_End', 'Class', 'Present in fetal tissues', 'Present in adult tissues', 'CRE module'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = sc.read_h5ad(h5ad_file)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:46:14.499936670Z",
     "start_time": "2023-12-15T14:46:14.474794834Z"
    }
   },
   "outputs": [],
   "source": [
    "adata_barcodes = adata.obs.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:56:13.154204628Z",
     "start_time": "2023-12-15T15:56:13.136171928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.16 ms, sys: 0 ns, total: 3.16 ms\n",
      "Wall time: 3.17 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# split index for barcodes CBs\n",
    "barcodes = []\n",
    "for entry in adata_barcodes:\n",
    "    barcode = entry.split('+')[1]\n",
    "    barcodes.append(barcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening bam file...\n",
      "Creating chunks of size 100000bp...\n",
      "Counting insertsizes across 30895 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30895/30895 [20:48<00:00, 24.75it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading file - elapsed time: 0:20:48\n",
      "Converting counts to dataframe...\n",
      "Done getting insertsizes from fragments!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_bam = insertsizes.insertsize_from_bam(bamfile=bamfile,\n",
    "                        barcodes=barcodes,\n",
    "                        barcode_tag='CB',\n",
    "                        chunk_size=100000,\n",
    "                        regions=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count insertsizes from fragments...\n",
      "Starting counting fragments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 17it [01:41,  5.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading file - elapsed time: 0:02:04\n",
      "Converting counts to dataframe...\n",
      "Done getting insertsizes from fragments!\n"
     ]
    }
   ],
   "source": [
    "table_from_fragments = insertsizes.insertsize_from_fragments(fragments=fragments_file,\n",
    "                              barcodes=barcodes,\n",
    "                              n_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_insertsize</th>\n",
       "      <th>insertsize_count</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAATCCGCATAAACGTCCCGTT</th>\n",
       "      <td>200.82</td>\n",
       "      <td>5761</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAATCCGCATAAATGCTACGGG</th>\n",
       "      <td>162.90</td>\n",
       "      <td>15741</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 1, 3, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAATCCGCATAACATGAAGCGC</th>\n",
       "      <td>191.14</td>\n",
       "      <td>2638</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAATCCGCATAACCGCTAATGA</th>\n",
       "      <td>174.73</td>\n",
       "      <td>4213</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAATCCGCATAACTTCGACCAG</th>\n",
       "      <td>184.72</td>\n",
       "      <td>17424</td>\n",
       "      <td>[0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 14, 0, 2, 3, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTCGTCCGACTGCCGTCTCAAC</th>\n",
       "      <td>184.97</td>\n",
       "      <td>3669</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTCGTCCGACTGCGTCGAGTAC</th>\n",
       "      <td>194.13</td>\n",
       "      <td>11340</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTCGTCCGACTTGAGTGCTGTG</th>\n",
       "      <td>186.06</td>\n",
       "      <td>3786</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTCGTCCGACTTTCGAAGAAGG</th>\n",
       "      <td>191.28</td>\n",
       "      <td>5783</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTCGTCCGACTTTGTGTTACCG</th>\n",
       "      <td>188.35</td>\n",
       "      <td>4693</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9110 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mean_insertsize  insertsize_count  \\\n",
       "AAATCCGCATAAACGTCCCGTT           200.82              5761   \n",
       "AAATCCGCATAAATGCTACGGG           162.90             15741   \n",
       "AAATCCGCATAACATGAAGCGC           191.14              2638   \n",
       "AAATCCGCATAACCGCTAATGA           174.73              4213   \n",
       "AAATCCGCATAACTTCGACCAG           184.72             17424   \n",
       "...                                 ...               ...   \n",
       "TTCGTCCGACTGCCGTCTCAAC           184.97              3669   \n",
       "TTCGTCCGACTGCGTCGAGTAC           194.13             11340   \n",
       "TTCGTCCGACTTGAGTGCTGTG           186.06              3786   \n",
       "TTCGTCCGACTTTCGAAGAAGG           191.28              5783   \n",
       "TTCGTCCGACTTTGTGTTACCG           188.35              4693   \n",
       "\n",
       "                                                                     dist  \n",
       "AAATCCGCATAAACGTCCCGTT  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, ...  \n",
       "AAATCCGCATAAATGCTACGGG  [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 1, 3, 9, ...  \n",
       "AAATCCGCATAACATGAAGCGC  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "AAATCCGCATAACCGCTAATGA  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, ...  \n",
       "AAATCCGCATAACTTCGACCAG  [0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 14, 0, 2, 3, 10...  \n",
       "...                                                                   ...  \n",
       "TTCGTCCGACTGCCGTCTCAAC  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "TTCGTCCGACTGCGTCGAGTAC  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...  \n",
       "TTCGTCCGACTTGAGTGCTGTG  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...  \n",
       "TTCGTCCGACTTTCGAAGAAGG  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "TTCGTCCGACTTTGTGTTACCG  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[9110 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5758/3223871923.py:1: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block2_values] [items->Index(['dist'], dtype='object')]\n",
      "\n",
      "  table_from_fragments.to_hdf('count_table_heart_lv.h5',\n"
     ]
    }
   ],
   "source": [
    "table_from_fragments.to_hdf('count_table_heart_lv.h5',\n",
    "                            key='df', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_fragments.to_csv('count_table_heart_lv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_table = pd.read_csv('count_table_heart_lv.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0 0 0 ... 0 0 0]'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_table['dist'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_list_to_file(str_list, file_path):\n",
    "    \"\"\"\n",
    "    Stores a list of strings to a file, with each string on a new line.\n",
    "\n",
    "    Args:\n",
    "    str_list (list of str): The list of strings to store.\n",
    "    file_path (str): The path to the file where the list should be stored.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in str_list:\n",
    "            file.write(f\"{item}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_list_to_file(barcodes, 'barcodes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a list of strings from a file, assuming each line in the file is a separate string.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The path to the file to read.\n",
    "\n",
    "    Returns:\n",
    "    list of str: The list of strings read from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [line.strip() for line in file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9110"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(barcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TAGTGCTGTCTCGCTTAGCCTT',\n",
       " 'AAATCCGCATAACCGCTAATGA',\n",
       " 'AAATCCGCATACCAATCGCTTG',\n",
       " 'AAATCCGCATACGCGGTATGTA',\n",
       " 'AAATCCGCATATTAGGAGGTCT',\n",
       " 'AAATCCGCATCATCTCGACAAT',\n",
       " 'AAATCCGCATCGCAAGTAAAGC',\n",
       " 'AAATCCGCATGCTATGTCTCTC',\n",
       " 'AAATCCGCATGGGAACAAGTCA',\n",
       " 'AAATCCGCATGTACATCTTCAC',\n",
       " 'AAATCCGCATGTCCACCTAGAC',\n",
       " 'AAATCCGCATGTTCTCCTACTC',\n",
       " 'AAATCCGCATTACGATCTATCG',\n",
       " 'AAATCCGCATTAGGTTGCACTA',\n",
       " 'AAATCCGCATTATGGTCCAGAC',\n",
       " 'AAATCCGCATTCCACACCTCCA',\n",
       " 'AAATCCGCATTTGCATGTACGC',\n",
       " 'AACGACCAAAAAAGCCCACGAC',\n",
       " 'AACGACCAAAACAGAAGGACCT',\n",
       " 'AACGACCAAAACATCCGGTCTC',\n",
       " 'AACGACCAAAAGGTCCCAATCG',\n",
       " 'AACGACCAAAAGTGTCGGGTTG',\n",
       " 'AACGACCAAAATTGGACTTAGC',\n",
       " 'AACGACCAAACACGTTCCAGCA',\n",
       " 'AACGACCAAACAGCGAAGACCT',\n",
       " 'AACGACCAAACGACGAGAGATC',\n",
       " 'AACGACCAAACGATTTCACCGA',\n",
       " 'AACGACCAAACGCTATCTCTTG',\n",
       " 'AACGACCAAACTTACAATGCGA',\n",
       " 'AACGACCAAAGAAGCCGTTGGT',\n",
       " 'AACGACCAAAGTAACTTGCCCT',\n",
       " 'AACGACCAAATAAGACTGAGGG',\n",
       " 'AACGACCAAATCGGTACTAACT',\n",
       " 'AACGACGTGTACCACGCAAACT',\n",
       " 'AACGACGTGTGGAACGGTCTCA',\n",
       " 'AACGACGTGTTGCTAATGACGT',\n",
       " 'AAGCAAAGTCAAATAAGGCCAG',\n",
       " 'AAGCAAAGTCAATCCCGCGCTT',\n",
       " 'AAGCAAAGTCATAACGCAAGTC',\n",
       " 'AAGCAAAGTCCAGTTGACGCTC',\n",
       " 'AAGCAAAGTCCTTCGCGTAACG',\n",
       " 'AAGCAAAGTCGTATATCCGGAA',\n",
       " 'AAGCAAAGTCGTCCTATGTCCG',\n",
       " 'AAGCAAAGTCGTGCAAACCACT',\n",
       " 'AAGCAAAGTCGTTGTACGAGCC',\n",
       " 'AAGCAAAGTCTAGACTGGACCA',\n",
       " 'AAGCAAAGTCTGAACGCAATCC',\n",
       " 'AAGCAAAGTCTGCACGTTAACC',\n",
       " 'AAGCTTGTGCAGATGCCGATGT',\n",
       " 'AAGCTTGTGCATGTCTAACGCC',\n",
       " 'AAGCTTGTGCATTTCGATGACG',\n",
       " 'AAGCTTGTGCCTAACAGTACTC',\n",
       " 'AAGCTTGTGCGAGTTATTTCGG',\n",
       " 'AAGCTTGTGCTACAACGTAGCT',\n",
       " 'AAGCTTGTGCTAGGTAAGACCA',\n",
       " 'AAGCTTGTGCTCCACTTGGATA',\n",
       " 'AAGCTTGTGCTGACTATTTCCC',\n",
       " 'AAGTCCTTAGATGTGCCTTCTG',\n",
       " 'AAGTCCTTAGATTGCCTTCTTC',\n",
       " 'AAGTCCTTAGATTGGGCTAGTT',\n",
       " 'AAGTCCTTAGATTTGCCCAGCT',\n",
       " 'AAGTCCTTAGCAAGTTAGTGAC',\n",
       " 'AAGTCCTTAGCGTCAATAGCTG',\n",
       " 'AAGTCCTTAGCGTGATGAGGTA',\n",
       " 'AAGTCCTTAGGAAAGCATGCGA',\n",
       " 'AAGTCCTTAGGATAGGTGTAGC',\n",
       " 'AAGTCCTTAGGGATTGTTTCTG',\n",
       " 'AAGTCCTTAGGGGACAGTGTAT',\n",
       " 'AAGTCCTTAGTAGTCCACCAGT',\n",
       " 'AAGTCCTTAGTCGACCAGATCT',\n",
       " 'AAGTCCTTAGTCTTTGACGCTG',\n",
       " 'AAGTCCTTAGTGAGCTAGACGC',\n",
       " 'AAGTCCTTAGTGGCTCGACTCA',\n",
       " 'ACAAACCCTCAGCATACTCAGC',\n",
       " 'ACAAACCCTCAGCGAATAACCC',\n",
       " 'ACAAACCCTCAGGAACTGTCGT',\n",
       " 'ACAAACCCTCGAGGCATTTGCA',\n",
       " 'ACAAACCCTCGATCCATTCGTG',\n",
       " 'ACAAACCCTCGCAGAGAGAGAA',\n",
       " 'ACAAACCCTCGCGACGTTACGA',\n",
       " 'ACAAACCCTCTATCAAAGGGTC',\n",
       " 'ACAAACCCTCTATCGCCGGGTA',\n",
       " 'ACAAACCCTCTCTAATGCCTGC',\n",
       " 'ACAAACCCTCTCTAATTGCGAC',\n",
       " 'ACAATGCTCCACCACTATTCAC',\n",
       " 'ACAATGCTCCACCCGAGTCAGT',\n",
       " 'ACAATGCTCCAGATTTGAGTCC',\n",
       " 'ACAATGCTCCAGGAACTGTCGT',\n",
       " 'ACAATGCTCCATCGTACCCGTT',\n",
       " 'ACAATGCTCCATGTGCCTTCTG',\n",
       " 'ACAATGCTCCCAGCGAAGACCT',\n",
       " 'ACAATGCTCCCATTAGTGTACC',\n",
       " 'ACAATGCTCCCGTCCCTAGAAC',\n",
       " 'ACAATGCTCCGGCACTTCATCT',\n",
       " 'ACAATGCTCCGTTATCAGGGCC',\n",
       " 'ACAATGCTCCTACACCAGAGCC',\n",
       " 'ACAATGCTCCTATGGTCCAGAC',\n",
       " 'ACAATGCTCCTCAAGGCCTGGA',\n",
       " 'ACAATGCTCCTGAAGTTCCGAG',\n",
       " 'ACAATGCTCCTGCGTTGATGTG',\n",
       " 'ACAGCAAGTCAAATGCTACGGG',\n",
       " 'ACAGCAAGTCAGCAGTTGGTGG',\n",
       " 'ACAGCAAGTCATCCGTGGTTGC',\n",
       " 'ACAGCAAGTCCACAAATCAACC',\n",
       " 'ACAGCAAGTCCGAACGATACAG',\n",
       " 'ACAGCAAGTCCGACTTGCAAGA',\n",
       " 'ACAGCAAGTCCGTCCCTAGAAC',\n",
       " 'ACAGCAAGTCCTCACAAGCCTA',\n",
       " 'ACAGCAAGTCGATGGCAAACAT',\n",
       " 'ACAGCAAGTCTAAGATGCTGGG',\n",
       " 'ACAGCAAGTCTCAGCCCAAGTT',\n",
       " 'ACAGCAAGTCTGACACAGCGAC',\n",
       " 'ACAGCAAGTCTTACAGCACCGG',\n",
       " 'ACAGCAAGTCTTCGGTTCCCAC',\n",
       " 'ACAGCAAGTCTTGAGTGCTGTG',\n",
       " 'ACCCTTATCTAACAGGCGGGTA',\n",
       " 'ACCCTTATCTAAGCGCCATCGA',\n",
       " 'ACCCTTATCTACTTGAGTCATC',\n",
       " 'ACCCTTATCTATGCGTAGCTGA',\n",
       " 'ACCCTTATCTCTGGGAAATGTT',\n",
       " 'ACCCTTATCTGAATCAGCTTGA',\n",
       " 'ACCCTTATCTGACGGAGAACCT',\n",
       " 'ACCCTTATCTGCAGCTGATGTG',\n",
       " 'ACCCTTATCTGGACGATGTCTG',\n",
       " 'ACCCTTATCTGGGTACAGCTTC',\n",
       " 'ACCCTTATCTGTCCAGATTTCC',\n",
       " 'ACCCTTATCTGTGTAACGCGGT',\n",
       " 'ACCCTTATCTGTTGTACGAGCC',\n",
       " 'ACCCTTATCTTTGAGTGCTGTG',\n",
       " 'ACCTTCAAGCACGCGATCAGTT',\n",
       " 'ACCTTCAAGCAGTGCCTAACAA',\n",
       " 'ACCTTCAAGCCGCGACTTGAGA',\n",
       " 'ACCTTCAAGCGACAATCGAGAA',\n",
       " 'ACCTTCAAGCGCGATGTACGAC',\n",
       " 'ACCTTCAAGCGGTGTTCCCTAC',\n",
       " 'ACCTTCAAGCGTTTGCCACACA',\n",
       " 'ACCTTCAAGCTGATCACTGCAT',\n",
       " 'ACCTTCAAGCTGTCAGGAACAC',\n",
       " 'ACCTTCAAGCTGTGATTCTGTG',\n",
       " 'ACCTTCAAGCTTAGGTAGCAAC',\n",
       " 'ACCTTCAAGCTTCCGCTGATAT',\n",
       " 'ACGTAGCGCAACCCACAGCAGT',\n",
       " 'ACGTAGCGCAACTGCGACAAAG',\n",
       " 'ACGTAGCGCAATAGCCTCCGTA',\n",
       " 'ACGTAGCGCACGTTTATCACAC',\n",
       " 'ACGTAGCGCAGAGATAGTACTG',\n",
       " 'ACGTAGCGCAGCTATTGGTAGG',\n",
       " 'ACGTAGCGCAGCTTTCCAACGG',\n",
       " 'ACGTAGCGCAGGATACTCGTGA',\n",
       " 'ACGTAGCGCAGGTAGAGAGCTC',\n",
       " 'ACGTAGCGCAGTCATAGCGTGT',\n",
       " 'ACGTAGCGCATAGTCCGAGATC',\n",
       " 'ACGTAGCGCATCTCTGACTTGA',\n",
       " 'ACGTAGCGCATGCGAAGCTCAC',\n",
       " 'ACGTAGCGCATGGAGTAACCAT',\n",
       " 'ACTTGCTTCTAAAGGAACAGAC',\n",
       " 'ACTTGCTTCTACCCACAGCAGT',\n",
       " 'ACTTGCTTCTAGTTTGGAGCAT',\n",
       " 'ACTTGCTTCTCAACCACGAGTG',\n",
       " 'ACTTGCTTCTCTTAGCGTGAGT',\n",
       " 'ACTTGCTTCTGAATCAGCTTGA',\n",
       " 'ACTTGCTTCTGTCCACCTAGAC',\n",
       " 'ACTTGCTTCTTGTTTCGGTACA',\n",
       " 'AGAAAGGCGGAGTTTGGAGCAT',\n",
       " 'AGAAAGGCGGCAAGTTAGTGAC',\n",
       " 'AGAAAGGCGGCCTCATAGTAGA',\n",
       " 'AGAAAGGCGGCTCGTAGAGCGT',\n",
       " 'AGAAAGGCGGGAAGGACCTAGT',\n",
       " 'AGAAAGGCGGGATGAAATCGGA',\n",
       " 'AGAAAGGCGGGCTTACAATCGT',\n",
       " 'AGAAAGGCGGTACATCACCTCA',\n",
       " 'AGAAAGGCGGTAGCGTCATATG',\n",
       " 'AGCCATAGGGAAACCTAAGTGG',\n",
       " 'AGCCATAGGGACGCAAGACGAG',\n",
       " 'AGCCATAGGGAGGTCACGACTA',\n",
       " 'AGCCATAGGGATCCGTGGTTGC',\n",
       " 'AGCCATAGGGATTACTCGTGGG',\n",
       " 'AGCCATAGGGCAAGATTACCCG',\n",
       " 'AGCCATAGGGCACCGTATGTTC',\n",
       " 'AGCCATAGGGCCGGTACATTCC',\n",
       " 'AGCCATAGGGCGTCGTGAAATT',\n",
       " 'AGCCATAGGGGATGAACATGTC',\n",
       " 'AGCCATAGGGGGATAAAGAAGG',\n",
       " 'AGCCATAGGGGTCGCTAAGTAA',\n",
       " 'AGCCATAGGGGTTATCAGGGCC',\n",
       " 'AGCCATAGGGTACCATCCCAGG',\n",
       " 'AGCCATAGGGTAGATACATCCC',\n",
       " 'AGCCATAGGGTAGGTTGCACTA',\n",
       " 'AGCGTGTCATAAGTAGCCCGCT',\n",
       " 'AGCGTGTCATACCACGAAGGGA',\n",
       " 'AGCGTGTCATAGTGCCTAACAA',\n",
       " 'AGCGTGTCATCACTGTGTTATG',\n",
       " 'AGCGTGTCATCAGTTATAGGCC',\n",
       " 'AGCGTGTCATCGACTTGCAAGA',\n",
       " 'AGCGTGTCATCGCCCTGCTATA',\n",
       " 'AGCGTGTCATCTGACGAGAAAC',\n",
       " 'AGCGTGTCATGCTCCGGAAATT',\n",
       " 'AGCGTGTCATGGGTTGAACCTA',\n",
       " 'AGCGTGTCATGTTCTCCTACTC',\n",
       " 'AGCGTGTCATTGCGTGCAGACA',\n",
       " 'AGCGTGTCATTGGACGACGATC',\n",
       " 'AGCGTGTCATTTGTCTCTTGGG',\n",
       " 'AGGCTGTGAGAAAGGAACAGAC',\n",
       " 'AGGCTGTGAGAAAGGATCGGCT',\n",
       " 'AGGCTGTGAGAGCTAGGGTGCT',\n",
       " 'AGGCTGTGAGCTTCTAGGCTAC',\n",
       " 'AGGCTGTGAGGAGTGTGGCATA',\n",
       " 'AGGCTGTGAGGATGAACATGTC',\n",
       " 'AGGCTGTGAGTGAACCTGATCG',\n",
       " 'AGGCTGTGAGTGTTTCCGTTGC',\n",
       " 'AGGGATAAACAACGAGCTGTGA',\n",
       " 'AGGGATAAACACGGGTTCTTGT',\n",
       " 'AGGGATAAACAGGAACTGTCGT',\n",
       " 'AGGGATAAACATCCGTGGTTGC',\n",
       " 'AGGGATAAACCAAGATTACCCG',\n",
       " 'AGGGATAAACCACAAATCAACC',\n",
       " 'AGGGATAAACCACCGAAGGTCA',\n",
       " 'AGGGATAAACCGATGTCCAAGT',\n",
       " 'AGGGATAAACGAGAACGTCATA',\n",
       " 'AGGGATAAACGATGTTTCACCA',\n",
       " 'AGGGATAAACGCGAACGTAGAC',\n",
       " 'AGGGATAAACGCGTGCTACCAA',\n",
       " 'AGGGATAAACGCTATTGGTAGG',\n",
       " 'AGGGATAAACGGGCTACAGTTC',\n",
       " 'AGGGATAAACTGCGTCGAGTAC',\n",
       " 'AGGGATCGTAACTAACGGTATC',\n",
       " 'AGGGATCGTAACTGGATATGCT',\n",
       " 'AGGGATCGTACCTATTGAAGCC',\n",
       " 'AGGGATCGTACGTTTATCACAC',\n",
       " 'AGGGATCGTAGAACACGTTCCA',\n",
       " 'AGGGATCGTAGACATCGAGTAG',\n",
       " 'AGGGATCGTAGACATTTGACAG',\n",
       " 'AGGGATCGTAGAGAACGTCATA',\n",
       " 'AGGGATCGTAGAGAGGTCCACT',\n",
       " 'AGGGATCGTATACTAGGGTTAG',\n",
       " 'AGGGATCGTATAGAGAGGCTGC',\n",
       " 'AGGGATCGTATCGGTACTAACT',\n",
       " 'ATACTCGCTTAAAGCGGGAGCT',\n",
       " 'ATACTCGCTTAAATTGAGGAGG',\n",
       " 'ATACTCGCTTACCACGAAGGGA',\n",
       " 'ATACTCGCTTAGGTTATCGATC',\n",
       " 'ATACTCGCTTATAACGCAAGTC',\n",
       " 'ATACTCGCTTATAGGTTTCGCC',\n",
       " 'ATACTCGCTTCGACATCTGGAT',\n",
       " 'ATACTCGCTTCGTAGGATTCTG',\n",
       " 'ATACTCGCTTCTACATTCACCA',\n",
       " 'ATACTCGCTTCTTCATGTACCT',\n",
       " 'ATACTCGCTTGCTAAGGATTAG',\n",
       " 'ATACTCGCTTGCTCCGGAAATT',\n",
       " 'ATACTCGCTTGGGCGTATTTGA',\n",
       " 'ATACTCGCTTGGTGTCGGTTGT',\n",
       " 'ATACTCGCTTGTCGCTAAGTAA',\n",
       " 'ATACTCGCTTGTGTTAGGCAAT',\n",
       " 'ATACTCGCTTTGTCCTGCGGTA',\n",
       " 'ATCCCTCCGAAGGAGTCGGAGA',\n",
       " 'ATCCCTCCGACCCAATACGTGG',\n",
       " 'ATCCCTCCGAGGAGTGTTGGAA',\n",
       " 'ATCCCTCCGATACCCAATGAAC',\n",
       " 'ATGCAGGTAAAAACTAGCCCTA',\n",
       " 'ATGCAGGTAAAACTAACGTCGA',\n",
       " 'ATGCAGGTAAACATCTAAGGAG',\n",
       " 'ATGCAGGTAAAGATTGGACAAG',\n",
       " 'ATGCAGGTAAAGGTGTGACCGT',\n",
       " 'ATGCAGGTAACAACCACGAGTG',\n",
       " 'ATGCAGGTAACTCGTAGAGCGT',\n",
       " 'ATGCAGGTAAGCACACTCTCCT',\n",
       " 'ATGCAGGTAAGTTACAGAACGC',\n",
       " 'ATGCAGGTAATGAGCTAGACGC',\n",
       " 'ATGTAACGACAATGGAGCAACA',\n",
       " 'ATGTAACGACATCAGTCAAGGA',\n",
       " 'ATGTAACGACATGGACGATCGT',\n",
       " 'ATGTAACGACCAAATGACGGGC',\n",
       " 'ATGTAACGACCATACCCGGTTC',\n",
       " 'ATGTAACGACCCACGTCACTTA',\n",
       " 'ATGTAACGACCGACTTGCAAGA',\n",
       " 'ATGTAACGACCTAATTTGTGGG',\n",
       " 'ATGTAACGACGGGCGTATTTGA',\n",
       " 'ATGTTCACGTTGTGATTCTGTG',\n",
       " 'ATGTTCACGTTTGAGCCCGGAT',\n",
       " 'CAATGATAGCCGACGCCTTATG',\n",
       " 'CAATGATAGCCTATCCCATCCG',\n",
       " 'CAATGATAGCCTTTAGCTGACT',\n",
       " 'CAATGATAGCGAGGGTGATTGC',\n",
       " 'CAATGATAGCGCGGAACTAGAG',\n",
       " 'CAATGATAGCGCTTCTCACAAG',\n",
       " 'CAATGATAGCTCACAGTGATAG',\n",
       " 'CACCCAGACTCACCGTATGTTC',\n",
       " 'CACCCAGACTCCATTACGATTC',\n",
       " 'CACCCAGACTGGGCTAGGAGAT',\n",
       " 'CACCCAGACTTCAGCCCAAGTT',\n",
       " 'CACCCAGACTTTACTTACACCC',\n",
       " 'CAGGAAAGCACAGCTCAGGACA',\n",
       " 'CAGGAAAGCACCTCGTCAGAAC',\n",
       " 'CAGGAAAGCACCTTCCACCTGT',\n",
       " 'CAGGAAAGCAGATCAGTAGGAT',\n",
       " 'CAGGAAAGCATAAACGCTCCAG',\n",
       " 'CAGGAAAGCATCGATCCATGGG',\n",
       " 'CAGGAAAGCATGCGTTGATGTG',\n",
       " 'CAGGAAAGCATTCTAACGCTTC',\n",
       " 'CAGGAAAGCATTTCTGCAGACT',\n",
       " 'CAGGTGAACCCTGGGCTTGGTA',\n",
       " 'CAGGTGAACCCTGTTGGTCCTT',\n",
       " 'CAGGTGAACCGAATCAGCTTGA',\n",
       " 'CAGGTGAACCGAGGAATCACGG',\n",
       " 'CAGGTGAACCGAGGCATTTGCA',\n",
       " 'CAGGTGAACCGCGTAACCAGTA',\n",
       " 'CAGGTGAACCGGATACTCGTGA',\n",
       " 'CAGGTGAACCGGCTGATATGCT',\n",
       " 'CAGGTGAACCGTGTGAGCATGC',\n",
       " 'CAGGTGAACCGTTGGGCTCAAG',\n",
       " 'CAGGTGAACCTATCGCCGGGTA',\n",
       " 'CAGGTGAACCTCCTTAGTCGGG',\n",
       " 'CAGGTGAACCTCTGTACGCTGA',\n",
       " 'CAGGTGAACCTGCTATCGGACG',\n",
       " 'CATCCACGGAAAAGCCCACGAC',\n",
       " 'CATCCACGGAAAGACACCAACC',\n",
       " 'CATCCACGGAACAACAGCGGAT',\n",
       " 'CATCCACGGAACCCGATGGCTA',\n",
       " 'CATCCACGGAAGCCTCACCATC',\n",
       " 'CATCCACGGAAGGCTGCGTCTA',\n",
       " 'CATCCACGGAAGTTTGGAGCAT',\n",
       " 'CATCCACGGAATGTGCATCTGG',\n",
       " 'CATCCACGGACACTGTGTTATG',\n",
       " 'CATCCACGGACCTCGTCAGAAC',\n",
       " 'CATCCACGGACCTTATATGTCC',\n",
       " 'CATCCACGGACGTTTATCACAC',\n",
       " 'CATCCACGGACTTAGACGTCTT',\n",
       " 'CATCCACGGAGATCTACGCCGT',\n",
       " 'CATCCACGGAGGAGTTTGCACA',\n",
       " 'CATCCACGGATGACCACCAATT',\n",
       " 'CATCCACGGATTATGGTAGACC',\n",
       " 'CATCCGGAACATAGGTTTCGCC',\n",
       " 'CATCCGGAACATGGACGATCGT',\n",
       " 'CATCCGGAACCACTTCTCCTAC',\n",
       " 'CATCCGGAACCAGCTCAGGACA',\n",
       " 'CATCCGGAACCATCTCGACAAT',\n",
       " 'CATCCGGAACCTATCCCATCCG',\n",
       " 'CATCCGGAACCTTCTAGGCTAC',\n",
       " 'CATCCGGAACCTTTAGCTGACT',\n",
       " 'CATCCGGAACGATCCATTCGTG',\n",
       " 'CATCCGGAACGATGGCAAACAT',\n",
       " 'CATCCGGAACGCATGTATCCTT',\n",
       " 'CATCCGGAACGGTGGCCGTATA',\n",
       " 'CATCCGGAACGTACATAGATCC',\n",
       " 'CATGAACGTGACCACGAAGGGA',\n",
       " 'CATGAACGTGCGGAACATTGTA',\n",
       " 'CCAAGGTGATAAGACACCAACC',\n",
       " 'CCAAGGTGATAAGCGGGCTTGA',\n",
       " 'CCAAGGTGATAAGGGCAAATCA',\n",
       " 'CCAAGGTGATAGTGTCAGTCAA',\n",
       " 'CCAAGGTGATATGGACACGTTC',\n",
       " 'CCAAGGTGATCCTGGGTTAATC',\n",
       " 'CCAAGGTGATCTCACAAGCCTA',\n",
       " 'CCAAGGTGATCTCATGTAGTCT',\n",
       " 'CCAAGGTGATCTTTGGGACGGT',\n",
       " 'CCAAGGTGATGATTCGCGTGCA',\n",
       " 'CCAAGGTGATGATTTCCGGACT',\n",
       " 'CCAAGGTGATTTTACCACGCCA',\n",
       " 'CCAAGGTGATTTTACCCGTCGA',\n",
       " 'CCACAAATTCAACAGGCGGGTA',\n",
       " 'CCACAAATTCACCAATCGCTTG',\n",
       " 'CCACAAATTCACGCCGTGACTT',\n",
       " 'CCACAAATTCAGATCTGTGACG',\n",
       " 'CCACAAATTCAGGAGTCGGAGA',\n",
       " 'CCACAAATTCAGGTTATCGATC',\n",
       " 'CCACAAATTCCAAGATTACCCG',\n",
       " 'CCACAAATTCCCTTTCCGTATC',\n",
       " 'CCACAAATTCGACAATAGGAGA',\n",
       " 'CCACAAATTCGAGAACGTCATA',\n",
       " 'CCACAAATTCGCGTAACCAGTA',\n",
       " 'CCACAAATTCGGTTGTTCCATC',\n",
       " 'CCACAAATTCGTGTTAGGCAAT',\n",
       " 'CCGCTACTCACAGAATGCCTGA',\n",
       " 'CCGCTACTCAGATACTTGCTGG',\n",
       " 'CCGCTACTCAGATGAACATGTC',\n",
       " 'CCGCTACTCAGGCATAATGATC',\n",
       " 'CCGCTACTCATGGTATGGAGGT',\n",
       " 'CCGCTACTCATTGTTCCTCTGC',\n",
       " 'CCGGGATTTCATTTGCCCAGCT',\n",
       " 'CCGGGATTTCGCGCAGTTTATA',\n",
       " 'CCGGGATTTCGGGCATAAACGT',\n",
       " 'CCGGGATTTCGTCAGGGTACTT',\n",
       " 'CCGGGATTTCTCGGTACTAACT',\n",
       " 'CCGGGATTTCTGCCGTCTCAAC',\n",
       " 'CCGGGATTTCTGCTAATGACGT',\n",
       " 'CCGGGATTTCTTAGGGACCCTC',\n",
       " 'CCTCGATATGGGCTGATATGCT',\n",
       " 'CCTCGATATGGGTAATACTCGG',\n",
       " 'CCTTACTCAGACATGCCGTGCT',\n",
       " 'CCTTACTCAGACTGCACGCCAT',\n",
       " 'CCTTACTCAGATGACCACCAAT',\n",
       " 'CCTTACTCAGCACCGTATGTTC',\n",
       " 'CCTTACTCAGCCGGCATATGAG',\n",
       " 'CCTTACTCAGCCTGATAACCAA',\n",
       " 'CCTTACTCAGGAGTGTGGCATA',\n",
       " 'CCTTACTCAGGGAAGCTTATCC',\n",
       " 'CCTTACTCAGGGCCATCCACAA',\n",
       " 'CCTTACTCAGGGTCGTCAATCA',\n",
       " 'CCTTACTCAGGGTGTTCCCTAC',\n",
       " 'CCTTACTCAGTAGTCCACCAGT',\n",
       " 'CCTTACTCAGTCATTCACGAAG',\n",
       " 'CCTTACTCAGTCCAGACGACTT',\n",
       " 'CCTTACTCAGTGCCGTGAGACA',\n",
       " 'CCTTACTCAGTGGTATGGAGGT',\n",
       " 'CGCATCTCTGATGCGTATTCCC',\n",
       " 'CGCATCTCTGCCCATCTGGAGA',\n",
       " 'CGCATCTCTGGCGTAACCAGTA',\n",
       " 'CGCATCTCTGTACCGCTTCCAC',\n",
       " 'CGCCTTTACGACCCACATGCTT',\n",
       " 'CGCCTTTACGACTGCGACAAAG',\n",
       " 'CGCCTTTACGAGGGATCCATTA',\n",
       " 'CGCCTTTACGATCGTACCCGTT',\n",
       " 'CGCCTTTACGCACCGAAGGTCA',\n",
       " 'CGCCTTTACGCAGCAACCGATC',\n",
       " 'CGCCTTTACGCCCATGACGGAA',\n",
       " 'CGCCTTTACGCTCAGATAAGGG',\n",
       " 'CGCCTTTACGGCAGAGAGAGAA',\n",
       " 'CGCCTTTACGGGTAGAGAGCTC',\n",
       " 'CGCCTTTACGTACTAAGCAGTG',\n",
       " 'CGCCTTTACGTATCAAAGGGTC',\n",
       " 'CGCCTTTACGTCATCCGCGACA',\n",
       " 'CGCCTTTACGTCCCTAACCATA',\n",
       " 'CGCCTTTACGTTACTTACACCC',\n",
       " 'CGTATCTTGTCAAATGACGGGC',\n",
       " 'CGTATCTTGTCATTAGTGTACC',\n",
       " 'CGTATCTTGTCCACGTCACTTA',\n",
       " 'CGTATCTTGTCCCATCTGGAGA',\n",
       " 'CGTATCTTGTCGAGAATCGACG',\n",
       " 'CGTATCTTGTCGTCAATAGCTG',\n",
       " 'CGTATCTTGTGAATCAGCTTGA',\n",
       " 'CGTATCTTGTTGGAACAGTCTG',\n",
       " 'CGTATCTTGTTGGCTTCTACCA',\n",
       " 'CGTATCTTGTTTATGGTAGACC',\n",
       " 'CGTCAAGCAGACCCACAGCAGT',\n",
       " 'CGTCAAGCAGACCCGAACTCAT',\n",
       " 'CGTCAAGCAGCACCGAAGGTCA',\n",
       " 'CGTCAAGCAGCACCGTATGTTC',\n",
       " 'CGTCAAGCAGCGTACTTCCTCG',\n",
       " 'CGTCAAGCAGGAAGTTCTTAGG',\n",
       " 'CGTCAAGCAGGGTAAATCTCCC',\n",
       " 'CGTCAAGCAGTCAGCATTTAGG',\n",
       " 'CGTCAAGCAGTGGGATTGACAG',\n",
       " 'CGTCAAGCAGTTAAACGCGAAG',\n",
       " 'CGTCAAGCAGTTAGGATGCGCG',\n",
       " 'CGTCTGGACTAAACGGATCAGT',\n",
       " 'CGTCTGGACTAACTTCGACCAG',\n",
       " 'CGTCTGGACTACCAGTTCACCC',\n",
       " 'CGTCTGGACTACCCGAACTCAT',\n",
       " 'CGTCTGGACTACTGCGACAAAG',\n",
       " 'CGTCTGGACTAGCGGCTGAAAC',\n",
       " 'CGTCTGGACTAGTCTGTGTAGA',\n",
       " 'CGTCTGGACTCAAGTTAGTGAC',\n",
       " 'CGTCTGGACTCAGTTATAGGCC',\n",
       " 'CGTCTGGACTCGTTCGTTTATG',\n",
       " 'CGTCTGGACTGCTTCATGAACC',\n",
       " 'CGTCTGGACTGGGCTAGGAGAT',\n",
       " 'CGTCTGGACTGTGTCATGAAAG',\n",
       " 'CGTCTGGACTTCGATCCATGGG',\n",
       " 'CGTTGCCCAAACGTCCGTAATT',\n",
       " 'CGTTGCCCAAATAGGTGGTACC',\n",
       " 'CGTTGCCCAAATCTCGCCACTT',\n",
       " 'CGTTGCCCAACAAGATTACCCG',\n",
       " 'CGTTGCCCAACAGATCGAGGGA',\n",
       " 'CGTTGCCCAACAGTTGACGCTC',\n",
       " 'CGTTGCCCAACCAGCTTTCCGT',\n",
       " 'CGTTGCCCAACCTCGTCAGAAC',\n",
       " 'CGTTGCCCAACTCGTAGAGCGT',\n",
       " 'CGTTGCCCAACTTCGCGTAACG',\n",
       " 'CGTTGCCCAAGAGAGGTCCACT',\n",
       " 'CGTTGCCCAAGCGGATAGTAGG',\n",
       " 'CGTTGCCCAAGGAACGTCAGTT',\n",
       " 'CGTTGCCCAAGTCAGTAGTCTG',\n",
       " 'CGTTGCCCAAGTCGACAGATAG',\n",
       " 'CGTTGCCCAATCAGCATTTAGG',\n",
       " 'CGTTGCCCAATCCTGATTTAGC',\n",
       " 'CGTTGCCCAATGCGTCGAGTAC',\n",
       " 'CGTTGCCCAATTCGGTTCCCAC',\n",
       " 'CTAGTGTTGCAATCGAACACGT',\n",
       " 'CTAGTGTTGCAGATTTGAGTCC',\n",
       " 'CTAGTGTTGCAGCAGGTATGTT',\n",
       " 'CTAGTGTTGCATTAGGAGGTCT',\n",
       " 'CTAGTGTTGCCTTGCTGCCTAT',\n",
       " 'CTAGTGTTGCGACAATCGAGAA',\n",
       " 'CTAGTGTTGCGGACCGACAATG',\n",
       " 'CTAGTGTTGCGTGTCATGAAAG',\n",
       " 'CTAGTGTTGCGTTATCAGGGCC',\n",
       " 'CTAGTGTTGCTACTAAGCAGTG',\n",
       " 'CTAGTGTTGCTGGGTTTGTACT',\n",
       " 'CTAGTGTTGCTGGTGAGGTGAT',\n",
       " 'CTATGTCCAGAAGTTGTGCTAC',\n",
       " 'CTATGTCCAGCTCCCGTGTAAG',\n",
       " 'CTATGTCCAGCTTGCGGGTGTT',\n",
       " 'CTATGTCCAGGAGGAATCACGG',\n",
       " 'CTATGTCCAGGCACACTCTCCT',\n",
       " 'CTATGTCCAGGGGTACAGCTTC',\n",
       " 'CTATTTGCCTACATCTCTCCCG',\n",
       " 'CTATTTGCCTACCACGAAGGGA',\n",
       " 'CTATTTGCCTATCGAGTTAGGC',\n",
       " 'CTATTTGCCTCATGTTCTGTGA',\n",
       " 'CTATTTGCCTCGACGAAGAAGA',\n",
       " 'CTATTTGCCTCGATTGGCGATA',\n",
       " 'CTATTTGCCTCGTTCGTTTATG',\n",
       " 'CTATTTGCCTCTAGACGACCTG',\n",
       " 'CTATTTGCCTGTCCTATGTCCG',\n",
       " 'CTATTTGCCTTAGACTGGACCA',\n",
       " 'CTATTTGCCTTCGCTTCCGGTT',\n",
       " 'CTATTTGCCTTCGTCACTTAAG',\n",
       " 'CTCAACCAGGAAGTAGCCCGCT',\n",
       " 'CTCAACCAGGACGTCCGTAATT',\n",
       " 'CTCAACCAGGATTGCCTTCTTC',\n",
       " 'CTCAACCAGGCGACGCCTTATG',\n",
       " 'CTCAACCAGGCGCAAGTAAAGC',\n",
       " 'CTCAACCAGGCTATCCCATCCG',\n",
       " 'CTCAACCAGGCTTGCGGGTGTT',\n",
       " 'CTCAACCAGGGATGTTTCACCA',\n",
       " 'CTCAACCAGGGCCTAAACGAAG',\n",
       " 'CTCAACCAGGGCTATTGGTAGG',\n",
       " 'CTCAACCAGGTGAAGTTCCGAG',\n",
       " 'CTCAACCAGGTGACCACCAATT',\n",
       " 'CTCAACCAGGTGTCAGCACAAT',\n",
       " 'CTCAACCAGGTGTCCTGCGGTA',\n",
       " 'CTCAACCAGGTGTGACAGGTTC',\n",
       " 'CTCAACCAGGTGTTCGGATTAG',\n",
       " 'CTCAACCAGGTTGACAAGTCCT',\n",
       " 'CTCAACCAGGTTTCGAAGAAGG',\n",
       " 'CTGGTTCTCTAACCACCTAAAG',\n",
       " 'CTGGTTCTCTAGGGATCCATTA',\n",
       " 'CTGGTTCTCTAGTCCTGGTACT',\n",
       " 'CTGGTTCTCTATAGCTGTGTCT',\n",
       " 'CTGGTTCTCTATAGGTTTCGCC',\n",
       " 'CTGGTTCTCTCAGTTGACGCTC',\n",
       " 'CTGGTTCTCTCATGTCCGAGTC',\n",
       " 'CTGGTTCTCTCTAAATGTTCGG',\n",
       " 'CTGGTTCTCTGAGACAATGACG',\n",
       " 'CTGGTTCTCTGCTCAGCGTTTA',\n",
       " 'CTGGTTCTCTGGGCTACAGTTC',\n",
       " 'CTGGTTCTCTGTGTAACGCGGT',\n",
       " 'CTGGTTCTCTTCGTGGAATTAC',\n",
       " 'CTGGTTCTCTTGACCACCAATT',\n",
       " 'CTGGTTCTCTTGTCAGGAACAC',\n",
       " 'CTGGTTCTCTTTAGGGACCCTC',\n",
       " 'CTTCACAGTCAATCCCGCGCTT',\n",
       " 'CTTCACAGTCACATCTCTCCCG',\n",
       " 'CTTCACAGTCACTCTTCCTCAT',\n",
       " 'CTTCACAGTCCGCGAGAGCTAT',\n",
       " 'CTTCACAGTCCTTGCGGGTGTT',\n",
       " 'CTTCACAGTCGAACACGTTCCA',\n",
       " 'CTTCACAGTCGCCCGTGTTGAT',\n",
       " 'CTTCACAGTCGTGCTTTCAACA',\n",
       " 'CTTCACAGTCTAGGTAAGACCA',\n",
       " 'CTTCACAGTCTATCGCCGGGTA',\n",
       " 'CTTTCGCGTGATGCGTAGCTGA',\n",
       " 'CTTTCGCGTGCCTCGTCAGAAC',\n",
       " 'CTTTCGCGTGCGCCGAATTACT',\n",
       " 'CTTTCGCGTGTGGTGAGGTGAT',\n",
       " 'GACCAATCCTAAAGCACTAGCG',\n",
       " 'GACCAATCCTAGGCGGCATTAC',\n",
       " 'GACCAATCCTCCAGCTTTCCGT',\n",
       " 'GACCAATCCTCCTAATCCTGAG',\n",
       " 'GACCAATCCTCGACGAGAGATC',\n",
       " 'GACCAATCCTGCTGGTAGATAA',\n",
       " 'GACCAATCCTGTCCACCTAGAC',\n",
       " 'GACCAATCCTGTTGGGCTCAAG',\n",
       " 'GACCAATCCTTTTACCCGTCGA',\n",
       " 'GACCCTCAAAACGCAAGACGAG',\n",
       " 'GACCCTCAAAAGGTCCCAATCG',\n",
       " 'GACCCTCAAAAGTGTCAGTCAA',\n",
       " 'GACCCTCAAACAACCGTACATC',\n",
       " 'GACCCTCAAACGCCACCATTTC',\n",
       " 'GACCCTCAAAGCACACTCTCCT',\n",
       " 'GACCCTCAAAGTCGACAGATAG',\n",
       " 'GACCCTCAAATATAAGTGCTCC',\n",
       " 'GACCCTCAAATGCAGATCGGCT',\n",
       " 'GACCCTCAAATTTCCACGCAGT',\n",
       " 'GACCTTGATGAGGAACGCCGAT',\n",
       " 'GACCTTGATGCTGCTGGGTTCA',\n",
       " 'GACCTTGATGGGCCATCCACAA',\n",
       " 'GAGCACCACTAATCGCTGGATG',\n",
       " 'GAGCACCACTACACGCGTGAAC',\n",
       " 'GAGCACCACTAGATTGGACAAG',\n",
       " 'GAGCACCACTAGGAACTGTCGT',\n",
       " 'GAGCACCACTCACGCAAGGGAA',\n",
       " 'GAGCACCACTCATCTCGACAAT',\n",
       " 'GAGCACCACTCTAAGAGCTCAA',\n",
       " 'GAGCACCACTGACAATCGAGAA',\n",
       " 'GAGCACCACTGAGAGGTCCACT',\n",
       " 'GAGCACCACTGCGCCTGTATAA',\n",
       " 'GAGCACCACTGTACTGGACCCT',\n",
       " 'GAGCACCACTTACGGCTTTCCT',\n",
       " 'GAGCACCACTTGCTAGCCTATA',\n",
       " 'GAGCACCACTTTACTTACACCC',\n",
       " 'GAGGAAGTTGACTGCGACAAAG',\n",
       " 'GAGGAAGTTGCAAACACCACAT',\n",
       " 'GAGGAAGTTGCACCATGCCATT',\n",
       " 'GAGGAAGTTGCACCATTTACCC',\n",
       " 'GAGGAAGTTGCCGGAGAATACC',\n",
       " 'GAGGAAGTTGCTTAGCAAAGAG',\n",
       " 'GAGGAAGTTGGATTCCATTTGG',\n",
       " 'GAGGAAGTTGTGCGTTGATGTG',\n",
       " 'GAGGAAGTTGTTGAGTGCTGTG',\n",
       " 'GAGGTAATCTAATCGCTGGATG',\n",
       " 'GAGGTAATCTCACAACCATGAT',\n",
       " 'GAGGTAATCTCATGTTCTGTGA',\n",
       " 'GAGGTAATCTCTCAGATAAGGG',\n",
       " 'GAGGTAATCTGAAGTTCTTAGG',\n",
       " 'GAGGTAATCTGACGGCATAATG',\n",
       " 'GAGGTAATCTGGCGGGATATTC',\n",
       " 'GAGGTAATCTGGTAATACTCGG',\n",
       " 'GAGGTAATCTGGTTGTCCTCTG',\n",
       " 'GAGGTAATCTGTGAAGCTTATG',\n",
       " 'GAGGTAATCTTAACAGTGCCTT',\n",
       " 'GAGGTAATCTTCATCCGCGACA',\n",
       " 'GAGGTAATCTTTGCTGGATCGT',\n",
       " 'GAGGTAATCTTTTCCACGCAGT',\n",
       " 'GAGTGTGGTTATCATGCGTAGG',\n",
       " 'GAGTGTGGTTCACACCTATATC',\n",
       " 'GAGTGTGGTTCACCTGTAAGAC',\n",
       " 'GAGTGTGGTTCCGGTACATTCC',\n",
       " 'GAGTGTGGTTCTAAGAGCTCAA',\n",
       " 'GAGTGTGGTTGAATCGGTGTTC',\n",
       " 'GAGTGTGGTTGGTTATGAGTCG',\n",
       " 'GAGTGTGGTTTAGGCAGATGGA',\n",
       " 'GAGTGTGGTTTAGGTTGCACTA',\n",
       " 'GAGTGTGGTTTCGACCAGATCT',\n",
       " 'GAGTGTGGTTTCTTTGACGCTG',\n",
       " 'GATCGGGTGAACCGTCCCATTC',\n",
       " 'GATCGGGTGAAGGTTATCGATC',\n",
       " 'GATCGGGTGAATTGCCTTCTTC',\n",
       " 'GATCGGGTGAATTTAGGGCGGG',\n",
       " 'GATCGGGTGACTCGTAGAGCGT',\n",
       " 'GATCGGGTGAGTTCTTGACAAG',\n",
       " 'GATCGGGTGATACACTCCACAA',\n",
       " 'GATCGGGTGATCTTCCTGCAAC',\n",
       " 'GATCGGGTGATTTAACGCCTCT',\n",
       " 'GCCAGTGTTGAAGTGTCGTGGA',\n",
       " 'GCCAGTGTTGATAGGTGGTACC',\n",
       " 'GCCAGTGTTGATCATTGAGGCT',\n",
       " 'GCCAGTGTTGCACGCAAGGGAA',\n",
       " 'GCCAGTGTTGCAGCTCAGGACA',\n",
       " 'GCCAGTGTTGCTGGGAAATGTT',\n",
       " 'GCCAGTGTTGGATAGGTGTAGC',\n",
       " 'GCCAGTGTTGGCTAGTCAAGAG',\n",
       " 'GCCAGTGTTGGGTCGACAGAGA',\n",
       " 'GCCAGTGTTGGTGTCATGAAAG',\n",
       " 'GCCAGTGTTGTAGTCCGAGATC',\n",
       " 'GCCAGTGTTGTCTAATGCCTGC',\n",
       " 'GCCAGTGTTGTGGAACAGTCTG',\n",
       " 'GCTCTTGGAAAATCACGCTTCG',\n",
       " 'GCTCTTGGAAAGTGCCTAACAA',\n",
       " 'GCTCTTGGAACACCTTACTAGA',\n",
       " 'GCTCTTGGAACGCCGAATTACT',\n",
       " 'GCTCTTGGAACGCGTTAATATC',\n",
       " 'GCTCTTGGAACGTCGTGAAATT',\n",
       " 'GCTCTTGGAAGCCACCTGATAC',\n",
       " 'GCTCTTGGAAGCTCTTTCAGAA',\n",
       " 'GCTCTTGGAAGGAACGGTCTCA',\n",
       " 'GCTCTTGGAAGGCGACCTATGT',\n",
       " 'GCTCTTGGAAGTGCTTTCAACA',\n",
       " 'GCTCTTGGAAGTGTAACGCGGT',\n",
       " 'GCTCTTGGAATAACCCAACGAG',\n",
       " 'GCTCTTGGAATAGGCTAGTTTG',\n",
       " 'GCTCTTGGAATTTCGAAGAAGG',\n",
       " 'GCTGATCTTCAACATGAAGCGC',\n",
       " 'GCTGATCTTCACAGGGCCTCTT',\n",
       " 'GCTGATCTTCATAGCCTCCGTA',\n",
       " 'GCTGATCTTCATGCGTAGCTGA',\n",
       " 'GCTGATCTTCCCATCCTATCGT',\n",
       " 'GCTGATCTTCCGAAGATCACTC',\n",
       " 'GCTGATCTTCCGAAGGTTCGAT',\n",
       " 'GCTGATCTTCCTGGGAAATGTT',\n",
       " 'GCTGATCTTCCTTCTAGGCTAC',\n",
       " 'GCTGATCTTCCTTTAACTGCGC',\n",
       " 'GCTGATCTTCGACTTACTGCCG',\n",
       " 'GCTGATCTTCGAGCACATCTCC',\n",
       " 'GCTGATCTTCTGTTCTCTCTGG',\n",
       " 'GCTGGTAGGAAAAGGATCGGCT',\n",
       " 'GCTGGTAGGAACAACAGCGGAT',\n",
       " 'GCTGGTAGGACCACAGCGAAAC',\n",
       " 'GCTGGTAGGACCATCCTATCGT',\n",
       " 'GCTGGTAGGACCTTTCGGCAGT',\n",
       " 'GCTGGTAGGACGGCGACTAACA',\n",
       " 'GCTGGTAGGAGATACTTGCTGG',\n",
       " 'GCTGGTAGGAGGAGTTTGCACA',\n",
       " 'GCTGGTAGGAGGATAAAGAAGG',\n",
       " 'GCTGGTAGGATAGTCCGAGATC',\n",
       " 'GCTGGTAGGATATACGAACCCG',\n",
       " 'GCTGGTAGGATCCACATCACTG',\n",
       " 'GCTGGTAGGATCCAGACGACTT',\n",
       " 'GCTGGTAGGATCTATGCGTACC',\n",
       " 'GCTGGTAGGATTCGGTTCCCAC',\n",
       " 'GCTGGTAGGATTGCATGTACGC',\n",
       " 'GCTTCGCATTCACGCAAGGGAA',\n",
       " 'GCTTCGCATTCAGCGAAGACCT',\n",
       " 'GCTTCGCATTCTTCCTGCTGTC',\n",
       " 'GCTTCGCATTGGATAAAGAAGG',\n",
       " 'GCTTCGCATTGGTTTAGTCGTA',\n",
       " 'GCTTCGCATTTAAGATGCTGGG',\n",
       " 'GCTTCGCATTTTGCTGGATCGT',\n",
       " 'GCTTCGCATTTTGGTGATGTCT',\n",
       " 'GCTTCGCATTTTTAACGCCTCT',\n",
       " 'GGAATTCCGACAAGTGTCTGTA',\n",
       " 'GGAATTCCGACATGTTCTGTGA',\n",
       " 'GGAATTCCGACGCGACTTGAGA',\n",
       " 'GGAATTCCGACGTGCAACACAA',\n",
       " 'GGAATTCCGACTAAGGTCAGAA',\n",
       " 'GGAATTCCGAGCTACAATGTGG',\n",
       " 'GGAATTCCGAGGTGTTCCCTAC',\n",
       " 'GGAATTCCGATGAACCATCGTA',\n",
       " 'GGAATTCCGATTACTTACACCC',\n",
       " 'GGAGCCATTGAGATACCTGGAG',\n",
       " 'GGAGCCATTGAGGTGCAAGGTA',\n",
       " 'GGAGCCATTGATGTCGCCTGAA',\n",
       " 'GGAGCCATTGCCACCGCCTTAA',\n",
       " 'GGAGCCATTGCGCAAGTAAAGC',\n",
       " 'GGAGCCATTGCGTGTACTGAAT',\n",
       " 'GGAGCCATTGCTTAGACGTCTT',\n",
       " 'GGAGCCATTGGAGGAATCACGG',\n",
       " 'GGAGCCATTGGATGAACATGTC',\n",
       " 'GGAGCCATTGGATGGCAAACAT',\n",
       " 'GGAGCCATTGGCGTAACCAGTA',\n",
       " 'GGAGCCATTGGGCATAATGATC',\n",
       " 'GGAGCCATTGTCCGTAAGGTTC',\n",
       " 'GGCCTATGTTGATACGCCGTAG',\n",
       " 'GGCCTATGTTGATTCCATTTGG',\n",
       " 'GGCCTATGTTTAGATCCACGTA',\n",
       " 'GGTGACATACAACTAACGTCGA',\n",
       " 'GGTGACATACCAATCTGACGTA',\n",
       " 'GGTGACATACCGAAACTTACGG',\n",
       " 'GGTGACATACCGTGATGAGGTA',\n",
       " 'GGTGACATACCTCGTAGAGCGT',\n",
       " 'GGTGACATACCTTGGATGATGC',\n",
       " 'GGTGACATACCTTTGGGACGGT',\n",
       " 'GGTGACATACGAATCGGTGTTC',\n",
       " 'GGTGACATACGCTGATCCACCT',\n",
       " 'GGTGACATACTACTAGGGTTAG',\n",
       " 'GGTGACATACTTTCCACGCAGT',\n",
       " 'GTGGGCAATAACAACAGCGGAT',\n",
       " 'GTGGGCAATACTTGCGGGTGTT',\n",
       " 'GTGGGCAATAGGCTGATATGCT',\n",
       " 'GTGGGCAATAGTGCCTTTAAGC',\n",
       " 'GTGGTTGCCAAACATGAAGCGC',\n",
       " 'GTGGTTGCCAAAGCGGGCTTGA',\n",
       " 'GTGGTTGCCAAGCAGTTGGTGG',\n",
       " 'GTGGTTGCCAAGGCGACTACAC',\n",
       " 'GTGGTTGCCAATCAGTCAAGGA',\n",
       " 'GTGGTTGCCACACAAATCAACC',\n",
       " 'GTGGTTGCCACCGGTACATTCC',\n",
       " 'GTGGTTGCCACCTGATAACCAA',\n",
       " 'GTGGTTGCCACGCGATACAAAT',\n",
       " 'GTGGTTGCCACTGGGAAATGTT',\n",
       " 'GTGGTTGCCACTTAGCAAAGAG',\n",
       " 'GTGGTTGCCAGATCCATTCGTG',\n",
       " 'GTGGTTGCCAGATTCGCGTGCA',\n",
       " 'GTGGTTGCCAGCTTTCCAACGG',\n",
       " 'GTGGTTGCCAGTTGCGAATGGG',\n",
       " 'GTGGTTGCCATCGCTTCCGGTT',\n",
       " 'GTGGTTGCCATTGAGTGCTGTG',\n",
       " 'GTGGTTGCCATTTGTGTTACCG',\n",
       " 'GTGTGACCCTAACCATGCATGA',\n",
       " 'GTGTGACCCTATGTCTAACGCC',\n",
       " 'GTGTGACCCTCTGTCTACACGA',\n",
       " 'GTGTGACCCTGACGGCATAATG',\n",
       " 'GTGTGACCCTGCTTGATCCCAG',\n",
       " 'GTGTGACCCTGTCACGCCCAAT',\n",
       " 'GTGTGACCCTTAAACGCTCCAG',\n",
       " 'GTGTGACCCTTACTAGGGTTAG',\n",
       " 'GTGTGACCCTTAGCGTCATATG',\n",
       " 'GTGTGACCCTTAGTCCACCAGT',\n",
       " 'GTGTGACCCTTGCGAAGCTCAC',\n",
       " 'GTGTGACCCTTTGTCCTCGACG',\n",
       " 'GTGTGATGTAAATCGCTGGATG',\n",
       " 'GTGTGATGTAATTTCGATGACG',\n",
       " 'GTGTGATGTACGCCCTGCTATA',\n",
       " 'GTGTGATGTACTCACTGGATAT',\n",
       " 'GTGTGATGTAGGTCGTCAATCA',\n",
       " 'GTGTGATGTAGTACAACTCTAG',\n",
       " 'GTGTGATGTATCTAATGCCTGC',\n",
       " 'GTGTTCGCAGACCACGGCTGTT',\n",
       " 'GTGTTCGCAGCAGACCTGCTCA',\n",
       " 'GTGTTCGCAGCCCAATACGTGG',\n",
       " 'GTGTTCGCAGCCGGATCAAGTT',\n",
       " 'GTGTTCGCAGCGGCGACTAACA',\n",
       " 'GTGTTCGCAGCTCCCGTGTAAG',\n",
       " 'GTGTTCGCAGCTGTCTACACGA',\n",
       " 'GTGTTCGCAGGAGTGTGGCATA',\n",
       " 'GTGTTCGCAGGATAGACGAAGA',\n",
       " 'GTGTTCGCAGGATGAACATGTC',\n",
       " 'GTGTTCGCAGGCTATGTCTCTC',\n",
       " 'GTGTTCGCAGGGTAGAGAGCTC',\n",
       " 'GTGTTCGCAGTATCAAAGGGTC',\n",
       " 'GTGTTCGCAGTTCCGAGCAACT',\n",
       " 'GTTCTCTCCTAAACTAGCCCTA',\n",
       " 'GTTCTCTCCTATCGAGTTAGGC',\n",
       " 'GTTCTCTCCTCTGACGAGAAAC',\n",
       " 'GTTCTCTCCTGGGTTGAACCTA',\n",
       " 'GTTCTCTCCTGGTGGCCGTATA',\n",
       " 'GTTCTCTCCTGTATGCCGAGAA',\n",
       " 'GTTCTCTCCTTACCTCCAACTT',\n",
       " 'GTTCTCTCCTTAGATCTGCCAA',\n",
       " 'TAAGAGGCCGAACTGATAGGAG',\n",
       " 'TAAGAGGCCGAAGCGCCATCGA',\n",
       " 'TAAGAGGCCGAGGTCACGACTA',\n",
       " 'TAAGAGGCCGGTACCTATTCCA',\n",
       " 'TAAGAGGCCGTGTGATTCTGTG',\n",
       " 'TAAGGTAGGGACAGGGCCTCTT',\n",
       " 'TAAGGTAGGGGAGCATCACTAC',\n",
       " 'TAAGGTAGGGGAGGCATTTGCA',\n",
       " 'TAAGGTAGGGGCCACCTGATAC',\n",
       " 'TAAGGTAGGGGCTATGTCTCTC',\n",
       " 'TAAGGTAGGGGGGAACAAGTCA',\n",
       " 'TAAGGTAGGGTCCAGACGACTT',\n",
       " 'TAAGGTAGGGTGGAGTAACCAT',\n",
       " 'TACCGTACCTAACCCACTATCT',\n",
       " 'TACCGTACCTACCGTCCCATTC',\n",
       " 'TACCGTACCTCAAGTGTCTGTA',\n",
       " 'TACCGTACCTCTGGGAAATGTT',\n",
       " 'TACCGTACCTGCGAACGTAGAC',\n",
       " 'TACCGTACCTGCTACAATGTGG',\n",
       " 'TACCGTACCTGCTCAGCGTTTA',\n",
       " 'TACCGTACCTGCTTAAGCCGTT',\n",
       " 'TACCGTACCTGGAAGTTCAGCA',\n",
       " 'TACCGTACCTGGGCAGTGGTAA',\n",
       " 'TACCGTACCTGTAGCCTGTTAC',\n",
       " 'TACCGTACCTTACCCAATGAAC',\n",
       " 'TACCGTACCTTACGATCTATCG',\n",
       " 'TACCGTACCTTAGCGTCATATG',\n",
       " 'TACCGTACCTTCCCTATAGCCA',\n",
       " 'TACTCGATTCAAACGGATCAGT',\n",
       " 'TACTCGATTCAGCGAATAACCC',\n",
       " 'TACTCGATTCCGATTTCACCGA',\n",
       " 'TACTCGATTCCGCGATACAAAT',\n",
       " 'TACTCGATTCCGCTATCTCTTG',\n",
       " 'TACTCGATTCCGGAACTATACT',\n",
       " 'TACTCGATTCGATATGGTGCCA',\n",
       " 'TACTCGATTCGCGACGTTACGA',\n",
       " 'TACTCGATTCGCTACAATGTGG',\n",
       " 'TACTCGATTCGGGACTAATTCC',\n",
       " 'TACTCGATTCTTAAACGCGAAG',\n",
       " 'TACTCGATTCTTTCCACGCAGT',\n",
       " 'TAGACCAGGGAAAGCCCACGAC',\n",
       " 'TAGACCAGGGAACCACCTAAAG',\n",
       " 'TAGACCAGGGACAGATCAACGC',\n",
       " 'TAGACCAGGGACTAGTCCGGAA',\n",
       " 'TAGACCAGGGACTGCGACAAAG',\n",
       " 'TAGACCAGGGAGGGAATGTTCA',\n",
       " 'TAGACCAGGGATAAGGTCCTGA',\n",
       " 'TAGACCAGGGATCCAAGCCGTC',\n",
       " 'TAGACCAGGGCCTCATAGTAGA',\n",
       " 'TAGACCAGGGCCTTATATGTCC',\n",
       " 'TAGACCAGGGCGATTTCACCGA',\n",
       " 'TAGACCAGGGGACGCAACCTCT',\n",
       " 'TAGACCAGGGGAGTGGTTGACC',\n",
       " 'TAGACCAGGGGCTATTGGTAGG',\n",
       " 'TAGACCAGGGGCTTCTCACAAG',\n",
       " 'TAGACCAGGGGTGAAGCTTATG',\n",
       " 'TAGACCAGGGTAACAGTGCCTT',\n",
       " 'TAGACCAGGGTCGCGGAAAGTC',\n",
       " 'TAGACCAGGGTGCGTTGATGTG',\n",
       " 'TAGACCAGGGTGTCCTGCGGTA',\n",
       " 'TAGACCAGGGTTAGAGCCCACT',\n",
       " 'TAGACCAGGGTTAGGTAGCAAC',\n",
       " 'TAGACCAGGGTTAGGTAGTGGG',\n",
       " 'TAGACCAGGGTTGCATGTACGC',\n",
       " 'TAGACTTGACAACTGCGTCTTT',\n",
       " 'TAGACTTGACAGTTTGGAGCAT',\n",
       " 'TAGACTTGACATGTGCATCTGG',\n",
       " 'TAGACTTGACCGAAGGGATCAT',\n",
       " 'TAGACTTGACCGGCGACTAACA',\n",
       " 'TAGACTTGACCGTCGTGAAATT',\n",
       " 'TAGACTTGACCTTAGGTAGCGT',\n",
       " 'TAGACTTGACCTTGCGGGTGTT',\n",
       " 'TAGACTTGACGATGAAATCGGA',\n",
       " 'TAGACTTGACGCTCAGCGTTTA',\n",
       " 'TAGACTTGACGGCACTTCATCT',\n",
       " 'TAGACTTGACTACGTGCAGTAT',\n",
       " 'TAGACTTGACTATCCATGACCC',\n",
       " 'TAGACTTGACTGCTAGCCTATA',\n",
       " 'TAGGGTGGCAACCCACAGCAGT',\n",
       " 'TAGGGTGGCAATCGAGGCCTAA',\n",
       " 'TAGGGTGGCAATTCATTTCCGC',\n",
       " 'TAGGGTGGCAATTGGGCTAGTT',\n",
       " 'TAGGGTGGCACAACCACGAGTG',\n",
       " 'TAGGGTGGCACGCGCTATACGA',\n",
       " 'TAGGGTGGCACGGATCACCTGA',\n",
       " 'TAGGGTGGCAGATACGCCGTAG',\n",
       " 'TAGGGTGGCAGGATAAAGAAGG',\n",
       " 'TAGGGTGGCAGTTGTGATGTGC',\n",
       " 'TAGGGTGGCATCGCGAGATAGT',\n",
       " 'TAGGGTGGCATGTTCTCTCTGG',\n",
       " 'TAGTGCTGTCCGATATCGATCC',\n",
       " 'TAGTGCTGTCTAGGCAGATGGA',\n",
       " 'TATGTATGGCAAGTTGTGCTAC',\n",
       " 'TATGTATGGCAATTCTGTTGCG',\n",
       " 'TATGTATGGCCACACCTATATC',\n",
       " 'TATGTATGGCCGAAGGTTCGAT',\n",
       " 'TATGTATGGCCTAGACGACCTG',\n",
       " 'TATGTATGGCCTGGTAGCGTAT',\n",
       " 'TATGTATGGCGTTTGCCACACA',\n",
       " 'TATGTATGGCTATCAAAGGGTC',\n",
       " 'TATGTATGGCTCGACCAGATCT',\n",
       " 'TCACACGTCGAGCCATGAGAAT',\n",
       " 'TCACACGTCGCGAAGGTTCGAT',\n",
       " 'TCACACGTCGCTACTCGATCTC',\n",
       " 'TCACACGTCGGATTTCCGGACT',\n",
       " 'TCACACGTCGGCTTTGCGTCGT',\n",
       " 'TCACACGTCGGTAACTTGCCCT',\n",
       " 'TCACACGTCGTCACCAAACCGG',\n",
       " 'TCACACGTCGTGGGTTTGTACT',\n",
       " 'TCACACGTCGTTGTCCTCGACG',\n",
       " 'TCACAGAGAGAAATGCTACGGG',\n",
       " 'TCACAGAGAGATGTCTAACGCC',\n",
       " 'TCACAGAGAGCCACAGCGAAAC',\n",
       " 'TCACAGAGAGCCACTCTAGGTC',\n",
       " 'TCACAGAGAGCGCCCTGCTATA',\n",
       " 'TCACAGAGAGCGCGACTTGAGA',\n",
       " 'TCACAGAGAGCTCACAAGCCTA',\n",
       " 'TCACAGAGAGCTTACAATGCGA',\n",
       " 'TCACAGAGAGCTTAGCGTGAGT',\n",
       " 'TCACAGAGAGGGATAAAGAAGG',\n",
       " 'TCACAGAGAGTACCATCCCAGG',\n",
       " 'TCACAGAGAGTCTCTGACTTGA',\n",
       " 'TCACAGAGAGTTAGTGCTAGCA',\n",
       " 'TCCGACGCTAAAACTAGCCCTA',\n",
       " 'TCCGACGCTAAAGCGGAACCTA',\n",
       " 'TCCGACGCTAAATCGTCTGGAC',\n",
       " 'TCCGACGCTAAGATCTGTGACG',\n",
       " 'TCCGACGCTACAGCGTTGGTAC',\n",
       " 'TCCGACGCTACATGTTCTGTGA',\n",
       " 'TCCGACGCTACCCATGACGGAA',\n",
       " 'TCCGACGCTACGCGATACAAAT',\n",
       " 'TCCGACGCTACTTGAGACTCCG',\n",
       " 'TCCGACGCTAGACAATAGGAGA',\n",
       " 'TCCGACGCTAGATTCGCGTGCA',\n",
       " 'TCCGACGCTAGTTACAGAACGC',\n",
       " 'TCCGACGCTAGTTCTTGACAAG',\n",
       " 'TCCGACGCTATTCTCATGGTCA',\n",
       " 'TCGGCTTATTAACATGAAGCGC',\n",
       " 'TCGGCTTATTAACTAACGTCGA',\n",
       " 'TCGGCTTATTACATCTAAGGAG',\n",
       " 'TCGGCTTATTACGCTTACCCGA',\n",
       " 'TCGGCTTATTAGCATACTCAGC',\n",
       " 'TCGGCTTATTCACCGAAGGTCA',\n",
       " 'TCGGCTTATTCACTGTGTTATG',\n",
       " 'TCGGCTTATTCAGAATCCTTCC',\n",
       " 'TCGGCTTATTCGAGAATCGACG',\n",
       " 'TCGGCTTATTCTCACTGGATAT',\n",
       " 'TCGGCTTATTCTGTCTACACGA',\n",
       " 'TCGGCTTATTGAAGGGTGGTAT',\n",
       " 'TCGGCTTATTGCTTTGCGTCGT',\n",
       " 'TCGGCTTATTGGGACTAATTCC',\n",
       " 'TCGGCTTATTTAACAGTGCCTT',\n",
       " 'TCGGCTTATTTCATCCGCGACA',\n",
       " 'TCGGCTTATTTGAACGCAATCC',\n",
       " 'TCGGCTTATTTGAGCTAGACGC',\n",
       " 'TCGGCTTATTTTCAGTCGCCTA',\n",
       " 'TCGGCTTATTTTCCGAGCAACT',\n",
       " 'TCGGCTTATTTTTCCACGCAGT',\n",
       " 'TCGTTGCTCTCAGTTATAGGCC',\n",
       " 'TCGTTGCTCTCGAAGATCACTC',\n",
       " 'TCGTTGCTCTGGCATAATGATC',\n",
       " 'TCGTTGCTCTGTTGCGAATGGG',\n",
       " 'TCGTTGCTCTTAACAGTGCCTT',\n",
       " 'TCGTTGCTCTTAACCCAACGAG',\n",
       " 'TCGTTGCTCTTCGTCACTTAAG',\n",
       " 'TCGTTGCTCTTCTAATTGCGAC',\n",
       " 'TCGTTGCTCTTGACTATTTCCC',\n",
       " 'TCGTTGCTCTTGCTGAGTTCCT',\n",
       " 'TCTTGTTGCCACTCTTCCTCAT',\n",
       " 'TCTTGTTGCCCAAATCTCGCCG',\n",
       " 'TCTTGTTGCCCGATATCGATCC',\n",
       " 'TCTTGTTGCCCTCAGATAAGGG',\n",
       " 'TCTTGTTGCCGACTTACTGCCG',\n",
       " 'TCTTGTTGCCTACGGCTTTCCT',\n",
       " 'TGAGCGAGTAAAAGGATCGGCT',\n",
       " 'TGAGCGAGTAACCCGATGGCTA',\n",
       " 'TGAGCGAGTAAGTCGGTTGTGA',\n",
       " 'TGAGCGAGTACAGTTATAGGCC',\n",
       " 'TGAGCGAGTACGACGAGAGATC',\n",
       " 'TGAGCGAGTACGTCCCTAGAAC',\n",
       " 'TGAGCGAGTAGATTTCCGGACT',\n",
       " 'TGAGCGAGTAGTTGCGAATGGG',\n",
       " 'TGAGCGAGTATAGGCAGATGGA',\n",
       " 'TGAGCGAGTATCGCGATCTGAC',\n",
       " 'TGAGCGAGTATCGTGGAATTAC',\n",
       " 'TGAGCGAGTATGCTAATGACGT',\n",
       " 'TGAGCGAGTATTCTCATGGTCA',\n",
       " 'TGATTACGCGAAAGGATCGGCT',\n",
       " 'TGATTACGCGACGCCGTGACTT',\n",
       " 'TGATTACGCGAGGTCACGACTA',\n",
       " 'TGATTACGCGCACGCAAGGGAA',\n",
       " 'TGATTACGCGCAGTGATTCTGT',\n",
       " 'TGATTACGCGCAGTTGACGCTC',\n",
       " 'TGATTACGCGCATACCCGGTTC',\n",
       " 'TGATTACGCGCGGATCACCTGA',\n",
       " 'TGATTACGCGTATCCATGACCC',\n",
       " 'TGATTACGCGTATGGTCCAGAC',\n",
       " 'TGATTACGCGTTAGCTTCTCTG',\n",
       " 'TGATTACGCGTTAGGGACCCTC',\n",
       " 'TGGAGGACGAAACGACTGCTCG',\n",
       " 'TGGAGGACGAAGGAACGCCGAT',\n",
       " ...]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_list_from_file('barcodes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TAGTGCTGTCTCGCTTAGCCTT',\n",
       " 'AAATCCGCATAACCGCTAATGA',\n",
       " 'AAATCCGCATACCAATCGCTTG',\n",
       " 'AAATCCGCATACGCGGTATGTA',\n",
       " 'AAATCCGCATATTAGGAGGTCT',\n",
       " 'AAATCCGCATCATCTCGACAAT',\n",
       " 'AAATCCGCATCGCAAGTAAAGC',\n",
       " 'AAATCCGCATGCTATGTCTCTC',\n",
       " 'AAATCCGCATGGGAACAAGTCA',\n",
       " 'AAATCCGCATGTACATCTTCAC',\n",
       " 'AAATCCGCATGTCCACCTAGAC',\n",
       " 'AAATCCGCATGTTCTCCTACTC',\n",
       " 'AAATCCGCATTACGATCTATCG',\n",
       " 'AAATCCGCATTAGGTTGCACTA',\n",
       " 'AAATCCGCATTATGGTCCAGAC',\n",
       " 'AAATCCGCATTCCACACCTCCA',\n",
       " 'AAATCCGCATTTGCATGTACGC',\n",
       " 'AACGACCAAAAAAGCCCACGAC',\n",
       " 'AACGACCAAAACAGAAGGACCT',\n",
       " 'AACGACCAAAACATCCGGTCTC',\n",
       " 'AACGACCAAAAGGTCCCAATCG',\n",
       " 'AACGACCAAAAGTGTCGGGTTG',\n",
       " 'AACGACCAAAATTGGACTTAGC',\n",
       " 'AACGACCAAACACGTTCCAGCA',\n",
       " 'AACGACCAAACAGCGAAGACCT',\n",
       " 'AACGACCAAACGACGAGAGATC',\n",
       " 'AACGACCAAACGATTTCACCGA',\n",
       " 'AACGACCAAACGCTATCTCTTG',\n",
       " 'AACGACCAAACTTACAATGCGA',\n",
       " 'AACGACCAAAGAAGCCGTTGGT',\n",
       " 'AACGACCAAAGTAACTTGCCCT',\n",
       " 'AACGACCAAATAAGACTGAGGG',\n",
       " 'AACGACCAAATCGGTACTAACT',\n",
       " 'AACGACGTGTACCACGCAAACT',\n",
       " 'AACGACGTGTGGAACGGTCTCA',\n",
       " 'AACGACGTGTTGCTAATGACGT',\n",
       " 'AAGCAAAGTCAAATAAGGCCAG',\n",
       " 'AAGCAAAGTCAATCCCGCGCTT',\n",
       " 'AAGCAAAGTCATAACGCAAGTC',\n",
       " 'AAGCAAAGTCCAGTTGACGCTC',\n",
       " 'AAGCAAAGTCCTTCGCGTAACG',\n",
       " 'AAGCAAAGTCGTATATCCGGAA',\n",
       " 'AAGCAAAGTCGTCCTATGTCCG',\n",
       " 'AAGCAAAGTCGTGCAAACCACT',\n",
       " 'AAGCAAAGTCGTTGTACGAGCC',\n",
       " 'AAGCAAAGTCTAGACTGGACCA',\n",
       " 'AAGCAAAGTCTGAACGCAATCC',\n",
       " 'AAGCAAAGTCTGCACGTTAACC',\n",
       " 'AAGCTTGTGCAGATGCCGATGT',\n",
       " 'AAGCTTGTGCATGTCTAACGCC',\n",
       " 'AAGCTTGTGCATTTCGATGACG',\n",
       " 'AAGCTTGTGCCTAACAGTACTC',\n",
       " 'AAGCTTGTGCGAGTTATTTCGG',\n",
       " 'AAGCTTGTGCTACAACGTAGCT',\n",
       " 'AAGCTTGTGCTAGGTAAGACCA',\n",
       " 'AAGCTTGTGCTCCACTTGGATA',\n",
       " 'AAGCTTGTGCTGACTATTTCCC',\n",
       " 'AAGTCCTTAGATGTGCCTTCTG',\n",
       " 'AAGTCCTTAGATTGCCTTCTTC',\n",
       " 'AAGTCCTTAGATTGGGCTAGTT',\n",
       " 'AAGTCCTTAGATTTGCCCAGCT',\n",
       " 'AAGTCCTTAGCAAGTTAGTGAC',\n",
       " 'AAGTCCTTAGCGTCAATAGCTG',\n",
       " 'AAGTCCTTAGCGTGATGAGGTA',\n",
       " 'AAGTCCTTAGGAAAGCATGCGA',\n",
       " 'AAGTCCTTAGGATAGGTGTAGC',\n",
       " 'AAGTCCTTAGGGATTGTTTCTG',\n",
       " 'AAGTCCTTAGGGGACAGTGTAT',\n",
       " 'AAGTCCTTAGTAGTCCACCAGT',\n",
       " 'AAGTCCTTAGTCGACCAGATCT',\n",
       " 'AAGTCCTTAGTCTTTGACGCTG',\n",
       " 'AAGTCCTTAGTGAGCTAGACGC',\n",
       " 'AAGTCCTTAGTGGCTCGACTCA',\n",
       " 'ACAAACCCTCAGCATACTCAGC',\n",
       " 'ACAAACCCTCAGCGAATAACCC',\n",
       " 'ACAAACCCTCAGGAACTGTCGT',\n",
       " 'ACAAACCCTCGAGGCATTTGCA',\n",
       " 'ACAAACCCTCGATCCATTCGTG',\n",
       " 'ACAAACCCTCGCAGAGAGAGAA',\n",
       " 'ACAAACCCTCGCGACGTTACGA',\n",
       " 'ACAAACCCTCTATCAAAGGGTC',\n",
       " 'ACAAACCCTCTATCGCCGGGTA',\n",
       " 'ACAAACCCTCTCTAATGCCTGC',\n",
       " 'ACAAACCCTCTCTAATTGCGAC',\n",
       " 'ACAATGCTCCACCACTATTCAC',\n",
       " 'ACAATGCTCCACCCGAGTCAGT',\n",
       " 'ACAATGCTCCAGATTTGAGTCC',\n",
       " 'ACAATGCTCCAGGAACTGTCGT',\n",
       " 'ACAATGCTCCATCGTACCCGTT',\n",
       " 'ACAATGCTCCATGTGCCTTCTG',\n",
       " 'ACAATGCTCCCAGCGAAGACCT',\n",
       " 'ACAATGCTCCCATTAGTGTACC',\n",
       " 'ACAATGCTCCCGTCCCTAGAAC',\n",
       " 'ACAATGCTCCGGCACTTCATCT',\n",
       " 'ACAATGCTCCGTTATCAGGGCC',\n",
       " 'ACAATGCTCCTACACCAGAGCC',\n",
       " 'ACAATGCTCCTATGGTCCAGAC',\n",
       " 'ACAATGCTCCTCAAGGCCTGGA',\n",
       " 'ACAATGCTCCTGAAGTTCCGAG',\n",
       " 'ACAATGCTCCTGCGTTGATGTG',\n",
       " 'ACAGCAAGTCAAATGCTACGGG',\n",
       " 'ACAGCAAGTCAGCAGTTGGTGG',\n",
       " 'ACAGCAAGTCATCCGTGGTTGC',\n",
       " 'ACAGCAAGTCCACAAATCAACC',\n",
       " 'ACAGCAAGTCCGAACGATACAG',\n",
       " 'ACAGCAAGTCCGACTTGCAAGA',\n",
       " 'ACAGCAAGTCCGTCCCTAGAAC',\n",
       " 'ACAGCAAGTCCTCACAAGCCTA',\n",
       " 'ACAGCAAGTCGATGGCAAACAT',\n",
       " 'ACAGCAAGTCTAAGATGCTGGG',\n",
       " 'ACAGCAAGTCTCAGCCCAAGTT',\n",
       " 'ACAGCAAGTCTGACACAGCGAC',\n",
       " 'ACAGCAAGTCTTACAGCACCGG',\n",
       " 'ACAGCAAGTCTTCGGTTCCCAC',\n",
       " 'ACAGCAAGTCTTGAGTGCTGTG',\n",
       " 'ACCCTTATCTAACAGGCGGGTA',\n",
       " 'ACCCTTATCTAAGCGCCATCGA',\n",
       " 'ACCCTTATCTACTTGAGTCATC',\n",
       " 'ACCCTTATCTATGCGTAGCTGA',\n",
       " 'ACCCTTATCTCTGGGAAATGTT',\n",
       " 'ACCCTTATCTGAATCAGCTTGA',\n",
       " 'ACCCTTATCTGACGGAGAACCT',\n",
       " 'ACCCTTATCTGCAGCTGATGTG',\n",
       " 'ACCCTTATCTGGACGATGTCTG',\n",
       " 'ACCCTTATCTGGGTACAGCTTC',\n",
       " 'ACCCTTATCTGTCCAGATTTCC',\n",
       " 'ACCCTTATCTGTGTAACGCGGT',\n",
       " 'ACCCTTATCTGTTGTACGAGCC',\n",
       " 'ACCCTTATCTTTGAGTGCTGTG',\n",
       " 'ACCTTCAAGCACGCGATCAGTT',\n",
       " 'ACCTTCAAGCAGTGCCTAACAA',\n",
       " 'ACCTTCAAGCCGCGACTTGAGA',\n",
       " 'ACCTTCAAGCGACAATCGAGAA',\n",
       " 'ACCTTCAAGCGCGATGTACGAC',\n",
       " 'ACCTTCAAGCGGTGTTCCCTAC',\n",
       " 'ACCTTCAAGCGTTTGCCACACA',\n",
       " 'ACCTTCAAGCTGATCACTGCAT',\n",
       " 'ACCTTCAAGCTGTCAGGAACAC',\n",
       " 'ACCTTCAAGCTGTGATTCTGTG',\n",
       " 'ACCTTCAAGCTTAGGTAGCAAC',\n",
       " 'ACCTTCAAGCTTCCGCTGATAT',\n",
       " 'ACGTAGCGCAACCCACAGCAGT',\n",
       " 'ACGTAGCGCAACTGCGACAAAG',\n",
       " 'ACGTAGCGCAATAGCCTCCGTA',\n",
       " 'ACGTAGCGCACGTTTATCACAC',\n",
       " 'ACGTAGCGCAGAGATAGTACTG',\n",
       " 'ACGTAGCGCAGCTATTGGTAGG',\n",
       " 'ACGTAGCGCAGCTTTCCAACGG',\n",
       " 'ACGTAGCGCAGGATACTCGTGA',\n",
       " 'ACGTAGCGCAGGTAGAGAGCTC',\n",
       " 'ACGTAGCGCAGTCATAGCGTGT',\n",
       " 'ACGTAGCGCATAGTCCGAGATC',\n",
       " 'ACGTAGCGCATCTCTGACTTGA',\n",
       " 'ACGTAGCGCATGCGAAGCTCAC',\n",
       " 'ACGTAGCGCATGGAGTAACCAT',\n",
       " 'ACTTGCTTCTAAAGGAACAGAC',\n",
       " 'ACTTGCTTCTACCCACAGCAGT',\n",
       " 'ACTTGCTTCTAGTTTGGAGCAT',\n",
       " 'ACTTGCTTCTCAACCACGAGTG',\n",
       " 'ACTTGCTTCTCTTAGCGTGAGT',\n",
       " 'ACTTGCTTCTGAATCAGCTTGA',\n",
       " 'ACTTGCTTCTGTCCACCTAGAC',\n",
       " 'ACTTGCTTCTTGTTTCGGTACA',\n",
       " 'AGAAAGGCGGAGTTTGGAGCAT',\n",
       " 'AGAAAGGCGGCAAGTTAGTGAC',\n",
       " 'AGAAAGGCGGCCTCATAGTAGA',\n",
       " 'AGAAAGGCGGCTCGTAGAGCGT',\n",
       " 'AGAAAGGCGGGAAGGACCTAGT',\n",
       " 'AGAAAGGCGGGATGAAATCGGA',\n",
       " 'AGAAAGGCGGGCTTACAATCGT',\n",
       " 'AGAAAGGCGGTACATCACCTCA',\n",
       " 'AGAAAGGCGGTAGCGTCATATG',\n",
       " 'AGCCATAGGGAAACCTAAGTGG',\n",
       " 'AGCCATAGGGACGCAAGACGAG',\n",
       " 'AGCCATAGGGAGGTCACGACTA',\n",
       " 'AGCCATAGGGATCCGTGGTTGC',\n",
       " 'AGCCATAGGGATTACTCGTGGG',\n",
       " 'AGCCATAGGGCAAGATTACCCG',\n",
       " 'AGCCATAGGGCACCGTATGTTC',\n",
       " 'AGCCATAGGGCCGGTACATTCC',\n",
       " 'AGCCATAGGGCGTCGTGAAATT',\n",
       " 'AGCCATAGGGGATGAACATGTC',\n",
       " 'AGCCATAGGGGGATAAAGAAGG',\n",
       " 'AGCCATAGGGGTCGCTAAGTAA',\n",
       " 'AGCCATAGGGGTTATCAGGGCC',\n",
       " 'AGCCATAGGGTACCATCCCAGG',\n",
       " 'AGCCATAGGGTAGATACATCCC',\n",
       " 'AGCCATAGGGTAGGTTGCACTA',\n",
       " 'AGCGTGTCATAAGTAGCCCGCT',\n",
       " 'AGCGTGTCATACCACGAAGGGA',\n",
       " 'AGCGTGTCATAGTGCCTAACAA',\n",
       " 'AGCGTGTCATCACTGTGTTATG',\n",
       " 'AGCGTGTCATCAGTTATAGGCC',\n",
       " 'AGCGTGTCATCGACTTGCAAGA',\n",
       " 'AGCGTGTCATCGCCCTGCTATA',\n",
       " 'AGCGTGTCATCTGACGAGAAAC',\n",
       " 'AGCGTGTCATGCTCCGGAAATT',\n",
       " 'AGCGTGTCATGGGTTGAACCTA',\n",
       " 'AGCGTGTCATGTTCTCCTACTC',\n",
       " 'AGCGTGTCATTGCGTGCAGACA',\n",
       " 'AGCGTGTCATTGGACGACGATC',\n",
       " 'AGCGTGTCATTTGTCTCTTGGG',\n",
       " 'AGGCTGTGAGAAAGGAACAGAC',\n",
       " 'AGGCTGTGAGAAAGGATCGGCT',\n",
       " 'AGGCTGTGAGAGCTAGGGTGCT',\n",
       " 'AGGCTGTGAGCTTCTAGGCTAC',\n",
       " 'AGGCTGTGAGGAGTGTGGCATA',\n",
       " 'AGGCTGTGAGGATGAACATGTC',\n",
       " 'AGGCTGTGAGTGAACCTGATCG',\n",
       " 'AGGCTGTGAGTGTTTCCGTTGC',\n",
       " 'AGGGATAAACAACGAGCTGTGA',\n",
       " 'AGGGATAAACACGGGTTCTTGT',\n",
       " 'AGGGATAAACAGGAACTGTCGT',\n",
       " 'AGGGATAAACATCCGTGGTTGC',\n",
       " 'AGGGATAAACCAAGATTACCCG',\n",
       " 'AGGGATAAACCACAAATCAACC',\n",
       " 'AGGGATAAACCACCGAAGGTCA',\n",
       " 'AGGGATAAACCGATGTCCAAGT',\n",
       " 'AGGGATAAACGAGAACGTCATA',\n",
       " 'AGGGATAAACGATGTTTCACCA',\n",
       " 'AGGGATAAACGCGAACGTAGAC',\n",
       " 'AGGGATAAACGCGTGCTACCAA',\n",
       " 'AGGGATAAACGCTATTGGTAGG',\n",
       " 'AGGGATAAACGGGCTACAGTTC',\n",
       " 'AGGGATAAACTGCGTCGAGTAC',\n",
       " 'AGGGATCGTAACTAACGGTATC',\n",
       " 'AGGGATCGTAACTGGATATGCT',\n",
       " 'AGGGATCGTACCTATTGAAGCC',\n",
       " 'AGGGATCGTACGTTTATCACAC',\n",
       " 'AGGGATCGTAGAACACGTTCCA',\n",
       " 'AGGGATCGTAGACATCGAGTAG',\n",
       " 'AGGGATCGTAGACATTTGACAG',\n",
       " 'AGGGATCGTAGAGAACGTCATA',\n",
       " 'AGGGATCGTAGAGAGGTCCACT',\n",
       " 'AGGGATCGTATACTAGGGTTAG',\n",
       " 'AGGGATCGTATAGAGAGGCTGC',\n",
       " 'AGGGATCGTATCGGTACTAACT',\n",
       " 'ATACTCGCTTAAAGCGGGAGCT',\n",
       " 'ATACTCGCTTAAATTGAGGAGG',\n",
       " 'ATACTCGCTTACCACGAAGGGA',\n",
       " 'ATACTCGCTTAGGTTATCGATC',\n",
       " 'ATACTCGCTTATAACGCAAGTC',\n",
       " 'ATACTCGCTTATAGGTTTCGCC',\n",
       " 'ATACTCGCTTCGACATCTGGAT',\n",
       " 'ATACTCGCTTCGTAGGATTCTG',\n",
       " 'ATACTCGCTTCTACATTCACCA',\n",
       " 'ATACTCGCTTCTTCATGTACCT',\n",
       " 'ATACTCGCTTGCTAAGGATTAG',\n",
       " 'ATACTCGCTTGCTCCGGAAATT',\n",
       " 'ATACTCGCTTGGGCGTATTTGA',\n",
       " 'ATACTCGCTTGGTGTCGGTTGT',\n",
       " 'ATACTCGCTTGTCGCTAAGTAA',\n",
       " 'ATACTCGCTTGTGTTAGGCAAT',\n",
       " 'ATACTCGCTTTGTCCTGCGGTA',\n",
       " 'ATCCCTCCGAAGGAGTCGGAGA',\n",
       " 'ATCCCTCCGACCCAATACGTGG',\n",
       " 'ATCCCTCCGAGGAGTGTTGGAA',\n",
       " 'ATCCCTCCGATACCCAATGAAC',\n",
       " 'ATGCAGGTAAAAACTAGCCCTA',\n",
       " 'ATGCAGGTAAAACTAACGTCGA',\n",
       " 'ATGCAGGTAAACATCTAAGGAG',\n",
       " 'ATGCAGGTAAAGATTGGACAAG',\n",
       " 'ATGCAGGTAAAGGTGTGACCGT',\n",
       " 'ATGCAGGTAACAACCACGAGTG',\n",
       " 'ATGCAGGTAACTCGTAGAGCGT',\n",
       " 'ATGCAGGTAAGCACACTCTCCT',\n",
       " 'ATGCAGGTAAGTTACAGAACGC',\n",
       " 'ATGCAGGTAATGAGCTAGACGC',\n",
       " 'ATGTAACGACAATGGAGCAACA',\n",
       " 'ATGTAACGACATCAGTCAAGGA',\n",
       " 'ATGTAACGACATGGACGATCGT',\n",
       " 'ATGTAACGACCAAATGACGGGC',\n",
       " 'ATGTAACGACCATACCCGGTTC',\n",
       " 'ATGTAACGACCCACGTCACTTA',\n",
       " 'ATGTAACGACCGACTTGCAAGA',\n",
       " 'ATGTAACGACCTAATTTGTGGG',\n",
       " 'ATGTAACGACGGGCGTATTTGA',\n",
       " 'ATGTTCACGTTGTGATTCTGTG',\n",
       " 'ATGTTCACGTTTGAGCCCGGAT',\n",
       " 'CAATGATAGCCGACGCCTTATG',\n",
       " 'CAATGATAGCCTATCCCATCCG',\n",
       " 'CAATGATAGCCTTTAGCTGACT',\n",
       " 'CAATGATAGCGAGGGTGATTGC',\n",
       " 'CAATGATAGCGCGGAACTAGAG',\n",
       " 'CAATGATAGCGCTTCTCACAAG',\n",
       " 'CAATGATAGCTCACAGTGATAG',\n",
       " 'CACCCAGACTCACCGTATGTTC',\n",
       " 'CACCCAGACTCCATTACGATTC',\n",
       " 'CACCCAGACTGGGCTAGGAGAT',\n",
       " 'CACCCAGACTTCAGCCCAAGTT',\n",
       " 'CACCCAGACTTTACTTACACCC',\n",
       " 'CAGGAAAGCACAGCTCAGGACA',\n",
       " 'CAGGAAAGCACCTCGTCAGAAC',\n",
       " 'CAGGAAAGCACCTTCCACCTGT',\n",
       " 'CAGGAAAGCAGATCAGTAGGAT',\n",
       " 'CAGGAAAGCATAAACGCTCCAG',\n",
       " 'CAGGAAAGCATCGATCCATGGG',\n",
       " 'CAGGAAAGCATGCGTTGATGTG',\n",
       " 'CAGGAAAGCATTCTAACGCTTC',\n",
       " 'CAGGAAAGCATTTCTGCAGACT',\n",
       " 'CAGGTGAACCCTGGGCTTGGTA',\n",
       " 'CAGGTGAACCCTGTTGGTCCTT',\n",
       " 'CAGGTGAACCGAATCAGCTTGA',\n",
       " 'CAGGTGAACCGAGGAATCACGG',\n",
       " 'CAGGTGAACCGAGGCATTTGCA',\n",
       " 'CAGGTGAACCGCGTAACCAGTA',\n",
       " 'CAGGTGAACCGGATACTCGTGA',\n",
       " 'CAGGTGAACCGGCTGATATGCT',\n",
       " 'CAGGTGAACCGTGTGAGCATGC',\n",
       " 'CAGGTGAACCGTTGGGCTCAAG',\n",
       " 'CAGGTGAACCTATCGCCGGGTA',\n",
       " 'CAGGTGAACCTCCTTAGTCGGG',\n",
       " 'CAGGTGAACCTCTGTACGCTGA',\n",
       " 'CAGGTGAACCTGCTATCGGACG',\n",
       " 'CATCCACGGAAAAGCCCACGAC',\n",
       " 'CATCCACGGAAAGACACCAACC',\n",
       " 'CATCCACGGAACAACAGCGGAT',\n",
       " 'CATCCACGGAACCCGATGGCTA',\n",
       " 'CATCCACGGAAGCCTCACCATC',\n",
       " 'CATCCACGGAAGGCTGCGTCTA',\n",
       " 'CATCCACGGAAGTTTGGAGCAT',\n",
       " 'CATCCACGGAATGTGCATCTGG',\n",
       " 'CATCCACGGACACTGTGTTATG',\n",
       " 'CATCCACGGACCTCGTCAGAAC',\n",
       " 'CATCCACGGACCTTATATGTCC',\n",
       " 'CATCCACGGACGTTTATCACAC',\n",
       " 'CATCCACGGACTTAGACGTCTT',\n",
       " 'CATCCACGGAGATCTACGCCGT',\n",
       " 'CATCCACGGAGGAGTTTGCACA',\n",
       " 'CATCCACGGATGACCACCAATT',\n",
       " 'CATCCACGGATTATGGTAGACC',\n",
       " 'CATCCGGAACATAGGTTTCGCC',\n",
       " 'CATCCGGAACATGGACGATCGT',\n",
       " 'CATCCGGAACCACTTCTCCTAC',\n",
       " 'CATCCGGAACCAGCTCAGGACA',\n",
       " 'CATCCGGAACCATCTCGACAAT',\n",
       " 'CATCCGGAACCTATCCCATCCG',\n",
       " 'CATCCGGAACCTTCTAGGCTAC',\n",
       " 'CATCCGGAACCTTTAGCTGACT',\n",
       " 'CATCCGGAACGATCCATTCGTG',\n",
       " 'CATCCGGAACGATGGCAAACAT',\n",
       " 'CATCCGGAACGCATGTATCCTT',\n",
       " 'CATCCGGAACGGTGGCCGTATA',\n",
       " 'CATCCGGAACGTACATAGATCC',\n",
       " 'CATGAACGTGACCACGAAGGGA',\n",
       " 'CATGAACGTGCGGAACATTGTA',\n",
       " 'CCAAGGTGATAAGACACCAACC',\n",
       " 'CCAAGGTGATAAGCGGGCTTGA',\n",
       " 'CCAAGGTGATAAGGGCAAATCA',\n",
       " 'CCAAGGTGATAGTGTCAGTCAA',\n",
       " 'CCAAGGTGATATGGACACGTTC',\n",
       " 'CCAAGGTGATCCTGGGTTAATC',\n",
       " 'CCAAGGTGATCTCACAAGCCTA',\n",
       " 'CCAAGGTGATCTCATGTAGTCT',\n",
       " 'CCAAGGTGATCTTTGGGACGGT',\n",
       " 'CCAAGGTGATGATTCGCGTGCA',\n",
       " 'CCAAGGTGATGATTTCCGGACT',\n",
       " 'CCAAGGTGATTTTACCACGCCA',\n",
       " 'CCAAGGTGATTTTACCCGTCGA',\n",
       " 'CCACAAATTCAACAGGCGGGTA',\n",
       " 'CCACAAATTCACCAATCGCTTG',\n",
       " 'CCACAAATTCACGCCGTGACTT',\n",
       " 'CCACAAATTCAGATCTGTGACG',\n",
       " 'CCACAAATTCAGGAGTCGGAGA',\n",
       " 'CCACAAATTCAGGTTATCGATC',\n",
       " 'CCACAAATTCCAAGATTACCCG',\n",
       " 'CCACAAATTCCCTTTCCGTATC',\n",
       " 'CCACAAATTCGACAATAGGAGA',\n",
       " 'CCACAAATTCGAGAACGTCATA',\n",
       " 'CCACAAATTCGCGTAACCAGTA',\n",
       " 'CCACAAATTCGGTTGTTCCATC',\n",
       " 'CCACAAATTCGTGTTAGGCAAT',\n",
       " 'CCGCTACTCACAGAATGCCTGA',\n",
       " 'CCGCTACTCAGATACTTGCTGG',\n",
       " 'CCGCTACTCAGATGAACATGTC',\n",
       " 'CCGCTACTCAGGCATAATGATC',\n",
       " 'CCGCTACTCATGGTATGGAGGT',\n",
       " 'CCGCTACTCATTGTTCCTCTGC',\n",
       " 'CCGGGATTTCATTTGCCCAGCT',\n",
       " 'CCGGGATTTCGCGCAGTTTATA',\n",
       " 'CCGGGATTTCGGGCATAAACGT',\n",
       " 'CCGGGATTTCGTCAGGGTACTT',\n",
       " 'CCGGGATTTCTCGGTACTAACT',\n",
       " 'CCGGGATTTCTGCCGTCTCAAC',\n",
       " 'CCGGGATTTCTGCTAATGACGT',\n",
       " 'CCGGGATTTCTTAGGGACCCTC',\n",
       " 'CCTCGATATGGGCTGATATGCT',\n",
       " 'CCTCGATATGGGTAATACTCGG',\n",
       " 'CCTTACTCAGACATGCCGTGCT',\n",
       " 'CCTTACTCAGACTGCACGCCAT',\n",
       " 'CCTTACTCAGATGACCACCAAT',\n",
       " 'CCTTACTCAGCACCGTATGTTC',\n",
       " 'CCTTACTCAGCCGGCATATGAG',\n",
       " 'CCTTACTCAGCCTGATAACCAA',\n",
       " 'CCTTACTCAGGAGTGTGGCATA',\n",
       " 'CCTTACTCAGGGAAGCTTATCC',\n",
       " 'CCTTACTCAGGGCCATCCACAA',\n",
       " 'CCTTACTCAGGGTCGTCAATCA',\n",
       " 'CCTTACTCAGGGTGTTCCCTAC',\n",
       " 'CCTTACTCAGTAGTCCACCAGT',\n",
       " 'CCTTACTCAGTCATTCACGAAG',\n",
       " 'CCTTACTCAGTCCAGACGACTT',\n",
       " 'CCTTACTCAGTGCCGTGAGACA',\n",
       " 'CCTTACTCAGTGGTATGGAGGT',\n",
       " 'CGCATCTCTGATGCGTATTCCC',\n",
       " 'CGCATCTCTGCCCATCTGGAGA',\n",
       " 'CGCATCTCTGGCGTAACCAGTA',\n",
       " 'CGCATCTCTGTACCGCTTCCAC',\n",
       " 'CGCCTTTACGACCCACATGCTT',\n",
       " 'CGCCTTTACGACTGCGACAAAG',\n",
       " 'CGCCTTTACGAGGGATCCATTA',\n",
       " 'CGCCTTTACGATCGTACCCGTT',\n",
       " 'CGCCTTTACGCACCGAAGGTCA',\n",
       " 'CGCCTTTACGCAGCAACCGATC',\n",
       " 'CGCCTTTACGCCCATGACGGAA',\n",
       " 'CGCCTTTACGCTCAGATAAGGG',\n",
       " 'CGCCTTTACGGCAGAGAGAGAA',\n",
       " 'CGCCTTTACGGGTAGAGAGCTC',\n",
       " 'CGCCTTTACGTACTAAGCAGTG',\n",
       " 'CGCCTTTACGTATCAAAGGGTC',\n",
       " 'CGCCTTTACGTCATCCGCGACA',\n",
       " 'CGCCTTTACGTCCCTAACCATA',\n",
       " 'CGCCTTTACGTTACTTACACCC',\n",
       " 'CGTATCTTGTCAAATGACGGGC',\n",
       " 'CGTATCTTGTCATTAGTGTACC',\n",
       " 'CGTATCTTGTCCACGTCACTTA',\n",
       " 'CGTATCTTGTCCCATCTGGAGA',\n",
       " 'CGTATCTTGTCGAGAATCGACG',\n",
       " 'CGTATCTTGTCGTCAATAGCTG',\n",
       " 'CGTATCTTGTGAATCAGCTTGA',\n",
       " 'CGTATCTTGTTGGAACAGTCTG',\n",
       " 'CGTATCTTGTTGGCTTCTACCA',\n",
       " 'CGTATCTTGTTTATGGTAGACC',\n",
       " 'CGTCAAGCAGACCCACAGCAGT',\n",
       " 'CGTCAAGCAGACCCGAACTCAT',\n",
       " 'CGTCAAGCAGCACCGAAGGTCA',\n",
       " 'CGTCAAGCAGCACCGTATGTTC',\n",
       " 'CGTCAAGCAGCGTACTTCCTCG',\n",
       " 'CGTCAAGCAGGAAGTTCTTAGG',\n",
       " 'CGTCAAGCAGGGTAAATCTCCC',\n",
       " 'CGTCAAGCAGTCAGCATTTAGG',\n",
       " 'CGTCAAGCAGTGGGATTGACAG',\n",
       " 'CGTCAAGCAGTTAAACGCGAAG',\n",
       " 'CGTCAAGCAGTTAGGATGCGCG',\n",
       " 'CGTCTGGACTAAACGGATCAGT',\n",
       " 'CGTCTGGACTAACTTCGACCAG',\n",
       " 'CGTCTGGACTACCAGTTCACCC',\n",
       " 'CGTCTGGACTACCCGAACTCAT',\n",
       " 'CGTCTGGACTACTGCGACAAAG',\n",
       " 'CGTCTGGACTAGCGGCTGAAAC',\n",
       " 'CGTCTGGACTAGTCTGTGTAGA',\n",
       " 'CGTCTGGACTCAAGTTAGTGAC',\n",
       " 'CGTCTGGACTCAGTTATAGGCC',\n",
       " 'CGTCTGGACTCGTTCGTTTATG',\n",
       " 'CGTCTGGACTGCTTCATGAACC',\n",
       " 'CGTCTGGACTGGGCTAGGAGAT',\n",
       " 'CGTCTGGACTGTGTCATGAAAG',\n",
       " 'CGTCTGGACTTCGATCCATGGG',\n",
       " 'CGTTGCCCAAACGTCCGTAATT',\n",
       " 'CGTTGCCCAAATAGGTGGTACC',\n",
       " 'CGTTGCCCAAATCTCGCCACTT',\n",
       " 'CGTTGCCCAACAAGATTACCCG',\n",
       " 'CGTTGCCCAACAGATCGAGGGA',\n",
       " 'CGTTGCCCAACAGTTGACGCTC',\n",
       " 'CGTTGCCCAACCAGCTTTCCGT',\n",
       " 'CGTTGCCCAACCTCGTCAGAAC',\n",
       " 'CGTTGCCCAACTCGTAGAGCGT',\n",
       " 'CGTTGCCCAACTTCGCGTAACG',\n",
       " 'CGTTGCCCAAGAGAGGTCCACT',\n",
       " 'CGTTGCCCAAGCGGATAGTAGG',\n",
       " 'CGTTGCCCAAGGAACGTCAGTT',\n",
       " 'CGTTGCCCAAGTCAGTAGTCTG',\n",
       " 'CGTTGCCCAAGTCGACAGATAG',\n",
       " 'CGTTGCCCAATCAGCATTTAGG',\n",
       " 'CGTTGCCCAATCCTGATTTAGC',\n",
       " 'CGTTGCCCAATGCGTCGAGTAC',\n",
       " 'CGTTGCCCAATTCGGTTCCCAC',\n",
       " 'CTAGTGTTGCAATCGAACACGT',\n",
       " 'CTAGTGTTGCAGATTTGAGTCC',\n",
       " 'CTAGTGTTGCAGCAGGTATGTT',\n",
       " 'CTAGTGTTGCATTAGGAGGTCT',\n",
       " 'CTAGTGTTGCCTTGCTGCCTAT',\n",
       " 'CTAGTGTTGCGACAATCGAGAA',\n",
       " 'CTAGTGTTGCGGACCGACAATG',\n",
       " 'CTAGTGTTGCGTGTCATGAAAG',\n",
       " 'CTAGTGTTGCGTTATCAGGGCC',\n",
       " 'CTAGTGTTGCTACTAAGCAGTG',\n",
       " 'CTAGTGTTGCTGGGTTTGTACT',\n",
       " 'CTAGTGTTGCTGGTGAGGTGAT',\n",
       " 'CTATGTCCAGAAGTTGTGCTAC',\n",
       " 'CTATGTCCAGCTCCCGTGTAAG',\n",
       " 'CTATGTCCAGCTTGCGGGTGTT',\n",
       " 'CTATGTCCAGGAGGAATCACGG',\n",
       " 'CTATGTCCAGGCACACTCTCCT',\n",
       " 'CTATGTCCAGGGGTACAGCTTC',\n",
       " 'CTATTTGCCTACATCTCTCCCG',\n",
       " 'CTATTTGCCTACCACGAAGGGA',\n",
       " 'CTATTTGCCTATCGAGTTAGGC',\n",
       " 'CTATTTGCCTCATGTTCTGTGA',\n",
       " 'CTATTTGCCTCGACGAAGAAGA',\n",
       " 'CTATTTGCCTCGATTGGCGATA',\n",
       " 'CTATTTGCCTCGTTCGTTTATG',\n",
       " 'CTATTTGCCTCTAGACGACCTG',\n",
       " 'CTATTTGCCTGTCCTATGTCCG',\n",
       " 'CTATTTGCCTTAGACTGGACCA',\n",
       " 'CTATTTGCCTTCGCTTCCGGTT',\n",
       " 'CTATTTGCCTTCGTCACTTAAG',\n",
       " 'CTCAACCAGGAAGTAGCCCGCT',\n",
       " 'CTCAACCAGGACGTCCGTAATT',\n",
       " 'CTCAACCAGGATTGCCTTCTTC',\n",
       " 'CTCAACCAGGCGACGCCTTATG',\n",
       " 'CTCAACCAGGCGCAAGTAAAGC',\n",
       " 'CTCAACCAGGCTATCCCATCCG',\n",
       " 'CTCAACCAGGCTTGCGGGTGTT',\n",
       " 'CTCAACCAGGGATGTTTCACCA',\n",
       " 'CTCAACCAGGGCCTAAACGAAG',\n",
       " 'CTCAACCAGGGCTATTGGTAGG',\n",
       " 'CTCAACCAGGTGAAGTTCCGAG',\n",
       " 'CTCAACCAGGTGACCACCAATT',\n",
       " 'CTCAACCAGGTGTCAGCACAAT',\n",
       " 'CTCAACCAGGTGTCCTGCGGTA',\n",
       " 'CTCAACCAGGTGTGACAGGTTC',\n",
       " 'CTCAACCAGGTGTTCGGATTAG',\n",
       " 'CTCAACCAGGTTGACAAGTCCT',\n",
       " 'CTCAACCAGGTTTCGAAGAAGG',\n",
       " 'CTGGTTCTCTAACCACCTAAAG',\n",
       " 'CTGGTTCTCTAGGGATCCATTA',\n",
       " 'CTGGTTCTCTAGTCCTGGTACT',\n",
       " 'CTGGTTCTCTATAGCTGTGTCT',\n",
       " 'CTGGTTCTCTATAGGTTTCGCC',\n",
       " 'CTGGTTCTCTCAGTTGACGCTC',\n",
       " 'CTGGTTCTCTCATGTCCGAGTC',\n",
       " 'CTGGTTCTCTCTAAATGTTCGG',\n",
       " 'CTGGTTCTCTGAGACAATGACG',\n",
       " 'CTGGTTCTCTGCTCAGCGTTTA',\n",
       " 'CTGGTTCTCTGGGCTACAGTTC',\n",
       " 'CTGGTTCTCTGTGTAACGCGGT',\n",
       " 'CTGGTTCTCTTCGTGGAATTAC',\n",
       " 'CTGGTTCTCTTGACCACCAATT',\n",
       " 'CTGGTTCTCTTGTCAGGAACAC',\n",
       " 'CTGGTTCTCTTTAGGGACCCTC',\n",
       " 'CTTCACAGTCAATCCCGCGCTT',\n",
       " 'CTTCACAGTCACATCTCTCCCG',\n",
       " 'CTTCACAGTCACTCTTCCTCAT',\n",
       " 'CTTCACAGTCCGCGAGAGCTAT',\n",
       " 'CTTCACAGTCCTTGCGGGTGTT',\n",
       " 'CTTCACAGTCGAACACGTTCCA',\n",
       " 'CTTCACAGTCGCCCGTGTTGAT',\n",
       " 'CTTCACAGTCGTGCTTTCAACA',\n",
       " 'CTTCACAGTCTAGGTAAGACCA',\n",
       " 'CTTCACAGTCTATCGCCGGGTA',\n",
       " 'CTTTCGCGTGATGCGTAGCTGA',\n",
       " 'CTTTCGCGTGCCTCGTCAGAAC',\n",
       " 'CTTTCGCGTGCGCCGAATTACT',\n",
       " 'CTTTCGCGTGTGGTGAGGTGAT',\n",
       " 'GACCAATCCTAAAGCACTAGCG',\n",
       " 'GACCAATCCTAGGCGGCATTAC',\n",
       " 'GACCAATCCTCCAGCTTTCCGT',\n",
       " 'GACCAATCCTCCTAATCCTGAG',\n",
       " 'GACCAATCCTCGACGAGAGATC',\n",
       " 'GACCAATCCTGCTGGTAGATAA',\n",
       " 'GACCAATCCTGTCCACCTAGAC',\n",
       " 'GACCAATCCTGTTGGGCTCAAG',\n",
       " 'GACCAATCCTTTTACCCGTCGA',\n",
       " 'GACCCTCAAAACGCAAGACGAG',\n",
       " 'GACCCTCAAAAGGTCCCAATCG',\n",
       " 'GACCCTCAAAAGTGTCAGTCAA',\n",
       " 'GACCCTCAAACAACCGTACATC',\n",
       " 'GACCCTCAAACGCCACCATTTC',\n",
       " 'GACCCTCAAAGCACACTCTCCT',\n",
       " 'GACCCTCAAAGTCGACAGATAG',\n",
       " 'GACCCTCAAATATAAGTGCTCC',\n",
       " 'GACCCTCAAATGCAGATCGGCT',\n",
       " 'GACCCTCAAATTTCCACGCAGT',\n",
       " 'GACCTTGATGAGGAACGCCGAT',\n",
       " 'GACCTTGATGCTGCTGGGTTCA',\n",
       " 'GACCTTGATGGGCCATCCACAA',\n",
       " 'GAGCACCACTAATCGCTGGATG',\n",
       " 'GAGCACCACTACACGCGTGAAC',\n",
       " 'GAGCACCACTAGATTGGACAAG',\n",
       " 'GAGCACCACTAGGAACTGTCGT',\n",
       " 'GAGCACCACTCACGCAAGGGAA',\n",
       " 'GAGCACCACTCATCTCGACAAT',\n",
       " 'GAGCACCACTCTAAGAGCTCAA',\n",
       " 'GAGCACCACTGACAATCGAGAA',\n",
       " 'GAGCACCACTGAGAGGTCCACT',\n",
       " 'GAGCACCACTGCGCCTGTATAA',\n",
       " 'GAGCACCACTGTACTGGACCCT',\n",
       " 'GAGCACCACTTACGGCTTTCCT',\n",
       " 'GAGCACCACTTGCTAGCCTATA',\n",
       " 'GAGCACCACTTTACTTACACCC',\n",
       " 'GAGGAAGTTGACTGCGACAAAG',\n",
       " 'GAGGAAGTTGCAAACACCACAT',\n",
       " 'GAGGAAGTTGCACCATGCCATT',\n",
       " 'GAGGAAGTTGCACCATTTACCC',\n",
       " 'GAGGAAGTTGCCGGAGAATACC',\n",
       " 'GAGGAAGTTGCTTAGCAAAGAG',\n",
       " 'GAGGAAGTTGGATTCCATTTGG',\n",
       " 'GAGGAAGTTGTGCGTTGATGTG',\n",
       " 'GAGGAAGTTGTTGAGTGCTGTG',\n",
       " 'GAGGTAATCTAATCGCTGGATG',\n",
       " 'GAGGTAATCTCACAACCATGAT',\n",
       " 'GAGGTAATCTCATGTTCTGTGA',\n",
       " 'GAGGTAATCTCTCAGATAAGGG',\n",
       " 'GAGGTAATCTGAAGTTCTTAGG',\n",
       " 'GAGGTAATCTGACGGCATAATG',\n",
       " 'GAGGTAATCTGGCGGGATATTC',\n",
       " 'GAGGTAATCTGGTAATACTCGG',\n",
       " 'GAGGTAATCTGGTTGTCCTCTG',\n",
       " 'GAGGTAATCTGTGAAGCTTATG',\n",
       " 'GAGGTAATCTTAACAGTGCCTT',\n",
       " 'GAGGTAATCTTCATCCGCGACA',\n",
       " 'GAGGTAATCTTTGCTGGATCGT',\n",
       " 'GAGGTAATCTTTTCCACGCAGT',\n",
       " 'GAGTGTGGTTATCATGCGTAGG',\n",
       " 'GAGTGTGGTTCACACCTATATC',\n",
       " 'GAGTGTGGTTCACCTGTAAGAC',\n",
       " 'GAGTGTGGTTCCGGTACATTCC',\n",
       " 'GAGTGTGGTTCTAAGAGCTCAA',\n",
       " 'GAGTGTGGTTGAATCGGTGTTC',\n",
       " 'GAGTGTGGTTGGTTATGAGTCG',\n",
       " 'GAGTGTGGTTTAGGCAGATGGA',\n",
       " 'GAGTGTGGTTTAGGTTGCACTA',\n",
       " 'GAGTGTGGTTTCGACCAGATCT',\n",
       " 'GAGTGTGGTTTCTTTGACGCTG',\n",
       " 'GATCGGGTGAACCGTCCCATTC',\n",
       " 'GATCGGGTGAAGGTTATCGATC',\n",
       " 'GATCGGGTGAATTGCCTTCTTC',\n",
       " 'GATCGGGTGAATTTAGGGCGGG',\n",
       " 'GATCGGGTGACTCGTAGAGCGT',\n",
       " 'GATCGGGTGAGTTCTTGACAAG',\n",
       " 'GATCGGGTGATACACTCCACAA',\n",
       " 'GATCGGGTGATCTTCCTGCAAC',\n",
       " 'GATCGGGTGATTTAACGCCTCT',\n",
       " 'GCCAGTGTTGAAGTGTCGTGGA',\n",
       " 'GCCAGTGTTGATAGGTGGTACC',\n",
       " 'GCCAGTGTTGATCATTGAGGCT',\n",
       " 'GCCAGTGTTGCACGCAAGGGAA',\n",
       " 'GCCAGTGTTGCAGCTCAGGACA',\n",
       " 'GCCAGTGTTGCTGGGAAATGTT',\n",
       " 'GCCAGTGTTGGATAGGTGTAGC',\n",
       " 'GCCAGTGTTGGCTAGTCAAGAG',\n",
       " 'GCCAGTGTTGGGTCGACAGAGA',\n",
       " 'GCCAGTGTTGGTGTCATGAAAG',\n",
       " 'GCCAGTGTTGTAGTCCGAGATC',\n",
       " 'GCCAGTGTTGTCTAATGCCTGC',\n",
       " 'GCCAGTGTTGTGGAACAGTCTG',\n",
       " 'GCTCTTGGAAAATCACGCTTCG',\n",
       " 'GCTCTTGGAAAGTGCCTAACAA',\n",
       " 'GCTCTTGGAACACCTTACTAGA',\n",
       " 'GCTCTTGGAACGCCGAATTACT',\n",
       " 'GCTCTTGGAACGCGTTAATATC',\n",
       " 'GCTCTTGGAACGTCGTGAAATT',\n",
       " 'GCTCTTGGAAGCCACCTGATAC',\n",
       " 'GCTCTTGGAAGCTCTTTCAGAA',\n",
       " 'GCTCTTGGAAGGAACGGTCTCA',\n",
       " 'GCTCTTGGAAGGCGACCTATGT',\n",
       " 'GCTCTTGGAAGTGCTTTCAACA',\n",
       " 'GCTCTTGGAAGTGTAACGCGGT',\n",
       " 'GCTCTTGGAATAACCCAACGAG',\n",
       " 'GCTCTTGGAATAGGCTAGTTTG',\n",
       " 'GCTCTTGGAATTTCGAAGAAGG',\n",
       " 'GCTGATCTTCAACATGAAGCGC',\n",
       " 'GCTGATCTTCACAGGGCCTCTT',\n",
       " 'GCTGATCTTCATAGCCTCCGTA',\n",
       " 'GCTGATCTTCATGCGTAGCTGA',\n",
       " 'GCTGATCTTCCCATCCTATCGT',\n",
       " 'GCTGATCTTCCGAAGATCACTC',\n",
       " 'GCTGATCTTCCGAAGGTTCGAT',\n",
       " 'GCTGATCTTCCTGGGAAATGTT',\n",
       " 'GCTGATCTTCCTTCTAGGCTAC',\n",
       " 'GCTGATCTTCCTTTAACTGCGC',\n",
       " 'GCTGATCTTCGACTTACTGCCG',\n",
       " 'GCTGATCTTCGAGCACATCTCC',\n",
       " 'GCTGATCTTCTGTTCTCTCTGG',\n",
       " 'GCTGGTAGGAAAAGGATCGGCT',\n",
       " 'GCTGGTAGGAACAACAGCGGAT',\n",
       " 'GCTGGTAGGACCACAGCGAAAC',\n",
       " 'GCTGGTAGGACCATCCTATCGT',\n",
       " 'GCTGGTAGGACCTTTCGGCAGT',\n",
       " 'GCTGGTAGGACGGCGACTAACA',\n",
       " 'GCTGGTAGGAGATACTTGCTGG',\n",
       " 'GCTGGTAGGAGGAGTTTGCACA',\n",
       " 'GCTGGTAGGAGGATAAAGAAGG',\n",
       " 'GCTGGTAGGATAGTCCGAGATC',\n",
       " 'GCTGGTAGGATATACGAACCCG',\n",
       " 'GCTGGTAGGATCCACATCACTG',\n",
       " 'GCTGGTAGGATCCAGACGACTT',\n",
       " 'GCTGGTAGGATCTATGCGTACC',\n",
       " 'GCTGGTAGGATTCGGTTCCCAC',\n",
       " 'GCTGGTAGGATTGCATGTACGC',\n",
       " 'GCTTCGCATTCACGCAAGGGAA',\n",
       " 'GCTTCGCATTCAGCGAAGACCT',\n",
       " 'GCTTCGCATTCTTCCTGCTGTC',\n",
       " 'GCTTCGCATTGGATAAAGAAGG',\n",
       " 'GCTTCGCATTGGTTTAGTCGTA',\n",
       " 'GCTTCGCATTTAAGATGCTGGG',\n",
       " 'GCTTCGCATTTTGCTGGATCGT',\n",
       " 'GCTTCGCATTTTGGTGATGTCT',\n",
       " 'GCTTCGCATTTTTAACGCCTCT',\n",
       " 'GGAATTCCGACAAGTGTCTGTA',\n",
       " 'GGAATTCCGACATGTTCTGTGA',\n",
       " 'GGAATTCCGACGCGACTTGAGA',\n",
       " 'GGAATTCCGACGTGCAACACAA',\n",
       " 'GGAATTCCGACTAAGGTCAGAA',\n",
       " 'GGAATTCCGAGCTACAATGTGG',\n",
       " 'GGAATTCCGAGGTGTTCCCTAC',\n",
       " 'GGAATTCCGATGAACCATCGTA',\n",
       " 'GGAATTCCGATTACTTACACCC',\n",
       " 'GGAGCCATTGAGATACCTGGAG',\n",
       " 'GGAGCCATTGAGGTGCAAGGTA',\n",
       " 'GGAGCCATTGATGTCGCCTGAA',\n",
       " 'GGAGCCATTGCCACCGCCTTAA',\n",
       " 'GGAGCCATTGCGCAAGTAAAGC',\n",
       " 'GGAGCCATTGCGTGTACTGAAT',\n",
       " 'GGAGCCATTGCTTAGACGTCTT',\n",
       " 'GGAGCCATTGGAGGAATCACGG',\n",
       " 'GGAGCCATTGGATGAACATGTC',\n",
       " 'GGAGCCATTGGATGGCAAACAT',\n",
       " 'GGAGCCATTGGCGTAACCAGTA',\n",
       " 'GGAGCCATTGGGCATAATGATC',\n",
       " 'GGAGCCATTGTCCGTAAGGTTC',\n",
       " 'GGCCTATGTTGATACGCCGTAG',\n",
       " 'GGCCTATGTTGATTCCATTTGG',\n",
       " 'GGCCTATGTTTAGATCCACGTA',\n",
       " 'GGTGACATACAACTAACGTCGA',\n",
       " 'GGTGACATACCAATCTGACGTA',\n",
       " 'GGTGACATACCGAAACTTACGG',\n",
       " 'GGTGACATACCGTGATGAGGTA',\n",
       " 'GGTGACATACCTCGTAGAGCGT',\n",
       " 'GGTGACATACCTTGGATGATGC',\n",
       " 'GGTGACATACCTTTGGGACGGT',\n",
       " 'GGTGACATACGAATCGGTGTTC',\n",
       " 'GGTGACATACGCTGATCCACCT',\n",
       " 'GGTGACATACTACTAGGGTTAG',\n",
       " 'GGTGACATACTTTCCACGCAGT',\n",
       " 'GTGGGCAATAACAACAGCGGAT',\n",
       " 'GTGGGCAATACTTGCGGGTGTT',\n",
       " 'GTGGGCAATAGGCTGATATGCT',\n",
       " 'GTGGGCAATAGTGCCTTTAAGC',\n",
       " 'GTGGTTGCCAAACATGAAGCGC',\n",
       " 'GTGGTTGCCAAAGCGGGCTTGA',\n",
       " 'GTGGTTGCCAAGCAGTTGGTGG',\n",
       " 'GTGGTTGCCAAGGCGACTACAC',\n",
       " 'GTGGTTGCCAATCAGTCAAGGA',\n",
       " 'GTGGTTGCCACACAAATCAACC',\n",
       " 'GTGGTTGCCACCGGTACATTCC',\n",
       " 'GTGGTTGCCACCTGATAACCAA',\n",
       " 'GTGGTTGCCACGCGATACAAAT',\n",
       " 'GTGGTTGCCACTGGGAAATGTT',\n",
       " 'GTGGTTGCCACTTAGCAAAGAG',\n",
       " 'GTGGTTGCCAGATCCATTCGTG',\n",
       " 'GTGGTTGCCAGATTCGCGTGCA',\n",
       " 'GTGGTTGCCAGCTTTCCAACGG',\n",
       " 'GTGGTTGCCAGTTGCGAATGGG',\n",
       " 'GTGGTTGCCATCGCTTCCGGTT',\n",
       " 'GTGGTTGCCATTGAGTGCTGTG',\n",
       " 'GTGGTTGCCATTTGTGTTACCG',\n",
       " 'GTGTGACCCTAACCATGCATGA',\n",
       " 'GTGTGACCCTATGTCTAACGCC',\n",
       " 'GTGTGACCCTCTGTCTACACGA',\n",
       " 'GTGTGACCCTGACGGCATAATG',\n",
       " 'GTGTGACCCTGCTTGATCCCAG',\n",
       " 'GTGTGACCCTGTCACGCCCAAT',\n",
       " 'GTGTGACCCTTAAACGCTCCAG',\n",
       " 'GTGTGACCCTTACTAGGGTTAG',\n",
       " 'GTGTGACCCTTAGCGTCATATG',\n",
       " 'GTGTGACCCTTAGTCCACCAGT',\n",
       " 'GTGTGACCCTTGCGAAGCTCAC',\n",
       " 'GTGTGACCCTTTGTCCTCGACG',\n",
       " 'GTGTGATGTAAATCGCTGGATG',\n",
       " 'GTGTGATGTAATTTCGATGACG',\n",
       " 'GTGTGATGTACGCCCTGCTATA',\n",
       " 'GTGTGATGTACTCACTGGATAT',\n",
       " 'GTGTGATGTAGGTCGTCAATCA',\n",
       " 'GTGTGATGTAGTACAACTCTAG',\n",
       " 'GTGTGATGTATCTAATGCCTGC',\n",
       " 'GTGTTCGCAGACCACGGCTGTT',\n",
       " 'GTGTTCGCAGCAGACCTGCTCA',\n",
       " 'GTGTTCGCAGCCCAATACGTGG',\n",
       " 'GTGTTCGCAGCCGGATCAAGTT',\n",
       " 'GTGTTCGCAGCGGCGACTAACA',\n",
       " 'GTGTTCGCAGCTCCCGTGTAAG',\n",
       " 'GTGTTCGCAGCTGTCTACACGA',\n",
       " 'GTGTTCGCAGGAGTGTGGCATA',\n",
       " 'GTGTTCGCAGGATAGACGAAGA',\n",
       " 'GTGTTCGCAGGATGAACATGTC',\n",
       " 'GTGTTCGCAGGCTATGTCTCTC',\n",
       " 'GTGTTCGCAGGGTAGAGAGCTC',\n",
       " 'GTGTTCGCAGTATCAAAGGGTC',\n",
       " 'GTGTTCGCAGTTCCGAGCAACT',\n",
       " 'GTTCTCTCCTAAACTAGCCCTA',\n",
       " 'GTTCTCTCCTATCGAGTTAGGC',\n",
       " 'GTTCTCTCCTCTGACGAGAAAC',\n",
       " 'GTTCTCTCCTGGGTTGAACCTA',\n",
       " 'GTTCTCTCCTGGTGGCCGTATA',\n",
       " 'GTTCTCTCCTGTATGCCGAGAA',\n",
       " 'GTTCTCTCCTTACCTCCAACTT',\n",
       " 'GTTCTCTCCTTAGATCTGCCAA',\n",
       " 'TAAGAGGCCGAACTGATAGGAG',\n",
       " 'TAAGAGGCCGAAGCGCCATCGA',\n",
       " 'TAAGAGGCCGAGGTCACGACTA',\n",
       " 'TAAGAGGCCGGTACCTATTCCA',\n",
       " 'TAAGAGGCCGTGTGATTCTGTG',\n",
       " 'TAAGGTAGGGACAGGGCCTCTT',\n",
       " 'TAAGGTAGGGGAGCATCACTAC',\n",
       " 'TAAGGTAGGGGAGGCATTTGCA',\n",
       " 'TAAGGTAGGGGCCACCTGATAC',\n",
       " 'TAAGGTAGGGGCTATGTCTCTC',\n",
       " 'TAAGGTAGGGGGGAACAAGTCA',\n",
       " 'TAAGGTAGGGTCCAGACGACTT',\n",
       " 'TAAGGTAGGGTGGAGTAACCAT',\n",
       " 'TACCGTACCTAACCCACTATCT',\n",
       " 'TACCGTACCTACCGTCCCATTC',\n",
       " 'TACCGTACCTCAAGTGTCTGTA',\n",
       " 'TACCGTACCTCTGGGAAATGTT',\n",
       " 'TACCGTACCTGCGAACGTAGAC',\n",
       " 'TACCGTACCTGCTACAATGTGG',\n",
       " 'TACCGTACCTGCTCAGCGTTTA',\n",
       " 'TACCGTACCTGCTTAAGCCGTT',\n",
       " 'TACCGTACCTGGAAGTTCAGCA',\n",
       " 'TACCGTACCTGGGCAGTGGTAA',\n",
       " 'TACCGTACCTGTAGCCTGTTAC',\n",
       " 'TACCGTACCTTACCCAATGAAC',\n",
       " 'TACCGTACCTTACGATCTATCG',\n",
       " 'TACCGTACCTTAGCGTCATATG',\n",
       " 'TACCGTACCTTCCCTATAGCCA',\n",
       " 'TACTCGATTCAAACGGATCAGT',\n",
       " 'TACTCGATTCAGCGAATAACCC',\n",
       " 'TACTCGATTCCGATTTCACCGA',\n",
       " 'TACTCGATTCCGCGATACAAAT',\n",
       " 'TACTCGATTCCGCTATCTCTTG',\n",
       " 'TACTCGATTCCGGAACTATACT',\n",
       " 'TACTCGATTCGATATGGTGCCA',\n",
       " 'TACTCGATTCGCGACGTTACGA',\n",
       " 'TACTCGATTCGCTACAATGTGG',\n",
       " 'TACTCGATTCGGGACTAATTCC',\n",
       " 'TACTCGATTCTTAAACGCGAAG',\n",
       " 'TACTCGATTCTTTCCACGCAGT',\n",
       " 'TAGACCAGGGAAAGCCCACGAC',\n",
       " 'TAGACCAGGGAACCACCTAAAG',\n",
       " 'TAGACCAGGGACAGATCAACGC',\n",
       " 'TAGACCAGGGACTAGTCCGGAA',\n",
       " 'TAGACCAGGGACTGCGACAAAG',\n",
       " 'TAGACCAGGGAGGGAATGTTCA',\n",
       " 'TAGACCAGGGATAAGGTCCTGA',\n",
       " 'TAGACCAGGGATCCAAGCCGTC',\n",
       " 'TAGACCAGGGCCTCATAGTAGA',\n",
       " 'TAGACCAGGGCCTTATATGTCC',\n",
       " 'TAGACCAGGGCGATTTCACCGA',\n",
       " 'TAGACCAGGGGACGCAACCTCT',\n",
       " 'TAGACCAGGGGAGTGGTTGACC',\n",
       " 'TAGACCAGGGGCTATTGGTAGG',\n",
       " 'TAGACCAGGGGCTTCTCACAAG',\n",
       " 'TAGACCAGGGGTGAAGCTTATG',\n",
       " 'TAGACCAGGGTAACAGTGCCTT',\n",
       " 'TAGACCAGGGTCGCGGAAAGTC',\n",
       " 'TAGACCAGGGTGCGTTGATGTG',\n",
       " 'TAGACCAGGGTGTCCTGCGGTA',\n",
       " 'TAGACCAGGGTTAGAGCCCACT',\n",
       " 'TAGACCAGGGTTAGGTAGCAAC',\n",
       " 'TAGACCAGGGTTAGGTAGTGGG',\n",
       " 'TAGACCAGGGTTGCATGTACGC',\n",
       " 'TAGACTTGACAACTGCGTCTTT',\n",
       " 'TAGACTTGACAGTTTGGAGCAT',\n",
       " 'TAGACTTGACATGTGCATCTGG',\n",
       " 'TAGACTTGACCGAAGGGATCAT',\n",
       " 'TAGACTTGACCGGCGACTAACA',\n",
       " 'TAGACTTGACCGTCGTGAAATT',\n",
       " 'TAGACTTGACCTTAGGTAGCGT',\n",
       " 'TAGACTTGACCTTGCGGGTGTT',\n",
       " 'TAGACTTGACGATGAAATCGGA',\n",
       " 'TAGACTTGACGCTCAGCGTTTA',\n",
       " 'TAGACTTGACGGCACTTCATCT',\n",
       " 'TAGACTTGACTACGTGCAGTAT',\n",
       " 'TAGACTTGACTATCCATGACCC',\n",
       " 'TAGACTTGACTGCTAGCCTATA',\n",
       " 'TAGGGTGGCAACCCACAGCAGT',\n",
       " 'TAGGGTGGCAATCGAGGCCTAA',\n",
       " 'TAGGGTGGCAATTCATTTCCGC',\n",
       " 'TAGGGTGGCAATTGGGCTAGTT',\n",
       " 'TAGGGTGGCACAACCACGAGTG',\n",
       " 'TAGGGTGGCACGCGCTATACGA',\n",
       " 'TAGGGTGGCACGGATCACCTGA',\n",
       " 'TAGGGTGGCAGATACGCCGTAG',\n",
       " 'TAGGGTGGCAGGATAAAGAAGG',\n",
       " 'TAGGGTGGCAGTTGTGATGTGC',\n",
       " 'TAGGGTGGCATCGCGAGATAGT',\n",
       " 'TAGGGTGGCATGTTCTCTCTGG',\n",
       " 'TAGTGCTGTCCGATATCGATCC',\n",
       " 'TAGTGCTGTCTAGGCAGATGGA',\n",
       " 'TATGTATGGCAAGTTGTGCTAC',\n",
       " 'TATGTATGGCAATTCTGTTGCG',\n",
       " 'TATGTATGGCCACACCTATATC',\n",
       " 'TATGTATGGCCGAAGGTTCGAT',\n",
       " 'TATGTATGGCCTAGACGACCTG',\n",
       " 'TATGTATGGCCTGGTAGCGTAT',\n",
       " 'TATGTATGGCGTTTGCCACACA',\n",
       " 'TATGTATGGCTATCAAAGGGTC',\n",
       " 'TATGTATGGCTCGACCAGATCT',\n",
       " 'TCACACGTCGAGCCATGAGAAT',\n",
       " 'TCACACGTCGCGAAGGTTCGAT',\n",
       " 'TCACACGTCGCTACTCGATCTC',\n",
       " 'TCACACGTCGGATTTCCGGACT',\n",
       " 'TCACACGTCGGCTTTGCGTCGT',\n",
       " 'TCACACGTCGGTAACTTGCCCT',\n",
       " 'TCACACGTCGTCACCAAACCGG',\n",
       " 'TCACACGTCGTGGGTTTGTACT',\n",
       " 'TCACACGTCGTTGTCCTCGACG',\n",
       " 'TCACAGAGAGAAATGCTACGGG',\n",
       " 'TCACAGAGAGATGTCTAACGCC',\n",
       " 'TCACAGAGAGCCACAGCGAAAC',\n",
       " 'TCACAGAGAGCCACTCTAGGTC',\n",
       " 'TCACAGAGAGCGCCCTGCTATA',\n",
       " 'TCACAGAGAGCGCGACTTGAGA',\n",
       " 'TCACAGAGAGCTCACAAGCCTA',\n",
       " 'TCACAGAGAGCTTACAATGCGA',\n",
       " 'TCACAGAGAGCTTAGCGTGAGT',\n",
       " 'TCACAGAGAGGGATAAAGAAGG',\n",
       " 'TCACAGAGAGTACCATCCCAGG',\n",
       " 'TCACAGAGAGTCTCTGACTTGA',\n",
       " 'TCACAGAGAGTTAGTGCTAGCA',\n",
       " 'TCCGACGCTAAAACTAGCCCTA',\n",
       " 'TCCGACGCTAAAGCGGAACCTA',\n",
       " 'TCCGACGCTAAATCGTCTGGAC',\n",
       " 'TCCGACGCTAAGATCTGTGACG',\n",
       " 'TCCGACGCTACAGCGTTGGTAC',\n",
       " 'TCCGACGCTACATGTTCTGTGA',\n",
       " 'TCCGACGCTACCCATGACGGAA',\n",
       " 'TCCGACGCTACGCGATACAAAT',\n",
       " 'TCCGACGCTACTTGAGACTCCG',\n",
       " 'TCCGACGCTAGACAATAGGAGA',\n",
       " 'TCCGACGCTAGATTCGCGTGCA',\n",
       " 'TCCGACGCTAGTTACAGAACGC',\n",
       " 'TCCGACGCTAGTTCTTGACAAG',\n",
       " 'TCCGACGCTATTCTCATGGTCA',\n",
       " 'TCGGCTTATTAACATGAAGCGC',\n",
       " 'TCGGCTTATTAACTAACGTCGA',\n",
       " 'TCGGCTTATTACATCTAAGGAG',\n",
       " 'TCGGCTTATTACGCTTACCCGA',\n",
       " 'TCGGCTTATTAGCATACTCAGC',\n",
       " 'TCGGCTTATTCACCGAAGGTCA',\n",
       " 'TCGGCTTATTCACTGTGTTATG',\n",
       " 'TCGGCTTATTCAGAATCCTTCC',\n",
       " 'TCGGCTTATTCGAGAATCGACG',\n",
       " 'TCGGCTTATTCTCACTGGATAT',\n",
       " 'TCGGCTTATTCTGTCTACACGA',\n",
       " 'TCGGCTTATTGAAGGGTGGTAT',\n",
       " 'TCGGCTTATTGCTTTGCGTCGT',\n",
       " 'TCGGCTTATTGGGACTAATTCC',\n",
       " 'TCGGCTTATTTAACAGTGCCTT',\n",
       " 'TCGGCTTATTTCATCCGCGACA',\n",
       " 'TCGGCTTATTTGAACGCAATCC',\n",
       " 'TCGGCTTATTTGAGCTAGACGC',\n",
       " 'TCGGCTTATTTTCAGTCGCCTA',\n",
       " 'TCGGCTTATTTTCCGAGCAACT',\n",
       " 'TCGGCTTATTTTTCCACGCAGT',\n",
       " 'TCGTTGCTCTCAGTTATAGGCC',\n",
       " 'TCGTTGCTCTCGAAGATCACTC',\n",
       " 'TCGTTGCTCTGGCATAATGATC',\n",
       " 'TCGTTGCTCTGTTGCGAATGGG',\n",
       " 'TCGTTGCTCTTAACAGTGCCTT',\n",
       " 'TCGTTGCTCTTAACCCAACGAG',\n",
       " 'TCGTTGCTCTTCGTCACTTAAG',\n",
       " 'TCGTTGCTCTTCTAATTGCGAC',\n",
       " 'TCGTTGCTCTTGACTATTTCCC',\n",
       " 'TCGTTGCTCTTGCTGAGTTCCT',\n",
       " 'TCTTGTTGCCACTCTTCCTCAT',\n",
       " 'TCTTGTTGCCCAAATCTCGCCG',\n",
       " 'TCTTGTTGCCCGATATCGATCC',\n",
       " 'TCTTGTTGCCCTCAGATAAGGG',\n",
       " 'TCTTGTTGCCGACTTACTGCCG',\n",
       " 'TCTTGTTGCCTACGGCTTTCCT',\n",
       " 'TGAGCGAGTAAAAGGATCGGCT',\n",
       " 'TGAGCGAGTAACCCGATGGCTA',\n",
       " 'TGAGCGAGTAAGTCGGTTGTGA',\n",
       " 'TGAGCGAGTACAGTTATAGGCC',\n",
       " 'TGAGCGAGTACGACGAGAGATC',\n",
       " 'TGAGCGAGTACGTCCCTAGAAC',\n",
       " 'TGAGCGAGTAGATTTCCGGACT',\n",
       " 'TGAGCGAGTAGTTGCGAATGGG',\n",
       " 'TGAGCGAGTATAGGCAGATGGA',\n",
       " 'TGAGCGAGTATCGCGATCTGAC',\n",
       " 'TGAGCGAGTATCGTGGAATTAC',\n",
       " 'TGAGCGAGTATGCTAATGACGT',\n",
       " 'TGAGCGAGTATTCTCATGGTCA',\n",
       " 'TGATTACGCGAAAGGATCGGCT',\n",
       " 'TGATTACGCGACGCCGTGACTT',\n",
       " 'TGATTACGCGAGGTCACGACTA',\n",
       " 'TGATTACGCGCACGCAAGGGAA',\n",
       " 'TGATTACGCGCAGTGATTCTGT',\n",
       " 'TGATTACGCGCAGTTGACGCTC',\n",
       " 'TGATTACGCGCATACCCGGTTC',\n",
       " 'TGATTACGCGCGGATCACCTGA',\n",
       " 'TGATTACGCGTATCCATGACCC',\n",
       " 'TGATTACGCGTATGGTCCAGAC',\n",
       " 'TGATTACGCGTTAGCTTCTCTG',\n",
       " 'TGATTACGCGTTAGGGACCCTC',\n",
       " 'TGGAGGACGAAACGACTGCTCG',\n",
       " 'TGGAGGACGAAGGAACGCCGAT',\n",
       " ...]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_insertsize</th>\n",
       "      <th>insertsize_count</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAATCCGCATAAACGTCCCGTT</th>\n",
       "      <td>200.82</td>\n",
       "      <td>5761</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAATCCGCATAAATGCTACGGG</th>\n",
       "      <td>162.90</td>\n",
       "      <td>15741</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 1, 3, 9, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAATCCGCATAACATGAAGCGC</th>\n",
       "      <td>191.14</td>\n",
       "      <td>2638</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAATCCGCATAACCGCTAATGA</th>\n",
       "      <td>174.73</td>\n",
       "      <td>4213</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAATCCGCATAACTTCGACCAG</th>\n",
       "      <td>184.72</td>\n",
       "      <td>17424</td>\n",
       "      <td>[0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 14, 0, 2, 3, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTCGTCCGACTGCCGTCTCAAC</th>\n",
       "      <td>184.97</td>\n",
       "      <td>3669</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTCGTCCGACTGCGTCGAGTAC</th>\n",
       "      <td>194.13</td>\n",
       "      <td>11340</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTCGTCCGACTTGAGTGCTGTG</th>\n",
       "      <td>186.06</td>\n",
       "      <td>3786</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTCGTCCGACTTTCGAAGAAGG</th>\n",
       "      <td>191.28</td>\n",
       "      <td>5783</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TTCGTCCGACTTTGTGTTACCG</th>\n",
       "      <td>188.35</td>\n",
       "      <td>4693</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9110 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        mean_insertsize  insertsize_count  \\\n",
       "AAATCCGCATAAACGTCCCGTT           200.82              5761   \n",
       "AAATCCGCATAAATGCTACGGG           162.90             15741   \n",
       "AAATCCGCATAACATGAAGCGC           191.14              2638   \n",
       "AAATCCGCATAACCGCTAATGA           174.73              4213   \n",
       "AAATCCGCATAACTTCGACCAG           184.72             17424   \n",
       "...                                 ...               ...   \n",
       "TTCGTCCGACTGCCGTCTCAAC           184.97              3669   \n",
       "TTCGTCCGACTGCGTCGAGTAC           194.13             11340   \n",
       "TTCGTCCGACTTGAGTGCTGTG           186.06              3786   \n",
       "TTCGTCCGACTTTCGAAGAAGG           191.28              5783   \n",
       "TTCGTCCGACTTTGTGTTACCG           188.35              4693   \n",
       "\n",
       "                                                                     dist  \n",
       "AAATCCGCATAAACGTCCCGTT  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, ...  \n",
       "AAATCCGCATAAATGCTACGGG  [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 1, 3, 9, ...  \n",
       "AAATCCGCATAACATGAAGCGC  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "AAATCCGCATAACCGCTAATGA  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, ...  \n",
       "AAATCCGCATAACTTCGACCAG  [0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 14, 0, 2, 3, 10...  \n",
       "...                                                                   ...  \n",
       "TTCGTCCGACTGCCGTCTCAAC  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "TTCGTCCGACTGCGTCGAGTAC  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, ...  \n",
       "TTCGTCCGACTTGAGTGCTGTG  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...  \n",
       "TTCGTCCGACTTTCGAAGAAGG  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "TTCGTCCGACTTTGTGTTACCG  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[9110 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiprocessing.synchronize.Lock"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lock = Lock()\n",
    "type(lock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_lock = Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lock) == type(Lock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count insertsizes from fragments...\n",
      "Starting counting fragments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "chunk = insertsize_from_fragments(fragments=fragments_file, barcodes=barcodes,\n",
    "                              n_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock lock object\n",
    "lock_instance = Lock()\n",
    "\n",
    "# Call the function with the mock lock\n",
    "insertsizes.init_pool_processes(lock_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertsizes._count_fragments_worker(chunk, managed_dict=managed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "managed_dict['output']['AGGGATAAACCACCGAAGGTCA']['dist'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(managed_dict['output']['AGGGATAAACCACCGAAGGTCA']['mean_insertsize']) == 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "managed_dict = {'output': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk.to_csv('example_chunk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_chunk = pd.read_csv('example_chunk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chr        True\n",
       "start      True\n",
       "stop       True\n",
       "barcode    True\n",
       "count      True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(chunk == read_chunk).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/workspace2/jdetlef/repos/PEAK_QC/experimental'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def _is_gz_file(filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check wheather file is a compressed .gz file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the file is a compressed .gz file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'rb') as test_f:\n",
    "        return test_f.read(2) == b'\\x1f\\x8b'\n",
    "\n",
    "\n",
    "@beartype\n",
    "def init_pool_processes(the_lock: Any) -> None:\n",
    "    '''\n",
    "    Initialize each process with a global variable lock.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    the_lock : Any\n",
    "        Lock object to be used by the processes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    global lock\n",
    "    lock = the_lock\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _check_in_list(element: Any, alist: list[Any] | set[Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if element is in list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    element : Any\n",
    "        Element that is checked for.\n",
    "    alist : list[Any] | set[Any]\n",
    "        List or set in which the element is searched for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if element is in list else False\n",
    "    \"\"\"\n",
    "\n",
    "    return element in alist\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _check_true(element: Any, alist: Optional[list[Any]] = None) -> bool:  # true regardless of input\n",
    "    \"\"\"\n",
    "    Return True regardless of input\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    element : Any\n",
    "        Element that is checked for.\n",
    "    alist: Optional[list[Any]]\n",
    "        List or set in which the element is searched for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if element is in list else False\n",
    "    \"\"\"\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _custom_callback(error: Exception) -> None:\n",
    "    \"\"\"\n",
    "    Error callback function for multiprocessing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    error : Exception\n",
    "        Error that is raised.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(error, flush=True)\n",
    "\n",
    "\n",
    "@beartype\n",
    "def insertsize_from_fragments(fragments: str,\n",
    "                              barcodes: Optional[list[str]] = None,\n",
    "                              n_threads: int = 8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Count the insertsizes of fragments in a fragments file and get basic statistics (mean and total count) per barcode.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fragments : str\n",
    "        Path to fragments file.\n",
    "    barcodes : list[str], optional\n",
    "        List of barcodes to count. If None, all barcodes are counted.\n",
    "    n_threads : int, default 8\n",
    "        Number of threads to use for multiprocessing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe containing the mean insertsizes and total counts per barcode.\n",
    "    \"\"\"\n",
    "    print('Count insertsizes from fragments...')\n",
    "    # Open fragments file\n",
    "    if _is_gz_file(fragments):\n",
    "        f = gzip.open(fragments, \"rt\")\n",
    "    else:\n",
    "        f = open(fragments, \"r\")\n",
    "\n",
    "    # Prepare function for checking against barcodes list\n",
    "    if barcodes is not None:\n",
    "        barcodes = set(barcodes)\n",
    "        check_in = _check_in_list\n",
    "    else:\n",
    "        check_in = _check_true\n",
    "\n",
    "    # Initialize iterator\n",
    "    iterator = pd.read_csv(fragments,\n",
    "                           delimiter='\\t',\n",
    "                           header=None,\n",
    "                           names=['chr', 'start', 'stop', 'barcode', 'count'],\n",
    "                           iterator=True,\n",
    "                           chunksize=1000000)\n",
    "\n",
    "    # start timer\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Initialize multiprocessing\n",
    "    m = Manager() # initialize manager\n",
    "    lock = Lock() # initialize lock\n",
    "    managed_dict = m.dict() # initialize managed dict\n",
    "    managed_dict['output'] = {}\n",
    "    # initialize pool\n",
    "    pool = Pool(processes=n_threads,\n",
    "                initializer=init_pool_processes,\n",
    "                initargs=(lock,),\n",
    "                maxtasksperchild=48)\n",
    "    jobs = []\n",
    "    print('Starting counting fragments...')\n",
    "    # split fragments into chunks\n",
    "    for chunk in tqdm(iterator, desc=\"Processing Chunks\"):\n",
    "        return chunk\n",
    "        # apply async job wit callback function\n",
    "        job = pool.apply_async(_count_fragments_worker,\n",
    "                               args=(chunk, barcodes, check_in, managed_dict),\n",
    "                               error_callback=_custom_callback)\n",
    "        jobs.append(job)\n",
    "\n",
    "    # close pool\n",
    "    pool.close()\n",
    "    # wait for all jobs to finish\n",
    "    pool.join()\n",
    "    # reset settings\n",
    "    count_dict = managed_dict['output']\n",
    "\n",
    "    # Close file and print elapsed time\n",
    "    end_time = datetime.datetime.now()\n",
    "    f.close()\n",
    "    elapsed = end_time - start_time\n",
    "    print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "    # Convert dict to pandas dataframe\n",
    "    print(\"Converting counts to dataframe...\")\n",
    "    table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "    # round mean_insertsize to 2 decimals\n",
    "    table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "    print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def _count_fragments_worker(chunk: pd.DataFrame,\n",
    "                            barcodes: Optional[list[str]] = None,\n",
    "                            check_in: Any = _check_true,\n",
    "                            managed_dict: dict = {'output': {}}) -> None:\n",
    "    \"\"\"\n",
    "    Worker function for counting fragments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk : pd.DataFrame\n",
    "        Chunk of fragments file.\n",
    "    barcodes : list[str], optional\n",
    "        List of barcodes to count. If None, all barcodes are counted.\n",
    "    check_in : Any, default _check_true\n",
    "        Function for checking if barcode is in barcodes list.\n",
    "    managed_dict : dict, default None\n",
    "        Dictionary for multiprocessing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize count_dict\n",
    "    count_dict = {}\n",
    "    # Iterate over chunk\n",
    "    for row in chunk.itertuples():\n",
    "        start = int(row[2])\n",
    "        end = int(row[3])\n",
    "        barcode = row[4]\n",
    "        count = int(row[5])\n",
    "        size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "        # Only add fragment if check is true\n",
    "        if check_in(barcode, barcodes) is True:\n",
    "            count_dict = _add_fragment(count_dict, barcode, size, count) # add fragment to count_dict\n",
    "\n",
    "    # Update managed_dict\n",
    "    lock.acquire() # acquire lock\n",
    "    latest = managed_dict['output']\n",
    "    managed_dict['output'] = _update_count_dict(latest, count_dict) # update managed dict\n",
    "    lock.release() # release lock\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _add_fragment(count_dict: dict[str, int],\n",
    "                  barcode: str,\n",
    "                  size: int,\n",
    "                  count: int = 1,\n",
    "                  max_size: int=1000) -> dict:\n",
    "    \"\"\"\n",
    "    Add fragment of size 'size' to count_dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    count_dict : dict[str, int]\n",
    "        Dictionary containing the counts per insertsize.\n",
    "    barcode : str\n",
    "        Barcode of the read.\n",
    "    size : int\n",
    "        Insertsize to add to count_dict.\n",
    "    count : int, default 1\n",
    "        Number of reads to add to count_dict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Updated count_dict.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize if barcode is seen for the first time\n",
    "    if barcode not in count_dict:\n",
    "        count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "    # Add read to dict\n",
    "    if size > 0 and size <= max_size:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "        count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "        # Update mean\n",
    "        mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "        total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "        diff = (size - mu) / total_count\n",
    "        count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "        # Save to distribution\n",
    "        if 'dist' not in count_dict[barcode]:  # initialize distribution\n",
    "            count_dict[barcode]['dist'] = np.zeros(max_size + 1)\n",
    "        count_dict[barcode]['dist'][size] += count # add count to distribution\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _update_count_dict(count_dict_1: dict, count_dict_2: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Updates the managed dict with the new counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    count_dict_1 : dict\n",
    "        Dictionary containing the counts per insertsize.\n",
    "    count_dict_2 : dict\n",
    "        Dictionary containing the counts per insertsize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Updated count_dict.\n",
    "    \"\"\"\n",
    "    # Check if count_dict_1 is empty:\n",
    "    if len(count_dict_1) == 0:\n",
    "        return count_dict_2\n",
    "    # Check if count_dict_2 is empty\n",
    "    if len(count_dict_2) == 0:\n",
    "        return count_dict_1\n",
    "\n",
    "    # make Dataframes for computation\n",
    "    df1 = pd.DataFrame(count_dict_1).T\n",
    "    df2 = pd.DataFrame(count_dict_2).T\n",
    "\n",
    "    # merge distributions\n",
    "    combined_dists = df1['dist'].combine(df2['dist'], func=_update_dist)\n",
    "    # merge counts\n",
    "    merged_counts = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True,\n",
    "                             how='outer').fillna(0)\n",
    "    # sum total counts/barcode\n",
    "    updated_counts = merged_counts.sum(axis=1)\n",
    "\n",
    "    # calculate scaling factors\n",
    "    x_scaling_factor = merged_counts[\"insertsize_count_x\"] / updated_counts\n",
    "    y_scaling_factor = merged_counts[\"insertsize_count_y\"] / updated_counts\n",
    "\n",
    "    # merge mean insertsizes\n",
    "    merged_mean_insertsizes = pd.merge(df1[\"mean_insertsize\"], df2[\"mean_insertsize\"], left_index=True,\n",
    "                                       right_index=True, how='outer').fillna(0)\n",
    "\n",
    "    # scale mean insertsizes\n",
    "    merged_mean_insertsizes[\"mean_insertsize_x\"] = merged_mean_insertsizes[\"mean_insertsize_x\"] * x_scaling_factor\n",
    "    merged_mean_insertsizes[\"mean_insertsize_y\"] = merged_mean_insertsizes[\"mean_insertsize_y\"] * y_scaling_factor\n",
    "\n",
    "    # sum the scaled means\n",
    "    updated_means = merged_mean_insertsizes.sum(axis=1)\n",
    "\n",
    "    # build the updated dictionary\n",
    "    updated_dict = pd.DataFrame(\n",
    "        {'mean_insertsize': updated_means, 'insertsize_count': updated_counts, 'dist': combined_dists}).T.to_dict()\n",
    "\n",
    "    return updated_dict\n",
    "\n",
    "\n",
    "@beartype\n",
    "def _update_dist(dist_1: npt.ArrayLike, dist_2: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    \"\"\"\n",
    "    Updates the Insertsize Distributions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dist_1 : npt.ArrayLike\n",
    "        Insertsize distribution 1.\n",
    "    dist_2 : npt.ArrayLike\n",
    "        Insertsize distribution 2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    npt.ArrayLike\n",
    "        Updated insertsize distribution.\n",
    "    \"\"\"\n",
    "    # check if both distributions are not empty\n",
    "    if not np.isnan(dist_1).any() and not np.isnan(dist_2).any():\n",
    "        updated_dist = dist_1 + dist_2 # add distributions\n",
    "        return updated_dist.astype(int)\n",
    "    # if one of the distributions is empty, return the other one\n",
    "    elif np.isnan(dist_1).any():\n",
    "        return dist_2.astype(int)\n",
    "    elif np.isnan(dist_2).any():\n",
    "        return dist_1.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertsize_from_fragments(fragments=fragments_file,\n",
    "                              barcodes=barcodes,\n",
    "                              n_threads = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sctoolbox.tools as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools._insertsize_from_fragments(fragments=fragments_file,\n",
    "                              barcodes=barcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peakqc.general as utils\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def open_bam(file: str,\n",
    "             mode: str,\n",
    "             verbosity: Literal[0, 1, 2, 3] = 3, **kwargs: Any) -> \"pysam.AlignmentFile\":\n",
    "    \"\"\"\n",
    "    Open bam file with pysam.AlignmentFile. On a specific verbosity level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        Path to bam file.\n",
    "    mode : str\n",
    "        Mode to open the file in. See pysam.AlignmentFile\n",
    "    verbosity : Literal[0, 1, 2, 3], default 3\n",
    "        Set verbosity level. Verbosity level 0 for no messages.\n",
    "    **kwargs : Any\n",
    "        Forwarded to pysam.AlignmentFile\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pysam.AlignmentFile\n",
    "        Object to work on SAM/BAM files.\n",
    "    \"\"\"\n",
    "\n",
    "    # check then load modules\n",
    "    utils.check_module(\"pysam\")\n",
    "    import pysam\n",
    "\n",
    "    # save verbosity, then set temporary one\n",
    "    former_verbosity = pysam.get_verbosity()\n",
    "    pysam.set_verbosity(verbosity)\n",
    "\n",
    "    # open file\n",
    "    handle = pysam.AlignmentFile(file, mode, **kwargs)\n",
    "\n",
    "    # return to former verbosity\n",
    "    pysam.set_verbosity(former_verbosity)\n",
    "\n",
    "    return handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_run = time.time()\n",
    "\n",
    "regions=None\n",
    "bam = bamfile\n",
    "chunk_size = 100000\n",
    "n_threads=10\n",
    "barcode_tag = 'CB'\n",
    "\n",
    "utils.check_module(\"pysam\")\n",
    "import pysam\n",
    "\n",
    "if isinstance(regions, str):\n",
    "    regions = [regions]\n",
    "\n",
    "# Prepare function for checking against barcodes list\n",
    "if barcodes is not None:\n",
    "    barcodes = set(barcodes)\n",
    "    check_in = _check_in_list\n",
    "else:\n",
    "    check_in = _check_true\n",
    "    \n",
    "# Open bamfile\n",
    "print(\"Opening bam file...\")\n",
    "if not os.path.exists(bam + \".bai\"):\n",
    "    print(\"Bamfile has no index - trying to index with pysam...\")\n",
    "    pysam.index(bam)\n",
    "\n",
    "bam_obj = open_bam(bam, \"rb\", require_index=True)\n",
    "chromosome_lengths = dict(zip(bam_obj.references, bam_obj.lengths))\n",
    "\n",
    "# Create chunked genome regions:\n",
    "print(f\"Creating chunks of size {chunk_size}bp...\")\n",
    "\n",
    "if regions is None:\n",
    "    regions = [f\"{chrom}:0-{length}\" for chrom, length in chromosome_lengths.items()]\n",
    "elif isinstance(regions, str):\n",
    "    regions = [regions]\n",
    "\n",
    "# Create chunks from larger regions\n",
    "regions_split = []\n",
    "for region in regions:\n",
    "    chromosome, start, end = re.split(\"[:-]\", region)\n",
    "    start = int(start)\n",
    "    end = int(end)\n",
    "    for chunk_start in range(start, end, chunk_size):\n",
    "        chunk_end = chunk_start + chunk_size\n",
    "        if chunk_end > end:\n",
    "            chunk_end = end\n",
    "        regions_split.append(f\"{chromosome}:{chunk_start}-{chunk_end}\")\n",
    "        \n",
    "# start timer\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Count insertsize per chunk using multiprocessing\n",
    "print(f\"Counting insertsizes across {len(regions_split)} chunks...\")\n",
    "count_dict = {}\n",
    "read_count = 0\n",
    "#pbar = tqdm(total=len(regions_split), desc=\"Progress: \", unit=\"chunks\")\n",
    "for region in tqdm(regions_split):\n",
    "    chrom, start, end = re.split(\"[:-]\", region)\n",
    "    for read in bam_obj.fetch(chrom, int(start), int(end)):\n",
    "        read_count += 1\n",
    "        try:\n",
    "            barcode = read.get_tag(barcode_tag)\n",
    "        except Exception:  # tag was not found\n",
    "            barcode = \"NA\"\n",
    "\n",
    "        # Add read to dict\n",
    "        if check_in(barcode, barcodes) is True:\n",
    "            size = abs(read.template_length) - 9  # length of insertion\n",
    "            count_dict = _add_fragment(count_dict, barcode, size)     \n",
    "\n",
    "        # Update progress\n",
    "#        pbar.update(1)\n",
    "#    pbar.close()  # close progress bar\n",
    "\n",
    "            \n",
    "# Close file and print elapsed time\n",
    "end_time = datetime.datetime.now()\n",
    "bam_obj.close()\n",
    "elapsed = end_time - start_time\n",
    "print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "# Convert dict to pandas dataframe\n",
    "print(\"Converting counts to dataframe...\")\n",
    "table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "# round mean_insertsize to 2 decimals\n",
    "table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "finish_run = time.time()\n",
    "\n",
    "print(f'Run finished in: {finish_run - start_run}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def _insertsize_from_bam(bam: str,\n",
    "                         barcode_tag: str = \"CB\",\n",
    "                         barcodes: Optional[list[str]] = None,\n",
    "                         regions: Optional[str | list[str]] = 'chr1:1-2000000',\n",
    "                         chunk_size: int = 100000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get insertsize distributions per barcode from bam file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bam : str\n",
    "        Path to bam file\n",
    "    barcode_tag : str, default \"CB\"\n",
    "        The read tag representing the barcode.\n",
    "    barcodes : Optional[list[str]], default None\n",
    "        List of barcodes to include in the analysis. If None, all barcodes are included.\n",
    "    regions : Optional[str | list[str]], default 'chr1:1-2000000'\n",
    "        Regions to include in the analysis. If None, all reads are included.\n",
    "    chunk_size : int, default 500000\n",
    "        Size of bp chunks to read from bam file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with insertsize distributions per barcode.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError:\n",
    "        1. No reads found in bam-file.\n",
    "        2. If no reads in bam-file overlap with barcodes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load modules\n",
    "    try:\n",
    "        import pysam\n",
    "    except:\n",
    "        print('Check')\n",
    "\n",
    "    if utils._is_notebook() is True:\n",
    "        from tqdm import tqdm_notebook as tqdm\n",
    "    else:\n",
    "        from tqdm import tqdm\n",
    "\n",
    "    if isinstance(regions, str):\n",
    "        regions = [regions]\n",
    "\n",
    "    # Prepare function for checking against barcodes list\n",
    "    if barcodes is not None:\n",
    "        barcodes = set(barcodes)\n",
    "        check_in = _check_in_list\n",
    "    else:\n",
    "        check_in = _check_true\n",
    "\n",
    "    # Open bamfile\n",
    "    logger.info(\"Opening bam file...\")\n",
    "    if not os.path.exists(bam + \".bai\"):\n",
    "        logger.warning(\"Bamfile has no index - trying to index with pysam...\")\n",
    "        pysam.index(bam)\n",
    "\n",
    "    bam_obj = sctoolbox.tools.bam.open_bam(bam, \"rb\", require_index=True)\n",
    "    chromosome_lengths = dict(zip(bam_obj.references, bam_obj.lengths))\n",
    "\n",
    "    # Create chunked genome regions:\n",
    "    logger.info(f\"Creating chunks of size {chunk_size}bp...\")\n",
    "\n",
    "    if regions is None:\n",
    "        regions = [f\"{chrom}:0-{length}\" for chrom, length in chromosome_lengths.items()]\n",
    "    elif isinstance(regions, str):\n",
    "        regions = [regions]\n",
    "\n",
    "    # Create chunks from larger regions\n",
    "    regions_split = []\n",
    "    for region in regions:\n",
    "        chromosome, start, end = re.split(\"[:-]\", region)\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        for chunk_start in range(start, end, chunk_size):\n",
    "            chunk_end = chunk_start + chunk_size\n",
    "            if chunk_end > end:\n",
    "                chunk_end = end\n",
    "            regions_split.append(f\"{chromosome}:{chunk_start}-{chunk_end}\")\n",
    "\n",
    "    # Count insertsize per chunk using multiprocessing\n",
    "    logger.info(f\"Counting insertsizes across {len(regions_split)} chunks...\")\n",
    "    count_dict = {}\n",
    "    read_count = 0\n",
    "    pbar = tqdm(total=len(regions_split), desc=\"Progress: \", unit=\"chunks\")\n",
    "    for region in regions_split:\n",
    "        chrom, start, end = re.split(\"[:-]\", region)\n",
    "        for read in bam_obj.fetch(chrom, int(start), int(end)):\n",
    "            read_count += 1\n",
    "            try:\n",
    "                barcode = read.get_tag(barcode_tag)\n",
    "            except Exception:  # tag was not found\n",
    "                barcode = \"NA\"\n",
    "\n",
    "            # Add read to dict\n",
    "            if check_in(barcode, barcodes) is True:\n",
    "                size = abs(read.template_length) - 9  # length of insertion\n",
    "                count_dict = _add_fragment(count_dict, barcode, size)\n",
    "\n",
    "        # Update progress\n",
    "        pbar.update(1)\n",
    "    pbar.close()  # close progress bar\n",
    "\n",
    "    bam_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peakqc.general as utils\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def open_bam(file: str,\n",
    "             mode: str,\n",
    "             verbosity: Literal[0, 1, 2, 3] = 3, **kwargs: Any) -> \"pysam.AlignmentFile\":\n",
    "    \"\"\"\n",
    "    Open bam file with pysam.AlignmentFile. On a specific verbosity level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        Path to bam file.\n",
    "    mode : str\n",
    "        Mode to open the file in. See pysam.AlignmentFile\n",
    "    verbosity : Literal[0, 1, 2, 3], default 3\n",
    "        Set verbosity level. Verbosity level 0 for no messages.\n",
    "    **kwargs : Any\n",
    "        Forwarded to pysam.AlignmentFile\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pysam.AlignmentFile\n",
    "        Object to work on SAM/BAM files.\n",
    "    \"\"\"\n",
    "\n",
    "    # check then load modules\n",
    "    utils.check_module(\"pysam\")\n",
    "    import pysam\n",
    "\n",
    "    # save verbosity, then set temporary one\n",
    "    former_verbosity = pysam.get_verbosity()\n",
    "    pysam.set_verbosity(verbosity)\n",
    "\n",
    "    # open file\n",
    "    handle = pysam.AlignmentFile(file, mode, **kwargs)\n",
    "\n",
    "    # return to former verbosity\n",
    "    pysam.set_verbosity(former_verbosity)\n",
    "\n",
    "    return handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_run = time.time()\n",
    "\n",
    "regions=None\n",
    "bam = bamfile\n",
    "chunk_size = 10000000\n",
    "n_threads=10\n",
    "cb_tag = 'CB'\n",
    "\n",
    "utils.check_module(\"pysam\")\n",
    "import pysam\n",
    "\n",
    "if isinstance(regions, str):\n",
    "    regions = [regions]\n",
    "\n",
    "# Prepare function for checking against barcodes list\n",
    "if barcodes is not None:\n",
    "    barcodes = set(barcodes)\n",
    "    check_in = _check_in_list\n",
    "else:\n",
    "    check_in = _check_true\n",
    "    \n",
    "# Open bamfile\n",
    "print(\"Opening bam file...\")\n",
    "if not os.path.exists(bam + \".bai\"):\n",
    "    print(\"Bamfile has no index - trying to index with pysam...\")\n",
    "    pysam.index(bam)\n",
    "\n",
    "bam_obj = open_bam(bam, \"rb\", require_index=True)\n",
    "chromosome_lengths = dict(zip(bam_obj.references, bam_obj.lengths))\n",
    "\n",
    "# Create chunked genome regions:\n",
    "print(f\"Creating chunks of size {chunk_size}bp...\")\n",
    "\n",
    "if regions is None:\n",
    "    regions = [f\"{chrom}:0-{length}\" for chrom, length in chromosome_lengths.items()]\n",
    "elif isinstance(regions, str):\n",
    "    regions = [regions]\n",
    "\n",
    "# Create chunks from larger regions\n",
    "regions_split = []\n",
    "for region in regions:\n",
    "    chromosome, start, end = re.split(\"[:-]\", region)\n",
    "    start = int(start)\n",
    "    end = int(end)\n",
    "    for chunk_start in range(start, end, chunk_size):\n",
    "        chunk_end = chunk_start + chunk_size\n",
    "        if chunk_end > end:\n",
    "            chunk_end = end\n",
    "        regions_split.append(f\"{chromosome}:{chunk_start}-{chunk_end}\")\n",
    "        \n",
    "# start timer\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Initialize multiprocessing\n",
    "m = Manager() # initialize manager\n",
    "lock = Lock() # initialize lock\n",
    "managed_dict = m.dict() # initialize managed dict\n",
    "managed_dict['output'] = {}\n",
    "# initialize pool\n",
    "pool = Pool(processes=n_threads,\n",
    "            initializer=init_pool_processes,\n",
    "            initargs=(lock,),\n",
    "            maxtasksperchild=48)\n",
    "jobs = []\n",
    "print('Starting counting fragments...')\n",
    "     \n",
    "tag_idx = None\n",
    "# Count insertsize per chunk using multiprocessing TODO here goes MP\n",
    "for region in tqdm(regions_split):\n",
    "    chrom, start, end = re.split(\"[:-]\", region)\n",
    "    #start_time = time.time()\n",
    "    chunk = list(bam_obj.fetch(chrom, int(start), int(end)))\n",
    "    chunk = [prep_reads(read) for read in chunk]\n",
    "    stop_time = time.time()\n",
    "    #print(f'reading: {stop_time - start_time}')\n",
    "    \n",
    "    job = pool.apply_async(_count_fragments_from_bam_worker,\n",
    "                       args=(chunk, barcodes, check_in, managed_dict),\n",
    "                       error_callback=_custom_callback)\n",
    "    jobs.append(job)\n",
    "\n",
    "# close pool\n",
    "pool.close()\n",
    "# wait for all jobs to finish\n",
    "pool.join()\n",
    "# reset settings\n",
    "count_dict = managed_dict['output']\n",
    "\n",
    "# Close file and print elapsed time\n",
    "end_time = datetime.datetime.now()\n",
    "bam_obj.close()\n",
    "elapsed = end_time - start_time\n",
    "print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "# Convert dict to pandas dataframe\n",
    "print(\"Converting counts to dataframe...\")\n",
    "table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "# round mean_insertsize to 2 decimals\n",
    "table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "finish_run = time.time()\n",
    "\n",
    "print(f'Run finished in: {finish_run - start_run}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_fragments_from_bam_worker(chunk: list,\n",
    "                            barcodes: Optional[list[str]] = None,\n",
    "                            check_in: Any = _check_true,\n",
    "                            managed_dict: dict = {'output': {}}) -> None:\n",
    "    \"\"\"\n",
    "    Worker function for counting fragments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk : pd.DataFrame\n",
    "        Chunk of fragments file.\n",
    "    barcodes : list[str], optional\n",
    "        List of barcodes to count. If None, all barcodes are counted.\n",
    "    check_in : Any, default _check_true\n",
    "        Function for checking if barcode is in barcodes list.\n",
    "    managed_dict : dict, default None\n",
    "        Dictionary for multiprocessing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize count_dict\n",
    "    count_dict = {}\n",
    "    \n",
    "    # define helper\n",
    "    for [barcode, size] in chunk:\n",
    "    \n",
    "        #barcode = pair[0]\n",
    "        #size = pair[1]\n",
    "        \n",
    "        if check_in(barcode, barcodes) is True:\n",
    "                count_dict = _add_fragment(count_dict, barcode, size) # add fragment to count_dict\n",
    "    \n",
    "    # process\n",
    "    #[process_reads(pair, count_dict) for pair in chunk]\n",
    "    # Update managed_dict\n",
    "    lock.acquire() # acquire lock\n",
    "    latest = managed_dict['output']\n",
    "    try:\n",
    "        managed_dict['output'] = _update_count_dict(latest, count_dict) # update managed dict\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(f'Exception: {e}')\n",
    "    lock.release() # release lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reads(pair):\n",
    "    \n",
    "    barcode = pair[0]\n",
    "    size = pair[1]\n",
    "    \n",
    "    if check_in(barcode, barcodes) is True:\n",
    "            count_dict = _add_fragment(count_dict, barcode, size) # add fragment to count_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_reads(read):\n",
    "\n",
    "    barcode = read.get_tag(cb_tag)\n",
    "    size = read.template_length - 9\n",
    "    \n",
    "    return [barcode, size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sctoolbox.tools as sctools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "count_table = sctools._insertsize_from_bam(bam=bamfile,\n",
    "                         barcode_tag=\"CB\",\n",
    "                         barcodes=list(barcodes),\n",
    "                         regions=None,\n",
    "                         chunk_size= 100000)\n",
    "\n",
    "stop = time.time()\n",
    "\n",
    "print(f'original implementation: {stop-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_table.loc['AAATCCGCATAAATGCTACGGG'][np.arange(0,50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.loc['AAATCCGCATAAATGCTACGGG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual imports\n",
    "import episcanpy as epi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import datetime\n",
    "from multiprocessing import Manager, Lock, Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "from beartype import beartype\n",
    "from beartype.typing import Any, Optional\n",
    "\n",
    "@beartype\n",
    "def _is_gz_file(filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check wheather file is a compressed .gz file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the file is a compressed .gz file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'rb') as test_f:\n",
    "        return test_f.read(2) == b'\\x1f\\x8b'\n",
    "\n",
    "class MPFragmentCounter():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Init class variables.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def init_pool_processes(self, the_lock):\n",
    "        '''\n",
    "        Initialize each process with a global variable lock.\n",
    "        '''\n",
    "        global lock\n",
    "        lock = the_lock\n",
    "\n",
    "    def _check_in_list(self, element: Any, alist: list[Any] | set[Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if element is in list.\n",
    "\n",
    "        TODO Do we need this function?\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        element : Any\n",
    "            Element that is checked for.\n",
    "        alist : list[Any] | set[Any]\n",
    "            List or set in which the element is searched for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if element is in list else False\n",
    "        \"\"\"\n",
    "\n",
    "        return element in alist\n",
    "\n",
    "    def _check_true(element: Any, alist: Optional[list[Any]] = None) -> bool:  # true regardless of input\n",
    "        \"\"\"\n",
    "        Return True regardless of input\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        element : Any\n",
    "            Element that is checked for.\n",
    "        alist: Optional[list[Any]]\n",
    "            List or set in which the element is searched for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if element is in list else False\n",
    "        \"\"\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    def custom_callback(self, error):\n",
    "    \tprint(error, flush=True)\n",
    "        \n",
    "\n",
    "    def insertsize_from_fragments(self, fragments: str,\n",
    "                                  barcodes: Optional[list[str]] = None,\n",
    "                                  n_threads: int = 8) -> pd.DataFrame:\n",
    "\n",
    "        print('Count insertsizes from fragments...')\n",
    "        # Open fragments file\n",
    "        if _is_gz_file(fragments):\n",
    "            f = gzip.open(fragments, \"rt\")\n",
    "        else:\n",
    "            f = open(fragments, \"r\")\n",
    "\n",
    "        # Prepare function for checking against barcodes list\n",
    "        if barcodes is not None:\n",
    "            barcodes = set(barcodes)\n",
    "            check_in = self._check_in_list\n",
    "        else:\n",
    "            check_in = self._check_true\n",
    "\n",
    "        iterator = pd.read_csv(fragments,\n",
    "                               delimiter='\\t',\n",
    "                               header=None,\n",
    "                               names=['chr', 'start', 'stop', 'barcode', 'count'],\n",
    "                               iterator=True,\n",
    "                               chunksize=5000000)\n",
    "\n",
    "        # start timer\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Initialize multiprocessing\n",
    "        m = Manager()\n",
    "        lock = Lock()\n",
    "        managed_dict = m.dict()\n",
    "        managed_dict['output'] = {}\n",
    "        pool = Pool(processes=n_threads, initializer=self.init_pool_processes, initargs=(lock,), maxtasksperchild=48)\n",
    "        jobs = []\n",
    "        print('Starting counting fragments...')\n",
    "        # split fragments into chunks\n",
    "        for chunk in tqdm(iterator, desc=\"Processing Chunks\"):\n",
    "            # apply async job wit callback function\n",
    "            job = pool.apply_async(self._count_fragments_worker, args=(chunk, barcodes, check_in, managed_dict), error_callback=self.custom_callback)\n",
    "            jobs.append(job)\n",
    "        \n",
    "        # close pool\n",
    "        pool.close()\n",
    "        # wait for all jobs to finish\n",
    "        pool.join()\n",
    "        # reset settings\n",
    "        count_dict = managed_dict['output']\n",
    "\n",
    "        # Close file and print elapsed time\n",
    "        end_time = datetime.datetime.now()\n",
    "        f.close()\n",
    "\n",
    "        elapsed = end_time - start_time\n",
    "        print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "        # Convert dict to pandas dataframe\n",
    "        print(\"Converting counts to dataframe...\")\n",
    "        table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "        #table = table[[\"insertsize_count\", \"mean_insertsize\"] + sorted(table.columns[2:])]\n",
    "        table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "        print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "        return table\n",
    "\n",
    "    def _count_fragments_worker(self, chunk, barcodes, check_in, managed_dict):\n",
    "        \"\"\"\n",
    "        Worker function for counting fragments.\n",
    "        Parameters\n",
    "        ----------\n",
    "        chunk\n",
    "        barcodes\n",
    "        check_in\n",
    "        managed_dict\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize count_dict\n",
    "        count_dict = {}\n",
    "        for row in chunk.itertuples():\n",
    "            start = int(row[2])\n",
    "            end = int(row[3])\n",
    "            barcode = row[4]\n",
    "            count = int(row[5])\n",
    "            size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "            # Only add fragment if check is true\n",
    "            if check_in(barcode, barcodes) is True:\n",
    "                count_dict = self._add_fragment(count_dict, barcode, size, count)\n",
    "\n",
    "        lock.acquire()\n",
    "        latest = managed_dict['output']\n",
    "        managed_dict['output'] = self._update_count_dict(latest, count_dict)\n",
    "        lock.release()\n",
    "\n",
    "    def _add_fragment(self, count_dict: dict[str, int],\n",
    "                      barcode: str,\n",
    "                      size: int,\n",
    "                      count: int = 1,\n",
    "                      max_size=1000):\n",
    "        \"\"\"\n",
    "        Add fragment of size 'size' to count_dict.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        count_dict : dict[str, int]\n",
    "            Dictionary containing the counts per insertsize.\n",
    "        barcode : str\n",
    "            Barcode of the read.\n",
    "        size : int\n",
    "            Insertsize to add to count_dict.\n",
    "        count : int, default 1\n",
    "            Number of reads to add to count_dict.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize if barcode is seen for the first time\n",
    "        if barcode not in count_dict:\n",
    "            count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "        # Add read to dict\n",
    "        if size > 0 and size <= max_size:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "            count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "            # Update mean\n",
    "            mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "            total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "            diff = (size - mu) / total_count\n",
    "            count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "            # Save to distribution\n",
    "            if 'dist' not in count_dict[barcode]:  # first time size is seen\n",
    "                count_dict[barcode]['dist'] = np.zeros(max_size+1)\n",
    "            count_dict[barcode]['dist'][size] += count\n",
    "\n",
    "        return count_dict\n",
    "\n",
    "    def _update_count_dict(self, count_dict_1, count_dict_2):\n",
    "        \"\"\"\n",
    "        updates\n",
    "        \"\"\"\n",
    "        # Check if count_dict_1 is empty:\n",
    "        if len(count_dict_1) == 0:\n",
    "            return count_dict_2\n",
    "\n",
    "        # make Dataframes for computation\n",
    "        df1 = pd.DataFrame(count_dict_1).T\n",
    "        df2 = pd.DataFrame(count_dict_2).T\n",
    "\n",
    "        # merge distributions\n",
    "        combined_dists = df1['dist'].combine(df2['dist'], func=self._update_dist)\n",
    "        # merge counts\n",
    "        merged_counts = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True,\n",
    "                                 how='outer').fillna(0)\n",
    "        # sum total counts/barcode\n",
    "        updated_counts = merged_counts.sum(axis=1)\n",
    "\n",
    "        # calculate scaling factors\n",
    "        x_scaling_factor = merged_counts[\"insertsize_count_x\"] / updated_counts\n",
    "        y_scaling_factor = merged_counts[\"insertsize_count_y\"] / updated_counts\n",
    "\n",
    "        # merge mean insertsizes\n",
    "        merged_mean_insertsizes = pd.merge(df1[\"mean_insertsize\"], df2[\"mean_insertsize\"], left_index=True,\n",
    "                                           right_index=True, how='outer').fillna(0)\n",
    "\n",
    "        # scale mean insertsizes\n",
    "        merged_mean_insertsizes[\"mean_insertsize_x\"] = merged_mean_insertsizes[\"mean_insertsize_x\"] * x_scaling_factor\n",
    "        merged_mean_insertsizes[\"mean_insertsize_y\"] = merged_mean_insertsizes[\"mean_insertsize_y\"] * y_scaling_factor\n",
    "\n",
    "        # sum the scaled means\n",
    "        updated_means = merged_mean_insertsizes.sum(axis=1)\n",
    "\n",
    "        # build the updated dictionary\n",
    "        updated_dict = pd.DataFrame(\n",
    "            {'mean_insertsize': updated_means, 'insertsize_count': updated_counts, 'dist': combined_dists}).T.to_dict()\n",
    "\n",
    "        return updated_dict\n",
    "\n",
    "\n",
    "    def _update_dist(self, dist_1, dist_2):\n",
    "        \"\"\"Updates the Insertsize Distributions\"\"\"\n",
    "        if not np.isnan(dist_1).any() and not np.isnan(dist_2).any():\n",
    "            updated_dist = dist_1 + dist_2\n",
    "            return updated_dist.astype(int)\n",
    "        elif np.isnan(dist_1).any():\n",
    "            return dist_2.astype(int)\n",
    "        elif np.isnan(dist_2).any():\n",
    "            return dist_1.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_df(dist):\n",
    "    \n",
    "    table_dict = {}\n",
    "    for row in dist.iterrows():\n",
    "        barcode = str(row[0])\n",
    "        table_dict[barcode] = {}\n",
    "\n",
    "        for i, counts in enumerate(row[1]['dist']):\n",
    "            table_dict[barcode][i] = counts\n",
    "    \n",
    "    dist_df = pd.DataFrame(table_dict).T\n",
    "    \n",
    "    return dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adata_barcodes = adata.obs.index.tolist()\n",
    "# split index for barcodes CBs\n",
    "barcodes = []\n",
    "for entry in adata_barcodes:\n",
    "    barcode = entry.split('+')[1]\n",
    "    barcodes.append(barcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "counter = MPFragmentCounter()\n",
    "table_mp = counter.insertsize_from_fragments(fragments_file, barcodes, n_threads=10)\n",
    "print(table_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_mp.loc['AAATCCGCATAAACGTCCCGTT']['dist'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "table_sctoolbox = tools._insertsize_from_fragments(fragments_file, barcodes)\n",
    "print(table_sctoolbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_sctoolbox.loc['AAATCCGCATAAACGTCCCGTT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_sctoolbox.loc['AAATCCGCATAAACGTCCCGTT'][[c for c in table_sctoolbox.columns if isinstance(c, int)]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_sctoolbox.loc['AAATCCGCATAAACGTCCCGTT'][[c for c in table_sctoolbox.columns if isinstance(c, int)]][0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_sctoolbox = table_sctoolbox[[c for c in table_sctoolbox.columns if isinstance(c, int)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_mp = get_dist_df(table_mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_sctoolbox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_mp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_table_mp = table_mp.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_table_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_table_sctoolbox = table_sctoolbox.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_table_sctoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_table_mp.equals(sorted_table_sctoolbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_table_mp == table_mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:57:38.120801356Z",
     "start_time": "2023-12-15T15:57:38.072063844Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_lines(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return sum(1 for line in file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:57:54.544122099Z",
     "start_time": "2023-12-15T15:57:39.446642696Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Replace 'yourfile.txt' with the path to your file\n",
    "number_of_lines = count_lines(fragments_file)\n",
    "print(f\"Total number of lines: {number_of_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:45:22.195137291Z",
     "start_time": "2023-12-15T15:45:22.182267393Z"
    }
   },
   "outputs": [],
   "source": [
    "#small_fragments = '/mnt/workspace2/jdetlef/data/public_data/cropped_heart_fragments.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_fragments = '/home/jan/Workspace/bio_data/small_fragments.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:51:15.418105880Z",
     "start_time": "2023-12-15T15:51:15.388304546Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class MPFragmentCounter():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Init class variables.\"\"\"\n",
    "        \n",
    "        self.m = Manager()\n",
    "        self.d = self.m.dict()\n",
    "        self.d['output'] = {}\n",
    "        self.lock = Lock()\n",
    "\n",
    "\n",
    "        \n",
    "    def _check_in_list(element: Any, alist: list[Any] | set[Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if element is in list.\n",
    "\n",
    "        TODO Do we need this function?\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        element : Any\n",
    "            Element that is checked for.\n",
    "        alist : list[Any] | set[Any]\n",
    "            List or set in which the element is searched for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if element is in list else False\n",
    "        \"\"\"\n",
    "\n",
    "        return element in alist\n",
    "\n",
    "\n",
    "    \n",
    "    def _check_true(element: Any, alist: Optional[list[Any]] = None) -> bool:  # true regardless of input\n",
    "        \"\"\"\n",
    "        Return True regardless of input\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        element : Any\n",
    "            Element that is checked for.\n",
    "        alist: Optional[list[Any]]\n",
    "            List or set in which the element is searched for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if element is in list else False\n",
    "        \"\"\"\n",
    "\n",
    "        return True\n",
    "\n",
    "    \n",
    "    def insertsize_from_fragments(self, fragments: str,\n",
    "                                  barcodes: Optional[list[str]] = None,\n",
    "                                  n_threads: int = 8) -> pd.DataFrame:\n",
    "        # Open fragments file\n",
    "        if _is_gz_file(fragments):\n",
    "            f = gzip.open(fragments, \"rt\")\n",
    "        else:\n",
    "            f = open(fragments, \"r\")\n",
    "\n",
    "        # Prepare function for checking against barcodes list\n",
    "        if barcodes is not None:\n",
    "            barcodes = set(barcodes)\n",
    "            check_in = self._check_in_list\n",
    "        else:\n",
    "            check_in = self._check_true\n",
    "\n",
    "        iterator = pd.read_csv(fragments,\n",
    "                               delimiter='\\t',\n",
    "                               header=None,\n",
    "                               names=['chr', 'start', 'stop', 'barcode', 'count'],\n",
    "                               iterator=True,\n",
    "                               chunksize=1000)\n",
    "\n",
    "        # start timer\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        pool = Pool(n_threads, maxtasksperchild=48)\n",
    "        jobs = []\n",
    "        # split fragments into chunks\n",
    "        for chunk in iterator:\n",
    "            # apply async job wit callback function\n",
    "            job = pool.apply_async(self._count_fragments_worker, args=(chunk, barcodes, check_in))\n",
    "            jobs.append(job)\n",
    "        # monitor progress\n",
    "        # utils.monitor_jobs(jobs, description=\"Progress\")\n",
    "        # close pool\n",
    "        pool.close()\n",
    "        # wait for all jobs to finish\n",
    "        pool.join()\n",
    "        # reset settings\n",
    "        count_dict = self.d\n",
    "        print('what is going on')\n",
    "        print(count_dict)\n",
    "        # Fill missing sizes with 0\n",
    "        max_fragment_size = 1001\n",
    "\n",
    "        for barcode in count_dict:\n",
    "            for size in range(max_fragment_size):\n",
    "                if size not in count_dict[barcode]:\n",
    "                    count_dict[barcode][size] = 0\n",
    "\n",
    "        # Close file and print elapsed time\n",
    "        end_time = datetime.datetime.now()\n",
    "        f.close()\n",
    "\n",
    "        elapsed = end_time - start_time\n",
    "        print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "        # Convert dict to pandas dataframe\n",
    "        print(\"Converting counts to dataframe...\")\n",
    "        table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "        table = table[[\"insertsize_count\", \"mean_insertsize\"] + sorted(table.columns[2:])]\n",
    "        table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "        print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "        return table\n",
    "\n",
    "    \n",
    "    def _count_fragments_worker(self, chunk, barcodes, check_in):\n",
    "        \n",
    "        count_dict = {}\n",
    "        \n",
    "        for i in range(len(chunk)):\n",
    "            row = chunk.iloc[i]\n",
    "            start = int(row['start'])\n",
    "            end = int(row['stop'])\n",
    "            barcode = row['barcode']\n",
    "            count = int(row['count'])\n",
    "            size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "            # Only add fragment if check is true\n",
    "            if check_in(barcode, barcodes) is True:\n",
    "                count_dict = self._add_fragment(count_dict, barcode, size, count)\n",
    "                \n",
    "        with self.lock:\n",
    "            self.d['output'] = update_count_dict(self.d['output'], count_dict)\n",
    "\n",
    "\n",
    "    def _add_fragment(count_dict: dict[str, int],\n",
    "                      barcode: str,\n",
    "                      size: int,\n",
    "                      count: int = 1):\n",
    "        \"\"\"\n",
    "        Add fragment of size 'size' to count_dict.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        count_dict : dict[str, int]\n",
    "            Dictionary containing the counts per insertsize.\n",
    "        barcode : str\n",
    "            Barcode of the read.\n",
    "        size : int\n",
    "            Insertsize to add to count_dict.\n",
    "        count : int, default 1\n",
    "            Number of reads to add to count_dict.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize if barcode is seen for the first time\n",
    "        if barcode not in count_dict:\n",
    "            count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "        # Add read to dict\n",
    "        if size >= 0 and size <= 1000:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "            count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "            # Update mean\n",
    "            mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "            total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "            diff = (size - mu) / total_count\n",
    "            count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "            # Save to distribution\n",
    "            if size not in count_dict[barcode]:  # first time size is seen\n",
    "                count_dict[barcode][size] = 0\n",
    "            count_dict[barcode][size] += count\n",
    "            \n",
    "        return count_dict\n",
    "    \n",
    "\n",
    "    def _log_result(self, result: Any) -> None:\n",
    "        \"\"\"Log results from mp_counter.\"\"\"\n",
    "\n",
    "        if self.merged_dict:\n",
    "            self.merged_dict = dict(Counter(self.merged_dict) + Counter(result))\n",
    "            # print('merging')\n",
    "        else:\n",
    "            self.merged_dict = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " mpc = MPFragmentCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "counts = mpc.insertsize_from_fragments(small_fragments, barcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dict['another'] = {'test': 'Hallo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dict['another']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict_1={}\n",
    "count_dict_1['ACGTT'] = {\"mean_insertsize\": 10, \"insertsize_count\": 5, 'dist': np.array([0,1,0,2,1,1,0])}\n",
    "count_dict_1['GTCCT'] = {\"mean_insertsize\": 10, \"insertsize_count\": 20, 'dist': np.array([0,0,0,1,2,2,1])}\n",
    "count_dict_1['GCGCG'] = {\"mean_insertsize\": 10, \"insertsize_count\": 20, 'dist': np.array([0,0,0,1,2,2,1])}\n",
    "\n",
    "count_dict_2={}\n",
    "count_dict_2['ACGTT'] = {\"mean_insertsize\": 20, \"insertsize_count\": 20, 'dist': np.array([2,1,1,0,1,1,0])}\n",
    "count_dict_2['GTCCT'] = {\"mean_insertsize\": 20, \"insertsize_count\": 5, 'dist': np.array([1,0,2,2,1,1,0])}\n",
    "count_dict_2['TTTAA'] = {\"mean_insertsize\": 20, \"insertsize_count\": 5, 'dist': np.array([1,0,2,2,1,1,0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make Dataframes for computation\n",
    "df1 = pd.DataFrame(count_dict_1).T\n",
    "df2 = pd.DataFrame(count_dict_2).T\n",
    "\n",
    "# merge counts\n",
    "combined_dists = df1['dist'].combine(df2['dist'], func=update_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counts = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True, how='outer').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # merge counts\n",
    "    merged_counts = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True, how='outer').fillna(0)\n",
    "    # sum total counts/barcode\n",
    "    updated_counts = merged_counts.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dists= pd.DataFrame({'combined_dists' : combined_dists})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = pd.DataFrame({'insertsize_counts' : updated_counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = pd.DataFrame({'insertsize_counts': {'TTTAA':20, 'ACGTT':25, 'GCGCG': 25, 'GTCCT': 5}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(some_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_count_dict(count_dict_1, count_dict_2):\n",
    "    \"\"\"\n",
    "    updates\n",
    "    \"\"\"\n",
    "    # Check if count_dict_1 is empty:\n",
    "    if len(count_dict_1) == 0:\n",
    "        return count_dict_2\n",
    "        \n",
    "    # make Dataframes for computation\n",
    "    df1 = pd.DataFrame(count_dict_1).T\n",
    "    df2 = pd.DataFrame(count_dict_2).T\n",
    "\n",
    "    # merge distributions\n",
    "    combined_dists = df1['dist'].combine(df2['dist'], func=update_dist)\n",
    "    \n",
    "    # merge counts\n",
    "    merged_counts = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True, how='outer').fillna(0)\n",
    "    # sum total counts/barcode\n",
    "    updated_counts = merged_counts.sum(axis=1)\n",
    "    \n",
    "\n",
    "    # calculate scaling factors\n",
    "    x_scaling_factor = merged_counts[\"insertsize_count_x\"] / updated_counts\n",
    "    y_scaling_factor = merged_counts[\"insertsize_count_y\"] / updated_counts\n",
    "\n",
    "    # merge mean insertsizes\n",
    "    merged_mean_insertsizes = pd.merge(df1[\"mean_insertsize\"], df2[\"mean_insertsize\"], left_index=True, right_index=True, how='outer').fillna(0)\n",
    "\n",
    "    # scale mean insertsizes\n",
    "    merged_mean_insertsizes[\"mean_insertsize_x\"] = merged_mean_insertsizes[\"mean_insertsize_x\"] * x_scaling_factor\n",
    "    merged_mean_insertsizes[\"mean_insertsize_y\"] = merged_mean_insertsizes[\"mean_insertsize_y\"] * y_scaling_factor\n",
    "\n",
    "    # sum the scaled means\n",
    "    updated_means = merged_mean_insertsizes.sum(axis=1)\n",
    "\n",
    "    # build the updated dictionary\n",
    "    updated_dict = pd.DataFrame({'mean_insertsize': updated_means, 'insertsize_count' : updated_counts, 'dist': combined_dists}).T.to_dict()\n",
    "    \n",
    "    \n",
    "    return updated_dict\n",
    "\n",
    "\n",
    "def update_dist(dist_1, dist_2):\n",
    "    \"\"\"Updates the Insertsize Distributions\"\"\"\n",
    "    if not np.isnan(dist_1).any() and not np.isnan(dist_2).any():\n",
    "        updated_dist = dist_1 + dist_2\n",
    "        return updated_dist\n",
    "    elif np.isnan(dist_1).any():\n",
    "        return dist_2\n",
    "    elif np.isnan(dist_2).any():\n",
    "        return dist_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'mean_insertsizes': updated_means, 'insertsize_counts' : updated_counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,3,21,0]) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_insertsizes = pd.merge(df1[\"insertsize_count\"], df2[\"insertsize_count\"], left_index=True, right_index=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaling_factor = merged_insertsizes[\"insertsize_count_x\"] / merged_insertsizes.sum(axis=1)\n",
    "y_scaling_factor = merged_insertsizes[\"insertsize_count_y\"] / merged_insertsizes.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mean_insertsizes = pd.merge(df1[\"mean_insertsize\"], df2[\"mean_insertsize\"], left_index=True, right_index=True)\n",
    "merged_mean_insertsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mean_insertsizes[\"mean_insertsize_x\"] = merged_mean_insertsizes[\"mean_insertsize_x\"] * x_scaling_factor\n",
    "merged_mean_insertsizes[\"mean_insertsize_y\"] = merged_mean_insertsizes[\"mean_insertsize_y\"] * y_scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mean_insertsizes.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mean_insertsizes * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Erstellen Sie zwei Beispieldatenframes\n",
    "df1 = pd.DataFrame({'Werte1': [1, 2, 3]}, index=['a', 'b', 'c'])\n",
    "df2 = pd.DataFrame({'Werte1': [4, 5, 6]}, index=['a', 'b', 'c'])\n",
    "\n",
    "# Mergen Sie die DataFrames am Index\n",
    "merged_df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "\n",
    "# Summieren Sie die Werte\n",
    "summed_df = merged_df.sum(axis=1)\n",
    "\n",
    "print(summed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T14:54:55.792910295Z",
     "start_time": "2023-12-15T14:49:43.045852210Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "count_table = tools._insertsize_from_fragments(small_fragments, barcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def _insertsize_from_fragments(fragments: str,\n",
    "                               barcodes: Optional[list[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get fragment insertsize distributions per barcode from fragments file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fragments : str\n",
    "        Path to fragments.bed(.gz) file.\n",
    "    barcodes : Optional[list[str]], default None\n",
    "        Only collect fragment sizes for the barcodes in barcodes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with insertsize distributions per barcode.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open fragments file\n",
    "    if utils._is_gz_file(fragments):\n",
    "        f = gzip.open(fragments, \"rt\")\n",
    "    else:\n",
    "        f = open(fragments, \"r\")\n",
    "\n",
    "    # Prepare function for checking against barcodes list\n",
    "    if barcodes is not None:\n",
    "        barcodes = set(barcodes)\n",
    "        check_in = _check_in_list\n",
    "    else:\n",
    "        check_in = _check_true\n",
    "\n",
    "    # Read fragments file and add to dict\n",
    "    print(\"Counting fragment lengths from fragments file...\")\n",
    "    start_time = datetime.datetime.now()\n",
    "    count_dict = {}\n",
    "    for line in f:\n",
    "        columns = line.rstrip().split(\"\\t\")\n",
    "        start = int(columns[1])\n",
    "        end = int(columns[2])\n",
    "        barcode = columns[3]\n",
    "        count = int(columns[4])\n",
    "        size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "        # Only add fragment if check is true\n",
    "        if check_in(barcode, barcodes) is True:\n",
    "            count_dict = _add_fragment(count_dict, barcode, size, count)\n",
    "\n",
    "    # Fill missing sizes with 0\n",
    "    max_fragment_size = 1001\n",
    "\n",
    "    for barcode in count_dict:\n",
    "        for size in range(max_fragment_size):\n",
    "            if size not in count_dict[barcode]:\n",
    "                count_dict[barcode][size] = 0\n",
    "\n",
    "    # Close file and print elapsed time\n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed = end_time - start_time\n",
    "    f.close()\n",
    "    print(\"Done reading file - elapsed time: {0}\".format(str(elapsed).split(\".\")[0]))\n",
    "\n",
    "    # Convert dict to pandas dataframe\n",
    "    print(\"Converting counts to dataframe...\")\n",
    "    table = pd.DataFrame.from_dict(count_dict, orient=\"index\")\n",
    "    table = table[[\"insertsize_count\", \"mean_insertsize\"] + sorted(table.columns[2:])]\n",
    "    table[\"mean_insertsize\"] = table[\"mean_insertsize\"].round(2)\n",
    "\n",
    "    print(\"Done getting insertsizes from fragments!\")\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def _add_fragment(count_dict: dict[str, int],\n",
    "                  barcode: str,\n",
    "                  size: int,\n",
    "                  count: int = 1) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Add fragment of size 'size' to count_dict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    count_dict : dict[str, int]\n",
    "        Dictionary containing the counts per insertsize.\n",
    "    barcode : str\n",
    "        Barcode of the read.\n",
    "    size : int\n",
    "        Insertsize to add to count_dict.\n",
    "    count : int, default 1\n",
    "        Number of reads to add to count_dict.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, int]\n",
    "        Updated count_dict\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize if barcode is seen for the first time\n",
    "    if barcode not in count_dict:\n",
    "        count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "    # Add read to dict\n",
    "    if size >= 0 and size <= 1000:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "        count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "        # Update mean\n",
    "        mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "        total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "        diff = (size - mu) / total_count\n",
    "        count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "        # Save to distribution\n",
    "        if size not in count_dict[barcode]:  # first time size is seen\n",
    "            count_dict[barcode][size] = 0\n",
    "        count_dict[barcode][size] += count\n",
    "\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def _is_gz_file(filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check wheather file is a compressed .gz file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the file is a compressed .gz file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filepath, 'rb') as test_f:\n",
    "        return test_f.read(2) == b'\\x1f\\x8b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@beartype\n",
    "def gunzip_file(f_in: str, f_out: str) -> None:\n",
    "    \"\"\"\n",
    "    Decompress file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_in : str\n",
    "        Path to compressed input file.\n",
    "    f_out : str\n",
    "        Destination to decompressed output file.\n",
    "    \"\"\"\n",
    "\n",
    "    with gzip.open(f_in, 'rb') as h_in:\n",
    "        with open(f_out, 'wb') as h_out:\n",
    "            shutil.copyfileobj(h_in, h_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = pd.read_csv(fragments_file,\n",
    "                       delimiter='\\t',\n",
    "                       header=None,\n",
    "                       names=['chr', 'start', 'stop', 'barcode', 'count'],\n",
    "                       iterator=True,\n",
    "                       chunksize=100000)\n",
    "updated = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "check_in = _check_true\n",
    "\n",
    "count_dict = {}\n",
    "\n",
    "for row in chunk.itertuples():\n",
    "    start = int(row[2])\n",
    "    end = int(row[3])\n",
    "    barcode = row[4]\n",
    "    count = int(row[5])\n",
    "    size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "    # Only add fragment if check is true\n",
    "    if check_in(barcode, barcodes) is True:\n",
    "        count_dict = _add_fragment(count_dict, barcode, size, count)\n",
    "        \n",
    "\n",
    "updated = update_count_dict(updated, count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_add_fragments(row, count_dict):\n",
    "    start = int(row[1])\n",
    "    end = int(row[2])\n",
    "    barcode = str(row[3])\n",
    "    count = int(row[4])\n",
    "    size = end - start - 9 \n",
    "\n",
    "    if check_in(barcode, barcodes) is True:\n",
    "        result = _add_fragment(count_dict, barcode, size, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = chunk.apply(lambda row: wrap_add_fragments(row, count_dict), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "check_in = _check_true\n",
    "\n",
    "count_dict = {}\n",
    "\n",
    "for i in range(len(chunk)):\n",
    "    row = chunk.iloc[i]\n",
    "    start = int(row['start'])\n",
    "    end = int(row['stop'])\n",
    "    barcode = row['barcode']\n",
    "    count = int(row['count'])\n",
    "    size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "    # Only add fragment if check is true\n",
    "    if check_in(barcode, barcodes) is True:\n",
    "        count_dict = _add_fragment(count_dict, barcode, size, count)\n",
    "        \n",
    "\n",
    "updated = update_count_dict(updated, count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(count_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _count_fragments_worker(self, chunk, barcodes, check_in):\n",
    "        \n",
    "        count_dict = {}\n",
    "        \n",
    "        for i in range(len(chunk)):\n",
    "            row = chunk.iloc[i]\n",
    "            start = int(row['start'])\n",
    "            end = int(row['stop'])\n",
    "            barcode = row['barcode']\n",
    "            count = int(row['count'])\n",
    "            size = end - start - 9  # length of insertion (-9 due to to shifted cutting of Tn5)\n",
    "\n",
    "            # Only add fragment if check is true\n",
    "            if check_in(barcode, barcodes) is True:\n",
    "                count_dict = self._add_fragment(count_dict, barcode, size, count)\n",
    "                \n",
    "        with self.lock:\n",
    "            self.d = update_count_dict(self.d, count_dict)\n",
    "            \n",
    "    def _check_true(element: Any, alist: Optional[list[Any]] = None) -> bool:  # true regardless of input\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _add_fragment(count_dict: dict[str, int],\n",
    "                      barcode: str,\n",
    "                      size: int,\n",
    "                      count: int = 1,\n",
    "                      max_size=1000):\n",
    "\n",
    "        # Initialize if barcode is seen for the first time\n",
    "        if barcode not in count_dict:\n",
    "            count_dict[barcode] = {\"mean_insertsize\": 0, \"insertsize_count\": 0}\n",
    "\n",
    "        # Add read to dict\n",
    "        if size >= 0 and size <= max_size:  # do not save negative insertsize, and set a cap on the maximum insertsize to limit outlier effects\n",
    "\n",
    "            count_dict[barcode][\"insertsize_count\"] += count\n",
    "\n",
    "            # Update mean\n",
    "            mu = count_dict[barcode][\"mean_insertsize\"]\n",
    "            total_count = count_dict[barcode][\"insertsize_count\"]\n",
    "            diff = (size - mu) / total_count\n",
    "            count_dict[barcode][\"mean_insertsize\"] = mu + diff\n",
    "\n",
    "            # Save to distribution\n",
    "            if size not in count_dict[barcode]:  # first time size is seen\n",
    "                sizes = np.arange(0,max_size+1)\n",
    "                count_dict[barcode]['dist'] = np.zeros(max_size)\n",
    "            count_dict[barcode]['dist'][size] += count\n",
    "            \n",
    "        return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,1001) + np.arange(0,1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict[']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peakqc",
   "language": "python",
   "name": "peakqc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
