{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Native Integration of Function Calling with Open-Source Language Models\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this demo, we will explore how to enhance language model predictions with the ability to call external functions, such as fetching the current weather or making a reservation, directly within the model's output. This capability is enabled by the `ToolCallSampler` class,\n",
        "\n",
        "We will begin by setting up our environment and installing necessary packages.\n"
      ],
      "metadata": {
        "id": "1-ek8zIsTCEV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6texdErJ8U2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/unaidedelf8777/function-sampler.git && cd function-sampler && pip install .\n",
        "!pip install bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining External Functions\n",
        "\n",
        "Before we can enhance our language model with external function calls, we need to define the functions that can be called. These functions are specified as a list of dictionaries, each containing details about the function name, description, parameters, and any required arguments. they are specified in the same format as the legacy OpenAI function calling format, along with support for the 'format', 'maxLength', an 'minLength' fields of the [json-schema spec](<https://json-schema.org/learn/getting-started-step-by-step>).\n"
      ],
      "metadata": {
        "id": "uC9IYJBMO1Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = [{\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"format\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"location\", \"format\"],\n",
        "            },\n",
        "       },\n",
        "       {\n",
        "    \"name\": \"get_reservation\",\n",
        "    \"description\": \"Retrieve a reservation at a restaurant\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"restaurant_name\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The name of the restaurant for which the reservation is made\"\n",
        "            },\n",
        "            \"reservation_date\": {\n",
        "                \"type\": \"string\",\n",
        "                \"format\": \"date\",\n",
        "                \"description\": \"The date of the reservation in YYYY-MM-DD format\"\n",
        "            },\n",
        "            \"reservation_time\": {\n",
        "                \"type\": \"string\",\n",
        "                \"format\": \"time\",\n",
        "                \"description\": \"The time of the reservation in HH:MM format\"\n",
        "            },\n",
        "            \"party_size\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"The number of people included in the reservation\"\n",
        "            },\n",
        "            \"contact_number\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"The contact phone number for the reservation confirmation\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"restaurant_name\", \"reservation_time\"]\n",
        "    }\n",
        "}\n",
        "]\n"
      ],
      "metadata": {
        "id": "3l3wB8fFKCYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Language Model\n",
        "\n",
        "To demonstrate function calls within text generation, we'll use a pre-trained causal language model from Hugging Face's `transformers` library. Along with the model, we also load its associated tokenizer, which will be used for encoding inputs and decoding outputs.\n",
        "\n",
        "We will also supply the tokenizer to the `ToolCallSampler` class, which will use our models tokenizer to construct a **FSM** ( finite-state machine )  for each function schema we gave.\n"
      ],
      "metadata": {
        "id": "DmhQX3pMQC4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\", load_in_4bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\")"
      ],
      "metadata": {
        "id": "3vfebh9lKJu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = \"<|im_start|>user\\nmake me a reservation at magianos for 6 pm. call functions by responding with the word:'<function>' \\n<|im_end|>\\n<|im_start|>assistant\\n \"\n",
        "tokens = tokenizer.encode(m, return_tensors='pt')"
      ],
      "metadata": {
        "id": "1Nl6sBQtKPGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring the ToolCallSampler\n",
        "\n",
        "The `ToolCallSampler` class is at the core of our function calling mechanism. It requires a configuration specifying the vocabulary size of the tokenizer, among other settings. Once configured, it will intercept and process function call patterns during text generation.\n"
      ],
      "metadata": {
        "id": "78kHziZTSjrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from function_sampler import ToolCallSamplerConfig, ToolCallSampler\n",
        "config = ToolCallSamplerConfig(vocab_size=len(tokenizer) )\n",
        "\n",
        "sampler = ToolCallSampler(tokenizer=tokenizer, functions=s, config=config)\n",
        "from transformers import LogitsProcessorList, TextStreamer\n",
        "\n",
        "streamer=TextStreamer(tokenizer)\n",
        "import time\n",
        "start = time.time()\n",
        "x = model.generate(\n",
        "    tokens.to(\"cuda\"),\n",
        "    max_new_tokens=800,\n",
        "    logits_processor=LogitsProcessorList([sampler]),\n",
        "    do_sample=True,\n",
        "    streamer=streamer,\n",
        "\n",
        ")\n",
        "taken = time.time() - start\n",
        "taken"
      ],
      "metadata": {
        "id": "1zAIR5mlKQ05"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}